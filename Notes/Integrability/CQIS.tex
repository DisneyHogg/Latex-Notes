\documentclass{article}

\usepackage{../../header-colourful}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Preamble

\title{Classical and Quantum Integrable Systems}
\author{Linden Disney-Hogg}
\date{November 2019}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
These are notes from the SMSTC course CQIS, lectured by Jos\'e Figueroa-O'Farrill, Bernd Schroers, Christian Korff, and Bart Vlaar. Unless I type up other things, this course will start at symplectic manifolds, as the notes on general manifold theory etc. are covered in the EKC notes, or in your first course on manifolds. These notes will aim to represent the union of what is covered in their notes, and what was said that I was able two write down during lectures. \\
It is entirely possible that my conventions do not line up between sections, though I will have endeavoured to reduce this. Moreover, there may be points when I have tried to retroactively change conventions and missed points, so be on your toes. \\
I will have arranged the notes to my liking, and added in extra bits where I have something to say, so the numbers in my notes will not line \\
Finally, as always, there may be typos in these notes, so please feel free to email me at \href{mailto:s1504632@ed.ac.uk}{\underline{s1503632@ed.ac.uk}} if you think there is an error, or more generally something that could be explained better, or would be a useful addition. 

\part{Classical Systems:}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Symplectic and Poisson Manifolds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Symplectic manifolds}

We start with a definition:

\begin{definition}[Symplectic Manifold]
A \bam{symplectic manifold} is a pair $(M,\omega)$ where $M$ is a smooth manifold and $\omega \in \Omega^2(M)$ is 
\begin{itemize}
    \item non-degenerate: $\forall X \in TM, \, i_X \omega \neq 0$
    \item closed: $d\omega = 0$
\end{itemize}
\end{definition}

\begin{prop}
If $M$ is a symplectic manifold, $M$ must be even-dimensional
\end{prop}
\begin{proof}
Let $\dim M = n$. This definition gives that $\forall a \in M$, the pair $(T_aM,\omega_a)$ is a symplectic vector space. The non-degeneracy condition then gives that, as a matrix, $det \omega_a \neq 0$. Now as $\omega$ is a two form, as a matrix $\omega_a^T = - \omega_a$, and so 
\eq{
\det(\omega_a) = \det(\omega_a^T) = \det(-\omega_a) = (-1)^n \det \omega_a
}
This then forces $n$ even. 
\end{proof}

Let us consider two examples; one trivial and one very important. 

\begin{example}
Let $M$ be an orientable surface. Then the volume form $\omega$ (which exists by orientability) is a non-degenerate two form on $M$, and $\omega$ is necessarily closed as $d\omega$ would be a 3-form on a 2-dimensional manifold. 
\end{example}


\begin{example}[Phase spaces]\label{ex:CQIS:phasespace}
Let $Q$ be a smooth manifold. We can construct the associated cotangent bundle $M = T^\ast Q \overset{\pi}{\to} Q$ where $p \in T^\ast_q Q \mapsto q$. In physics we call $Q$ the \bam{configuration space} and $M$ the \bam{phase space}. The phase can be given a \bam{tautological one form} $\theta : M \to T^\ast M$. This is defined as follows: taking $m=(q,p)\in M$ in locally trivialising coordinates
\eq{
\theta_{(q,p)} &= p \circ (d\pi)_{(q,p)} \\
\text{i.e. for } X \in T_{(q,p)}M, \, \theta_{(q,p)}(X) &= p\pround{d\pi_{(q,p)}(X)}
}
Note that this definition makes sense where we note $d\pi : TM \to TN, \, d\pi_m: T_mM \to T_{\pi(m)}N$, and we consider $p$ as a map $p : T_qQ \to \mbb{R}$. This give that $\theta_{(q,p)} : T_{(q,p)}M \to \mbb{R}$. If we choose our local coordinates s.t. $p_i$ is the dual to the vector $\del_{q^i}$, then letting $\ev{X}{(q,p)} = X^q\del_q + X^p \del_p$ (where here I have buried sums over indices) we can calculate
\eq{
\theta_{(q,p)}(X) &= p(X^q \del_q) = pX^q \Rightarrow \theta_{(q,p)} = p dq 
}
If we now take $\omega = -d\theta = dq \wedge dp$ we have constructed a symplectic form on $M$.  
\end{example}

With this additional structure on our manifolds, we can then ask for preservation of this structure.

\begin{definition}
Let $\omega$ be a symplectic form on $\omega$. A vector field $\xi \in \mf{X}(M)$ is \bam{symplectic} if $\mc{L}_\xi \omega = 0$
\end{definition}

\begin{prop}
$\xi $ is symplectic iff $d(i_\xi \omega) = 0$
\end{prop}
\begin{proof}
By Cartan's identities 
\eq{
\mc{L}_\xi \omega = i_\xi (d\omega) + d(i_\xi \omega)
}
Result follows. 
\end{proof}

\begin{definition}
A symplectic vector field $\xi$ is \bam{Hamiltonian} if $\exists f \in C^\infty(M)$ s.t. $i_\xi \omega = -df$. We write $\xi = \chi_f$, and call it the \bam{Hamiltonian vector fields associated to f}. 
\end{definition}

\begin{example}
To continue on our phase space example \ref{ex:CQIS:phasespace}, let us note that in local coordinates we can say 
\eq{
i_{\chi_f}\omega &= \chi_f^q dp - \chi_f^p dq \\
df &= (\del_q f) dq + (\del_p f) dp 
}
and hence 
\eq{
\chi_f = -(\del_p f) \del_q + (\del_q f) \del_p 
}
\end{example}

\begin{ex}
Shows that if $\xi,\eta\in \mf{X}(M)$ are symplectic, then 
\eq{
\comm[\xi]{\eta} = \chi_{\omega(\xi,\eta)}
}
\end{ex}

With this symplectic structure, we can now define a very useful quantity:

\begin{definition}
For $f,g \in C^\infty(M)$, the \bam{Poisson bracket} of $f,q$ is 
\eq{
\acomm[f]{g} = \chi_f(g)
}
\end{definition}

\begin{prop}[Properties of the Poisson bracket]
The Poisson bracket obeys the following properties:
\begin{itemize}
    \item $\mbb{R}$-bilinearity
    \item antisymmetry
    \item Jacobi identity: $\forall f,g,h \in C^\infty(M), \, \acomm[f]{\acomm[g]{h}} + \acomm[g]{\acomm[h]{f}} + \acomm[h]{\acomm[f]{g}} = 0$
    \item Leibniz: $\forall f, g, h\in C^\infty(M), \, \acomm[f]{gh} = \acomm[f]{g}h + g\acomm[f]{h}$
\end{itemize}
\end{prop}
\begin{proof}
We may prove antisymmetry by showing 
\eq{
\acomm[f]{g} = \chi_f(g) = i_{\chi_f}dg = -i_{\chi_f}i_{\chi_g}\omega = \omega(\chi_f,\chi_g)
}
which is antisymmetric. Linearity and Leibniz then follow from the fact that vector fields, which are derivations, have these properties. The Jacobi identity is inherited form the Lie bracket using 
\eq{
\comm[\chi_f]{\chi_g} = \chi_{\omega(\chi_f,\chi_g)} = \chi_{\acomm[f]{g}}
}
\end{proof}

\begin{example}
Going back to our phase space example \ref{ex:CQIS:phasespace}, we can work out that in local coordinates
\eq{
\acomm[f]{g} = \pd[f]{q}\pd[g]{q} - \pd[f]{p}\pd[g]{q}
}
\end{example}

We have the following theorem to encourages us to believe that our phase space example is a good one to consider:

\begin{theorem}[Darboux's theorem]
Let $\omega\in \Omega^2(M)$ be a symplectic form. $\forall m \in M$ there a neighbourhood $U \ni x$ with coordinates $q,p$ s.t. $\omega = dq \wedge dp$. Such coordinates are called \bam{Darboux coordinates}. 
\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Integrability}

We start this subsection with a definition:

\begin{definition}[Dynamical system]
A \bam{dynamical system} is a triple $(M,\omega,H)$ where $(M,\omega)$ is a symplectic manifold and $H \in C^\infty(M)$. $H$ is called the \bam{Hamiltonian}. 
\end{definition}

\begin{remark}
It is worthwhile now to ask why people started studying this stuff? The motivation came from classical dynamics. In the 1700s, people developed the idea of the \bam{principle of least action} governed classical dynamics, that is to say the equations of motion for a system could be obtained by minimising the action functional $S = \int L \, dt$, where $L\in C^\infty(TQ)$ is called the \bam{Lagrangian}. In practice $L$ is determined by the system you are considering. For natural Lagrangians, such as $L(q,\dot{q}) = \frac{1}{2}\dot{q}^2 - V(q)$ (that for a unit mass particle moving in one dimension in the potential $V$), these equation are second order (in this case $\ddot{q} = -V^\prime(q)$). In the 1800s, Hamilton took the Legendre transform of the Lagrangian to get $H \in C^\infty(T^\ast Q)$, now known as the Hamiltonian. He showed that the equation of motions from the Lagrangian equivalently now become $\dot{q} = \pd[H]{p},\, \dot{p} = -\pd[H]{q}$ (\bam{Hamilton's equations}), which have the benefit of being first order. We see that this is none other than saying $\dot{q} = \acomm[q]{H}, \, \dot{p} = \acomm[p]{H}$ taking the natural symplectic structure on the phase space $M$. This is what motivates us talking about a symplectic manifold with chosen function $H$ as dynamical systems.
\end{remark}

\begin{prop}
Let $f \in C^\infty(M)$. If $\acomm[f]{H} =0$, $f$ takes a constant value on integral curves of $\chi_H$. We say that $f$ is \bam{conserved quantity}
\end{prop}
\begin{proof}
Choose Darboux coordinates in a neighbourhood of $m \in M$. The integral curve is given locally by Hamilton's equations. Then 
\eq{
\frac{d}{dt} f(q(t),p(t)) &= (\del_q f) \dot{q} + (\del_p) \dot{p} \\
&= (\del_q f)(\del_p H) - (\del_p f)(\del_q H) = \acomm[f]{H}
}
\end{proof}

\begin{prop}
The Poisson bracket of two conserved quantities is conserved
\end{prop}
\begin{proof}
Use the Jacobi identity
\end{proof}

\begin{definition}
Given two conserved quantities $f,g$, we say they are in \bam{involution} if $\acomm[f]{g}$. This is equivalent to $\comm[\chi_f]{\chi_g} = 0$ 
\end{definition}

\begin{example}
By the antisymmetry of the Poisson bracket, we see $H$ is always a conserved quantity. Moreover, it is in involution with all other conserved quantities. 
\end{example}

\begin{definition}[Integrable]
A dynamical system $(M,\omega,H)$ is \bam{integrable} if $\exists H=f_1, \dots, f_k$, independent conserved quantities in involution, where $k= \frac{1}{2}\dim M$.
\end{definition}

\begin{remark}
The term \textit{independent} in the above definition means in the sense that $\forall m \in M$, the tangent vectors $\ev{\chi_{f_1}}{m}, \dots, \ev{\chi_{f_k}}{m} $ are linearly independent. Equivalently the one forms $(df_1)_m, \dots, (df_k)_m$ are independent. This prevents silly choices of conserved quantities such as $H^2, e^H, \dots$, which would mean we could take all dynamical systems to be integrable. 
\end{remark}

The reason for this definition comes from the following result:

\begin{theorem}[Arnold -Liouville Theorem]
Given an integrable system, let $M_c = \pbrace{m \in M \, | \, f_i(m) = c_i}$ for constants $c_1, \dots, c_k \in \mbb{R}$. Then 
\begin{enumerate}
    \item $M_c$ is a smooth manifold invariant under the flow of $H$
    \item If $M_c$ is compact and connected, it is diffeomorphic to the $k-$torus, i.e. $M_c \cong T^k$
    \item The flow of $H$ generates conditionally periodic motion, i.e. $\exists$ action angle coordinates on $M$, $\phi,I$ s.t. the flow is given by 
    \eq{
    \frac{d\phi}{dt} = \omega \quad \omega = \omega(I)
    }
    \item The canonical equations of motion can be integrated by quadrature.
\end{enumerate}
\end{theorem}
\begin{proof}
Though this is my favourite theorem, and it has a very nice proof, due to its length I will not recreate it here and instead direct you to Arnold's \textit{Mathematical Methods of Classical Mechanics} (1989) for a proper discussion of what all the terms mean and what the proof is. \\
A generalisation can be given for Poisson manifolds (see section below) and for a a full statement and proof of this I direct you to \cite{Laurent-Gengoux2011}
\end{proof}

\begin{corollary}
Every dynamical system with $\dim M = 2$ is integrable. 
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poisson Manifolds}

Note now that in order to define time evolution of a dynamical system, we did not need a symplectic structure, just a Poisson bracket. This leads us to a natural generalisation:

\begin{definition}[Poisson bivector]
Given a Poisson bracket, the \bam{Poisson bivector} $\Pi \in \Gamma(\Lambda^2 TM)$ is defined by 
\eq{
\Pi(df,dg) = \acomm[f]{g}
}
We say the pair $(M,\Pi)$ is a \bam{Poisson manifold}
\end{definition}

\begin{remark}
Note that the Poisson bivector is well defined, as if I change $f$ by a constant, it does not affect it's Poisson bracket with other functions. 
\end{remark}

\begin{example}[KKS Poisson bracket]\label{ex:CQIS:KKSbracket}
Let $\mf{g}$ be the Lie algebra associated with the Lie group $G$, and let $\mf{g}^\ast$ be its dual. For each $X \in \mf{g}$, define the linear function $l_X \in C^\infty(\mf{g}^\ast)$ by 
\eq{
\forall \alpha \in \mf{g}^\ast, \, l_X(\alpha) = \alpha(X)
}
Note the function $l:\mf{g} \to C^\infty(\mf{g}^\ast), \, X \mapsto l_X$ is linear. Define the bivector $\Pi$ by
\eq{
\Pi(dl_x,dl_Y) = l_{\comm[X]{Y}}
}
This is bilinear, and inherits antisymmetry and the Jacobi identity from the Lie algebra bracket. We also see 
\eq{
\Pi(dl_X,d(l_Yl_Z)) = \Pi(dl_X,l_Y dl_Z + l_Z dl_Y) = l_Y \Pi(dl_X,dl_Z) + l_Z\Pi(dl_X,dl_Y)
}
Now as we know that for $\alpha \in \mf{g}^\ast$, $\pbrace{(dl_X)_\alpha \, | \, X \in \mf{g}}$ spans $T_\alpha^\ast \mf{g}^\ast$, $\Pi$ extends to a bivector on all $\lambda^2 TM$, and we see that if we define $\acomm[\cdot]{\cdot}$ by $\acomm[f]{g} = \Pi(df,dg)$ it is a Poisson bracket. Hence $\mf{g}^\ast$ is naturally a Poisson manifold. The bracket is called the \bam{Kirillov-Kostant-Souriau (KKS)} bracket 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Aside - Hamiltonian formalism of PDEs}
\hl{This subsection can be skipped if you are following along with the SMSTC course, though this relates to the wider subject area of integrability}.
\newline
Once we have seen that to describe evolution it is sufficient to have a Poisson structure, it is not necessary to restrict to just finite-dimensional systems, as we will discuss below. 

\begin{definition}[$L^2$ space]
	The space $L^2(\mbb{R})$ is the set of functions $f:\mbb{R} \to \mbb{R}$ s.t.
	\eq{
\int_{\mbb{R}} f^2(x) \, dx < \infty	
}
\end{definition}

\begin{fact}
	$(L^2(\mbb{R}),\pangle{\cdot,\cdot})$ is a complete metric space where $\pangle{\cdot,\cdot}$ is the inner product
	\eq{
\pangle{u,v} = \int_{\mbb{R}} u(x) v(x) \, dx 	
}
\end{fact}

\begin{remark}
	In the following section, we will only consider functions which are smooth, and have well defined integrals whenever we want them to. \hl{I am no functional-analyst, so I do not know what kind of space I want, but the point of the above is it better at least be $L^2$, because I want that inner product}. In what follows in this subsection, I will call the space of such functions $M$. Natural extensions exist for functions of more than one variable, but for simplicity I will note write about them here. 
\end{remark}

Let $F:M \to \mbb{R}$ be some functional on our space that depends smoothly on its argument.  

\begin{definition}
	The \bam{functional derivative} of $F$ is $\frac{\delta F[u]}{\delta u} \in M$ defined s.t. 
	\eq{
\pangle{\frac{\delta F[u]}{\delta u},\phi} = \lim_{\eps \to 0} \frac{F[u+\eps \phi]- F[u]}{\eps}	
}
We denote 
\eq{
\frac{\delta F[u]}{\delta u}(x) = \frac{\delta F[u]}{\delta u(x)}
}
\end{definition}

\begin{remark}
	We can define a functional derivative as such by using a version of the Riesz representation theorem. 
\end{remark}

\begin{prop}
	Let $F$ be given by 
	\eq{
F[u] = \int_{\mbb{R}} f(u,u_x,u_{xx}, \dots) \, dx 	
}
Then 
\eq{
\frac{\delta F[u]}{\delta u} = \pd[f]{u} - \frac{d}{dx} \pd[f]{u_x} + \frac{d^2}{dx^2} \pd[f]{u_{xx}} - \dots = \sum_{k \geq 0} (-1)^k \frac{d^k}{dx^k}  \pd[f]{(\del_x^k u)}
}
\end{prop}
\begin{proof}
	We have
	\eq{
F[u+ \eps \phi] &= \int_{\mbb{R}} f(u+\eps\phi,u_x + \eps\phi_x , u_{xx} + \eps \phi_{xx}, \dots) \, dx \\
&= \int_{\mbb{R}} f + \eps\sum_{k \geq 0} (\del_x^k \phi)  \pd[f]{(\del_x^k u)} \, dx 
}
Now 
\eq{
\int_{\mbb{R}}(\del_x^k \phi)  g \, dx &= \int_{\mbb{R}} \frac{d}{dx}\psquare{(\del_x^{k-1} \phi) g} - (\del_x^{k-1} \phi) \frac{dg}{dx} \, dx \\
&= \psquare{(\del_x^{k-1} \phi) g}_{-\infty}^\infty - \int_{\mbb{R}}(\del_x^{k-1} \phi) \frac{dg}{dx} \, dx
}
If we assume sufficient decay conditions on $g,\phi$, and its derivative we can set the boundary term to 0 and then the result follows. 
\end{proof}

With the functional derivative we are able to define a Poisson bracket on the space of smooth functionals by 
\eq{
\acomm[F]{G}[u] = \int_{\mbb{R}} \frac{\delta F[u]}{\delta u(x)} \pd{x} \frac{\delta G[u]}{\delta u(x)} \, dx 
} 
With this Poisson bracket we can define time evolution of functionals $F=F(t)$ by picking a Hamiltonian functional $H$ and having
\eq{
\pround{\pd{t}F(t)}[u] = \acomm[F(t)]{H}[u]
}
Instead of viewing the time evolutions occuring as the functional changing, and the function $u$ remaining constant, we can move the time evolution into the function and have the functional remain constant, so we have 
\eq{
\pd{t} F[u(t)] = \acomm[F]{H}[u(t)] 
}
Now for each $y \in \mbb{R}$ we have the functional $y$ defined by 
\eq{
y[u] = u(y)
}
We can give this an integral representation by having 
\eq{
y[u] = \int_{\mbb{R}} \delta(y-x) u(x) \, dx
}
with which we see 
\eq{
\frac{\delta y[u]}{\delta u(x)} = \delta(y-x)
}
If we consider time evolution of this functional and then move the time evolution to $u$ we have 
\eq{
\pd{t} u(t,y) = \int_{\mbb{R}} \delta(y-x) \pd{x}\frac{\delta H[u(t)]}{\delta u(t,x)} \, dx  = \pd{y}\frac{\delta H[u(t)]}{\delta u(t,y)}
}
\begin{example}
	The \bam{Korteweg-de Vries (KdV) equation} is hugely fundamental in integrable systems for a variety of reason. It is a non-linear 3rd order pde given by 
	\eq{
u_t + u_{xxx} + 12 uu_x = 0	
} 
Now let us define the functional 
\eq{
H[u] &= \int_{\mbb{R}} \frac{1}{2}u_x^2 - 2u^3  \, dx 
}
We get 
\eq{
\frac{\delta H[u]}{\delta u} &= - \del_xu_x - 6u^2
}
and so the corresponding evolution equation for the Hamiltonian system is 
\eq{
u_t &= \del_x(-u_{xx}-6u^2) = -u_{xxx} - 12uu_x
}
This is just KdV.
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lie group actions and moment maps}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Group actions}

Let $G$ be a group and $M$ a manifold. 
\begin{definition}
A \bam{left action} of $G$ on $M$ is a smooth map 
\eq{
G \times M &\to M \\
(g,a) &\mapsto g \cdot a
}
s.t $g_1 \cdot(g_2 \cdot a) = (g_1 g_2) \cdot a$ an $e \cdot a = a$. 
\end{definition}

\begin{definition}
For $a \in M$ the \bam{orbit} of $a$ is 
\eq{
G \cdot a = \mc{O}_a = \pbrace{g \cdot a \, | \, g \in G}
}
\end{definition}

We have a map $\phi: H \to \Diff(M)$ with 
\eq{
\phi_g : M &\to M \\
a &\mapsto g \cdot a
}
This satisfies $\phi_{g_1} \circ \phi_{g_2} = \phi_{g_1 g_2}$. 

\begin{definition}
A $G$-action is \bam{effective} if $\ker\phi = e$. It is \bam{locally effective} if $\ker\phi$ is discrete and $\Lie(\ker\phi) = 0$.
\end{definition}

\begin{definition}
A $G$-action is called \bam{transitive} if $\forall a \in M, \, \mc{O}_a =M$
\end{definition}

\begin{definition}
The \bam{Stabiliser} of $a \in M$ is 
\eq{
G_a = \pbrace{g \in G \, | \, g \cdot a = a}
}
\end{definition}

\begin{theorem}
\eq{
G \cdot a  \cong \faktor{G}{G_a}
}
and moreover this is a $G$-equivariant diffeomorphism 
\end{theorem}

\begin{example}
Recall we have $\Ad:G \to GL(\mf{g})$ the \bam{adjoint rep}, where $\Ad_g = \psquare{(L_g)_\ast \circ (R_{g^{-1}})_\ast}_e$. For matrix groups this acts as 
\eq{
\Ad_g(X) = gX g^{-1}
}
We then get the \bam{coadjoint rep} $\Ad^\ast :G \to GL(\mf{g}^\ast)$ by 
\eq{
\Ad_g^\ast \alpha = \alpha \circ \Ad_g
}
\end{example}

Now if $G$ acts on $M$, we get a Lie algebra hom 
\eq{
\xi : \mf{g} &\to \mf{X}(M) \\
X &\mapsto \xi_X
}
where $\xi_X$ is the \bam{fundamental vector field} associated with $X$, defined by \eq{
(\xi_X f)(a) \equiv \ev{\frac{d}{dt}f\pround{\phi_{\exp(-tX)}(a)}}{t=0}
}
where $f \in C^\infty(M)$. 
\begin{fact}
The sign is chosen such that 
\eq{
\comm[\xi_X]{\xi_Y} = \xi_{\comm[X]{Y}}
}
\end{fact}

Suppose now $G$ acts transitively, and choose an 'origin' $o \in M$, and call $G_o=H$. We then must have 
\eq{
M = G \cdot o \cong \faktor{G}{H}
}
One says that $M$ is a \bam{homogeneous space} for $G$. Note that the $G$-equivariance means 
\begin{tkz}
M \arrow[r,"\cong"] \arrow[d,"\phi_g"'] & \faktor{G}{H} \arrow[d,"L_g"]\\ M \arrow[r,"\cong"] & \faktor{G}{H}
\end{tkz}
commutes. 

\begin{definition}[Linear isotropy representation]
Then map 
\eq{
H &\to \End(T_oM) \\
h &\mapsto ((\phi_h)_\ast)_o
}
is called the \bam{linear isotropy representation}
\end{definition}
\begin{prop}
The linear isotropy representation is a representation with rep space $T_oM$. 
\end{prop}
\begin{proof}
We then have the evaluation map 
\eq{
\mf{X}(M) &\overset{\eval_o}{\to} T_oM \\
\xi &\mapsto \xi_o
}
Concatenating $\mf{g} \overset{\xi}{\to} \mf{X}(M) \overset{\eval_o}{\to} T_oM$ means we get the short exact sequence (SES)
\eq{
0 \to \mf{h} \to \mf{g} \to T_o M \to 0
}
as $\ker(\eval_o \circ \xi) = \mf{h} = \Lie(H)$. Now $\forall h \in H$
\eq{
\phi_h(o) = o &\Rightarrow ((\phi_h)_\ast)_o : T_oM \to T_o M \\
\phi_{h_1} \circ \phi_{h_2} = \phi_{h_1 h_2} &\Rightarrow ((\phi_{h_1})_\ast)_o \circ ((\phi_{h_2})_\ast)_o = [(\phi_{h_1 h_2})_\ast]_o
}
The first line shows that $((\phi_h)_\ast)_o \in \End(T_oM)$, and the second shows that it is indeed a representation. 
\end{proof}

\begin{idea}
We also have that $H$ acts on $\mf{g},\mf{h}$ by $\ev{\Ad}{H}$. Thus
\eq{
0 \to \mf{h} \to \mf{g} \to T_o M \to 0
}
is a short exact sequence of $H$-modules. If the sequence splits, i.e. $\exists \mf{m} \subset \mf{g}$ s.t $\mf{g} = \mf{h} \oplus \mf{m}$ and $\Ad_H \mf{m} \subset \mf{m}$, we say $\faktor{G}{H}$ is \bam{reductive}.
\end{idea}

\begin{theorem}[Frobenius Reciprocity / Holonomy principle]
There is a 1-1 correspondence
\eq{
\pbrace{\text{$H$-invariant tensors on $T_oM$}} \leftrightarrow \pbrace{\text{$G$-invariant tensor fields on $M$}}
}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Symplectic actions}

We now want to let $G$ act on a symplectic manifold $(M,\omega)$. It is a natural consideration to ask whether $\omega$ is preserved

\begin{definition}
A group action on a symplectic manifold is \bam{symplectic} if it preserves the symplectic structure, i.e.
\eq{
\forall g \in G,\, \phi_g^\ast \omega = \omega
}
\end{definition}
\begin{prop}
If a group action is symplectic, the $\forall X \in \mf{g}$, $\xi_X$ is a symplectic vector field
\end{prop}
\begin{proof}
Infinitesimally, taking $g = \exp(tX)$ this says that 
\eq{
\mc{L}_{\xi_X}\omega = 0 &\Rightarrow \xi_X \text{ is symplectic} \\
&\Rightarrow d(i_{\xi_X} \omega) = 0 
}
\end{proof}
If $\xi_X$ is Hamiltonian, $\exists \psi_X$ s.t. $i_{\xi_X} \omega = -d\psi_X$. We can choose this such that we get a linear map 
\eq{
\mf{g} &\overset{\psi}{\to} C^\infty(M) \\
X &\mapsto \psi_X
}
We can then ask if $\psi$ is a Lie algebra homomorphism, i.e 
\eq{
\acomm[\psi_X]{\psi_Y} = \psi_{\comm[X]{Y}}
}

\begin{definition}
The $G$ action is called \bam{Hamiltonian} is $\forall X \in \mf{g}, \, \exists \psi_X \in C^\infty(M)$ s.t 
\eq{
\xi_X = \acomm[\psi_X]{\cdot} \\
\acomm[\psi_X]{\psi_Y} = \psi_{\comm[X]{Y}}
}
\end{definition}

\begin{remark}
The function $\psi$ should correspond to the Hamiltonian of the system, and you can see examples of this in the following subsection. 
\end{remark}

\begin{fact}
If $H_{dR}^1(M) = 0$, all group actions on $M$ are Hamiltonian 
\end{fact}

\begin{fact}
If $\comm[\mf{g}]{\mf{g}} = \mf{g}$, all group actions of $G$ on a symplectic manifold are Hamiltonian. 
\end{fact}

\begin{ex}
The function
\eq{
C(X,Y) = \acomm[\psi_X]{\psi_Y} - \psi_{\comm[X]{Y}}
}
is locally constant
\end{ex}

\begin{definition}
$\Ad : G \to GL(\mf{g})$ is a Lie group hom, so we have 
\eq{
\ad = (\Ad_\ast)_e : \mf{g} &\to \mf{gl}(\mf{g}) \\
X &\mapsto \comm[X]{\cdot}
}
\end{definition}

\begin{fact}
If $\mf{g}$ is semisimple, i.e. $\kappa(X,Y) = \tr(\ad_X \circ \ad_Y)$, then one can choose $\psi_X$ s.t $C(X,Y) = 0$. 
\end{fact}

\begin{definition}
Given a Hamiltonian action, the \bam{moment map} is 
\eq{
\mu : M & \to \mf{g}^\ast \\
a &\mapsto \mu(a)
}
s.t $\mu(a)(X) = \psi_X(a)$.
\end{definition}

Note we are getting the diagram 
\begin{tkz}
\mf{g} \arrow[r,"\xi"] \arrow[d,"\psi"'] & \mf{X}(M) \arrow[d,"\cong"] \\ C^\infty(M) \arrow[r,"d"'] & \Omega^1(M)  
\end{tkz}
where the map $\mf{X}(M) \to \Omega^1(M)$ is $X \to i_X\omega$. This is an isomorphism of vector spaces as $\omega$ is non-degenerate. 

\begin{lemma}
$\mu$ satisfies the condition that $\forall \eta \in T_a M, \, X \in \mf{g}$, 
\eq{
(d\mu)_a(\eta)(X) = -\omega_a(\xi_X,\eta)
}
\end{lemma}
\begin{proof}
By definition, substituting in. 
\end{proof}

\begin{prop}
If $G$ is connected, then $\mu$ is $G$-equivariant, i.e. 
\begin{tkz}
M \arrow[r,"\mu"] \arrow[d,"\phi_g"'] & \mf{g}^\ast \arrow[d,"\Ad_g^\ast"] \\ M \arrow[r,"\mu"'] & \mf{g}^\ast
\end{tkz}
commutes
\end{prop}
\begin{proof}
As $G$ is connected, it is sufficient to show $\mf{g}$ equivariance. Now the equivariance condition is 
\eq{
\mu \circ \phi_g &= \Ad_g^\ast \circ \mu \\
\Rightarrow \phi_g^\ast \psi_Y &= \psi_{\Ad_{g^{-1}}(Y)}
}
which for $g = \exp(tX)$ reads 
\eq{
-\xi_X \psi_Y = \psi_{\comm[-X]{Y}} = - \acomm[\psi_X]{\psi_Y}
}
which is satisfied by the definition of a Hamiltonian action. 
\end{proof}

\begin{remark}
We will see later a theorem that \hl{\dots}
\end{remark}

We have now shown that a moment map satisfies $(d\mu)_a(\eta)(X) = -\omega_a(\xi_X,\eta)$ and is $G$-equivariant. If a map $C^\infty(M) \to \mf{g}^\ast$ has these properties, we conversely say that it is a moment map. This characterisation becomes useful for defining induced moment maps. 

\begin{prop}\label{prop:CQIS:HamiltonianActionRestriction}
Let $G \lact M$ be a Hamiltonian action with moment map $\mu_G$, and $i : H \hookrightarrow G$ the inclusion of a Lie subgroup. Then the restriction of the action to $H$ is Hamiltonian with moment map $\mu_H =i^\ast \mu_G$
\end{prop}
\begin{proof}
We need that $\forall X \in \mf{h}^\ast, \eta \in T_aM,$ 
\eq{
(d\mu_H)_a(\eta)(X) = -\omega_a(\xi^H_X,\eta)
}
and that 
\eq{
\mu_H \circ \phi_h = \Ad_h^\ast \circ \mu_H
}
where $\xi_X^H$ is the fundamental vector field corresponding to $X\in \mf{h}$, as defined by the flow in $H$. Let $\xi_X^H$ be the corresponding quantity for the flow from $G$. As pullback and exterior derivative commute we have that $d\mu_H = i^\ast(d\mu_G)$, so 
\eq{
(d\mu_H)_a(\eta)(X) = -\omega_a(\xi^G_{di(X)},\eta)
}
but as $di = \id$, we are done for the first condition. Then second then immediately follows by the equivariance of $\mu_G$ restricted to $h \in H$, provided $i$ is a Lie group homomorphism, which is ensured by having $H$ be a Lie subgroup. 
\end{proof}

\begin{remark}
This is an example of a more general property that any Lie group homomorphism $f : H \to G$ gives a moment map for $H$ by $\mu_H = f^\ast \mu_G$ (e.g. see example \ref{example:CQIS:unitaryaction}). This is a particularly important corollary because of its implication for \bam{symmetry breaking}. Suppose $G$ is the group of diffeomorphisms of $M$ that preserve a Hamiltonian $H:M \to \mbb{R}$. Then $H$ and the components of $\mu$ are $G$-equivariant functions, and moreover these will be constants of any time evolution by Noether's theorem. If we add terms to the Hamiltonian which break the symmetry group to $H \leq G$, then the restriction of the moment map gives us the remaining quantities that are still preserved after symmetry breaking.  
\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reduced Phase Space}
We now replicate a section from \cite{Moser2010}. Using the above fact about equivariance we can make the following definition:
\begin{definition}
	Given $\alpha \in \mf{g}^\ast$, and assuming that $\mu^{-1}(\alpha)$ is a manifold, $G_\alpha$ is compact, and acts on $\mu^{-1}(\alpha)$ with no fixed points, the \bam{reduced phase space} is 
	\eq{
		\tilde{M} = \faktor{\psi^{-1}(\mu)}{G_\mu}
	}
	where $[p] = \pbrace{\phi_g p \, | \, g \in G_\mu}$. 
	We have the bundle structure 
	\begin{center}
		\begin{tikzcd}
			G_\mu \arrow[r] & \psi^{-1}(\mu) \arrow[d,"\pi"] \\
			& \tilde{M}
		\end{tikzcd}
	\end{center}
\end{definition}

$\tilde{M}$ is a symplectic manifold with form $\tilde{\omega}$ inherited from $M$ by $\pi^\ast \tilde{\omega} = i^\ast \omega$

\begin{center}
	\begin{tikzcd}
		\psi^{-1}(\mu) \arrow[r,"i"] \arrow[d,"\pi"] & (M,\omega)\\
		(\tilde{M},\tilde{\omega}) 
	\end{tikzcd}
\end{center}

Moreover it can be given a Hamiltonian $\tilde{H}$ specified by the condition $\tilde{H}\circ \pi = H \circ i $


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Examples of moment maps}
This subsection will just be dedicated to going through some problems on moment maps, and seeing some examples. 

\begin{example}
Let $(M, \omega) = (T^\ast Q, - d\theta)$. We have Darboux charts $(q^i,p_i)$ which make 
\eq{
\theta &= \sum_i p_i dq^i \\
\omega &= \sum_i dq^i \wedge dp_i
}
If $\phi \in \Diff(Q)$ get $T^\ast \phi \in \Diff(T^\ast Q)$
\eq{
(T^\ast \phi)_q : T^\ast_{\phi(q)} Q \to T^\ast_q Q
}
We know 
\eq{
d\phi : TQ &\to TQ \\
(q,X\in T_qQ) &\mapsto (\phi(q),d\phi_q(X) \in T_{\phi(q)}Q)
}
then 
\eq{
T^\ast \phi = (d\phi)^\ast : T^\ast Q &\to T^\ast Q \\
(\phi(q),p \in T_{\phi(q)}^\ast Q) &\mapsto (q,p \circ d\phi_q \in T_q^\ast Q) \\
(q,p) &\mapsto (\phi^{-1}(q),p \circ d\phi_{\phi^{-1}(q)})
}
This preserves $\theta$, i.e $(T^\ast \phi)^\ast \theta = \theta$. To see this observe 
\eq{
(T^\ast \phi)^\ast_{(q,p)} \theta_{(T^\ast \phi)(q,p)} &= \theta_{(T^\ast \phi)(q,p)} \circ (d(T^\ast \phi))_{(q,p)} \\
&= (p \circ d\phi_{\phi^{-1}(q)}) \circ (d\pi)_{(T^\ast \phi)(q,p)} \circ (d(T^\ast \phi))_{(q,p)} \\
&= (p \circ d\phi_{\phi^{-1}(q)})) \circ (d(\pi \circ T^\ast \phi))_{(q,p)} \\
&= (p \circ d\phi_{\phi^{-1}(q)}) \circ  (d(\phi^{-1} \circ \pi))_{(q,p)} \\
\text{(by chain rule)}&= p \circ d\pi_{(q,p)} = \theta_{(q,p)}
}

and as pullback commutes with $d$, must have $(T^\ast \phi)^\ast \omega = \omega$. Hence $G$ acts on $T^\ast Q$ symplectically. Moreover, the action is Hamiltonian as 
\eq{
\mc{L}_{\xi_X} \theta = 0 \Rightarrow i_{\xi_X} \omega = d(i_{\xi_X}\theta)
}
so we have $\psi_X = -\theta(\xi_X)$. To show $X \mapsto \psi_X$ is a LA hom note 
\eq{
\psi_{\comm[X]{Y}} &= -\theta(\xi_{\comm[X]{Y}}) \\
&= -\theta(\comm[\xi_X]{\xi_Y}) \\
&=- i_{\comm[\xi_X]{\xi_Y}}\theta \\
&= - \comm[\mc{L}_{\xi_X}]{i_{\xi_Y}}\theta \\
&= - \mc{L}_{\xi_X}i_{\xi_Y}\theta \\
&= -i_{\xi_X}d(i_{\xi_Y}\theta) \\
&= i_{\xi_X}i_{\xi_Y}d\theta \\
&= \omega(\xi_X,\xi_Y) = \acomm[\psi_X]{\psi_Y}
}
We can write down the moment map as, for $m \in M$ with $\pi(m)=q$ and $X \in \mf{g}$ 
\eq{
\mu(m)(X) = \psi_X(m) = -\theta(\xi_X)(m) = -m((\pi_\ast)_m \xi_X) = m((\zeta_X)_q)
}
where we have let $\zeta_X$ be the fundamental vector field corresponding to $X$ for the action on $Q$. \\
In Darboux charts we get 
\eq{
\psi_X(q,p) = \sum_i p_i \zeta^i_X(q)
}
These are called \bam{point transformations}. 
\end{example}

\begin{example}
Take $Q = \mbb{R}^n$ and let $G = \mbb{R}^n \lact Q$ act via translations i.e. for $x \in G$ we have $\phi_x:Q \to Q, \, \phi_x(q) = q+x$. We get an induced action on $M = T^\ast Q$ given by 
\eq{
T^\ast \phi_x : M &\to M \\
(q,p) &\mapsto (\phi_x^{-1}(q),(d\phi_x)_{\phi_x^{-1}(q)}(p))
}
In other words we get an action $\Phi : G \times M \to M, \, (x,m) \mapsto \Phi_x(m)$ where 
\eq{
\Phi_x(q,p) = (q-x,p)
}
as $d\phi_x = \id$. We take the canonical induced symplectic structure $\omega = dq \wedge dp$. Now we know $\mf{g} = \mbb{R}^n$ too and letting $x \in \mf{g}$ we have that $g(t) = \exp(tx)$ must satisfy 
\eq{
g(0)\cdot q = q &\Rightarrow g(0)=0 \\
\frac{d}{dt}\psquare{g(t)\cdot q} = x &\Rightarrow g^\prime(t) = x
}
which together mean $g(t) = xt$. As such 
\eq{
\ev{\frac{d}{dt} f(\Phi_{g(t)}(q,p))}{t=0} &= \ev{\frac{d}{dt}f(q-g(t),p)}{t=0} \\
&= -x\del_q f(q,p) \\
\Rightarrow \xi_x &= -x\del_q \\
\Rightarrow i_{\xi_x}\omega &= -xdp \\
\Rightarrow d\psi_x(q,p) &= x dp \\
\Rightarrow \psi_x(q,p) &= xp \\
\Rightarrow \mu(q,p) &= p
}
We might want to ask about the reduced phase space here. As the group is abelian we have that the coadjoint orbits are trivial and the reduced phase space is 
\eq{
\tilde{M} = \faktor{\mu^{-1}(p)}{G} = \pbrace{[(0,p)]} = \pbrace{\ast}
}
the point.
\end{example}

\begin{example}
Let $Q = \mbb{R}^n$ and take $M= T^\ast Q$ with the canonical symplectic structure. We will let $G= SO(n)$ act on $Q$ by multiplication and then have this induce an action on all of $M$. Write $R = R(t) = \exp(tA) \in G$, and then the action of $G$ on $Q$ is given by 
\eq{
\phi_R(v) &= Rv \\
\Rightarrow (d \phi_R)(w) &= Rw
}
 Hence we see the induced action on the entire phase space is still multiplication. This follows from the previous example taking $\phi = \phi_R^{-1}$. Then writing $p = p_i e^i$, where $e^i$ is dual to the standard basis of $\mbb{R}^n$ $e_i$, we get 
 \eq{
 p &\mapsto (p \circ d\phi_R^{-1}): w^j e_j \mapsto p_i e^i\pround{(R^{-1})_{jk} w^k e_j} = p_j (R^{-1})_{jk}
 }
 Using that $R \in SO(n) \Rightarrow R^{-1} = R^T$, we see 
 \eq{
 p_i \mapsto  (R^T)_{ji}p_j = R_{ij}p_j \Rightarrow p \mapsto Rp
 }
This allows us to calculate $\xi_A$ as for $f \in C^\infty(M)$,
\eq{
\xi_A(f) &= \ev{\frac{d}{dt} f(Rq,Rp)}{t=0} \\
&= \ev{\frac{d}{dt} f(q+tAq+\dots,p+tAp+\dots)}{t=0} \\
&= (Aq) \del_q f  + (Ap)\del_p f\\ 
\Rightarrow \xi_A &= (Aq) \del_q + (Ap)\del_p
}
where $q,p$ are coordinates on the phase space, which can be chosen globally in this case by fixing a basis of $\mbb{R}^n$. Note I have suppressed vector notation for visual simplicity, but really we should be thinking $\xi_A = (A_{ij}q_j) \del_{q_i} + (A_{ij}p_j)\del_{p_i}$. Now using $\omega = dq \wedge dp$ we find 
\eq{
i_{\xi_A} \omega = (Aq)dp - (Ap) dq
}
This is clearly symplectic, and moreover it is Hamiltonian with 
\eq{
\psi_A(q,p) = q^T A p 
}
This follows as $A$ must be antisymmetric to have $\exp(tA)$ orthogonal. If we choose the basis 
\eq{
\pbrace{T_{ij} = E_{ij} -E_{ji} \, | \, i<j}
}
of antisymmetric matrices we get 
\eq{
\psi_{T_{ij}}(q,p) = q_i p_j - q_j p_i = L_{ij}
}
a component of angular momentum. If we let $t_{ij}$ be the dual basis to $T_{ij}$, we find \eq{
\mu(q,p) = \sum_{i < j} L_{ij} t_{ij}
}
\end{example}


For the following example, we will need a slight bit of prep. 

\begin{definition}
The \bam{Pfaffian} of a matrix $M$ is $\Pfaff(M) = \sqrt{\det M}$
\end{definition}

\begin{lemma}
Let $M$ be an $n \times n$ antisymmetric matrix. Then if $n$ is even, $\Pfaff(M)$ is a polynomial in the entries of $M$
\end{lemma}
\begin{proof}
Lets first consider the antisymmetric matrix 
\eq{
M = \begin{pmatrix} 0 & a_1 & & &  & \\ -a_1 & 0 & a_2 && &\\  & -a_2 & 0 & a_3 & &  \\ &  & -a_3 & 0 & \ddots & \\ &&& \ddots  & \ddots & a_{n-1} \\
&&&& -a_{n-1} & 0
\end{pmatrix}
}
We can calculate 
\eq{
\det M &= a_1 \begin{vmatrix} a_1 & & & &  \\ -a_2 & 0 & a_3 & & \\ & -a_3 & 0 & \ddots & \\ && \ddots & \ddots & a_{n-1} \\ &&& -a_{n-1} & 0 \end{vmatrix} \\
&= a_1^2 \begin{vmatrix}  & & &  \\  0 & a_3 & & \\  -a_3 & 0 & \ddots & \\ & \ddots & \ddots & a_{n-1} \\ && -a_{n-1} & 0 \end{vmatrix} 
}
and so using induction 
\eq{
\det M = a_1^2 a_3^2 \dots a_{n-1}^2 \Rightarrow \Pfaff(M) = a_1 a_3 \dots a_{n-1}
}
Now take the most general antisymmetric matrix 
\eq{
\tilde{M} = \begin{pmatrix} 0 & a_{12} & a_{13} & a_{14}& \dots & a_{1n}\\ -a_{12} & 0 & a_{23} & a_{24}  & \dots  &a_{2n}\\ -a_{13} & -a_{23} & 0 & a_{34} &\dots  & a_{3n} \\ -a_{14}& -a_{24} &  -a_{34} & 0 & \ddots & \vdots \\ \vdots& \vdots& \vdots & \ddots  & \ddots & a_{n-1,n} \\
-a_{1n}& -a_{2n}& -a_{3n} & \dots & -a_{n-1,n} & 0
\end{pmatrix}
}
Applying row and column operations, which do not change the determinant, we can say that provided $a_{23}  \neq 0$
\eq{
\det \tilde{M} &= \begin{vmatrix} \frac{a_{12}a_{13}}{a_{23}} & a_{12} & 0 & a_{14}-\frac{a_{24}a_{13}}{a_{23}}& \dots & a_{1n}-\frac{a_{2n}a_{13}}{a_{23}}\\ -a_{12} & 0 & a_{23} & a_{24}  & \dots  &a_{2n}\\ -a_{13} & -a_{23} & 0 & a_{34} &\dots  & a_{3n} \\ -a_{14}& -a_{24} &  -a_{34} & 0 & \ddots & \vdots \\ \vdots& \vdots& \vdots & \ddots  & \ddots & a_{n-1,n} \\
-a_{1n}& -a_{2n}& -a_{3n} & \dots & -a_{n-1,n} & 0
\end{vmatrix} \\
&= \begin{vmatrix} 0 & a_{12} & 0 & a_{14}-\frac{a_{24}a_{13}}{a_{23}}& \dots & a_{1n}-\frac{a_{2n}a_{13}}{a_{23}}\\ -a_{12} & 0 & a_{23} & a_{24}  & \dots  &a_{2n}\\ 0 & -a_{23} & 0 & a_{34} &\dots  & a_{3n} \\ -a_{14}+\frac{a_{24}a_{13}}{a_{23}}& -a_{24} &  -a_{34} & 0 & \ddots & \vdots \\ \vdots& \vdots& \vdots & \ddots  & \ddots & a_{n-1,n} \\
-a_{1n}+\frac{a_{2n}a_{13}}{a_{23}}& -a_{2n}& -a_{3n} & \dots & -a_{n-1,n} & 0
\end{vmatrix}
}
Repeating this process of row elimination (possibly multiple times, requiring $a_{i,i+1}\neq 0$. Note I have designed this to not be the most efficient way of doing the elimination, which can be seen in the following remark. ) we reduce down to the case above, where the $a_i$ are rational functions of the $a_{ij}$. Moreover, it turns out that the polynomials of the $a_{ij}$ divided by are exactly multiplied back out when the determinant is taken, so in fact $\det\tilde{M}$ is a square of a polynomial in the entries. If in actuality we did have a $a_{i,i+1}=0$, then we may set it equal to $\eps >0$, go through the calculation, and then take $\eps \to 0$ at then end, which is valid from the continuity of $\det$.
\end{proof}

\begin{remark}
In order to aid my intuition on this problem I used Maple to do the calculation. So you can replicate this, I include the worksheet here: 
\begin{lstlisting}[language=Maple,frame=single]
with(LinearAlgebra):
M:=Matrix([[0,a,b,c],[-a,0,d,e],[-b,-d,0,f],[-c,-e,-f,0]]);
factor(Determinant(M));
for i from 2 to 3 do
    for j to i - 1 do 
        M:=RowOperation(M,[j,i],-M(j,i+1)/M(i,i+1)); 
        M:=Transpose(M); 
        M:=RowOperation(M,[j,i],-M(j,i+1)/M(i,i+1)); 
        M:=Transpose(M); 
    end do;
end do

N:=Matrix([[0,a,b,c,g,m],[-a,0,d,e,h,n],[-b,-d,0,f,k,o],
    [-c,-e,-f,0,l,p],[-g,-h,-k,-l,0,q],[-m,-n,-o,-p,-q,0]])
factor(Determinant(N));
for i from 2 to 3 do
    for j to i - 1 do 
        N:=RowOperation(N,[j,i],-N(j,i+1)/N(i,i+1)); 
        N:=Transpose(N); 
        N:=RowOperation(N,[j,i],-N(j,i+1)/N(i,i+1)); 
        N:=Transpose(N); 
    end do;
end do
N:=map(simplify,N) #Here I needed to run the reduction again.
for i from 2 to 3 do
    for j to i - 1 do 
        N:=RowOperation(N,[j,i],-N(j,i+1)/N(i,i+1)); 
        N:=Transpose(N); 
        N:=RowOperation(N,[j,i],-N(j,i+1)/N(i,i+1)); 
        N:=Transpose(N); 
    end do;
end do
N:=map(simplify,N)
\end{lstlisting}
As mentioned earlier, this is not the most efficient way to run this reduction, it would be better to have it start from the final row and column first, then work up to the first.
\end{remark}

\begin{example}
\hl{Put in here specific example of considering $SO(4)$ and see what the conserved quantities are.} 
\end{example}

\begin{example}
Instead of looking at only induced actions on phases spaces, we can consider the action of $G = U(n)$ on $M = \mbb{C}^n$ (with coordinate $z$) by multiplication. As before, because the action is just multiplication by a constant matrix we get, taking $A \in \mf{g}$, 
\eq{
\ev{\xi_A}{(z,\bar{z})} = (Az) \del_z + (\bar{A}\bar{z}) \bar{\del}_z 
}
Note here we are getting around difficulties with complex coordinates by treating $z,\bar{z}$ as independent variables when varying. This is possible as the vectors $\del_z, \bar{\del}_z$ are independent in a vector space, and so finding the e.o.m in these coordinates is equivalent to applying a change of basis matrix from the equations wrt the real and imaginary parts of $z$. If we had written $z=x+iy$ making $x,y$ the real coordinates on $\mbb{C}^n$ the natural symplectic structure to take would be $\omega = dx \wedge dy$. In terms of $z,\bar{z}$, this is the form 
\eq{
\omega = \frac{i}{2} dz \wedge d\bar{z}
}
Hence we find 
\eq{
i_{\xi_A}\omega &= \frac{i}{2}\psquare{(Az) d\bar{z} - (\bar{A}\bar{z})dz} = \frac{i}{2}d\psquare{\bar{z}^T A z} \\
\Rightarrow \psi_A(z) &= -\frac{i}{2}z^\dagger A z = \frac{1}{2i} \pangle{z,Az}
}
where I have explicitly chosen the inner product on $\mbb{C}^n$ given by $\pangle{z,w} = z^\dagger w$. In doing this, I am considering vectors as columns. Note in this notation 
\eq{
\xi_A = 2\real\pangle{\bar{\del},Az} 
}
We can then say $\mf{g} = \spn_{\mbb{R}}\pbrace{i(E_{ij}+E_{ji}), E_{ij}-E_{ji}} = \pbrace{S_{ij},T_{ij}}$, take the dual basis $\pbrace{s_{ij},t_{ij}}$ as before, and calculate $\mu$ if we want. An easy to see case is when $n=1$, wherein writing $A = i\theta$ we see 
\eq{
\psi_\theta(z) = \frac{1}{2}\theta \abs{z}^2 \Rightarrow \mu(z) = \frac{1}{2}\abs{z}^2
}
\end{example}

\begin{example}\label{example:CQIS:unitaryaction}
Suppose now we have a Lie group $G$ which has an action on $\mbb{C}^n$ via a smooth group representation $\rho : G \to U(n)$. By requiring the rep to be smooth we make it into a Lie group homomorphism, and so it induces a representation $d\rho : \mf{g} \to \mf{u}_n$, which has dual map $(d\rho)^\ast : \mf{u}_n^\ast \to \mf{g}^\ast$. This is the cotangent lift of $\rho$. We can then get a map $\mbb{C}^n \to \mf{g}^\ast$ using the following diagram:
\begin{tkz}
\mbb{C}^n \arrow[r,"\mu_{U(n)}"] \arrow[dr,dashed,"\mu_{G}"'] & \mf{u}_n^\ast \arrow[d,"T^\ast \rho"] \\
& \mf{g}^\ast 
\end{tkz}
In other words, $\mu_G = \rho^\ast \mu_{U(n)}$. To have $\mu_G :\mbb{C}^n \to \mf{g}^\ast$ be not just a map $\mbb{C}^n \to \mf{g}^\ast$, but a moment map, we proceed as in the proof of prop \ref{prop:CQIS:HamiltonianActionRestriction} to get 
\eq{
(d\mu_G)_z(\eta)(X) = -\omega(\xi^U_{d\rho(X)},\eta)
}
for $X \in \mf{g}, \, \eta \in T_z\mbb{C}^n$. By definition, letting $\phi:U(n) \to \End(\mbb{C}^n)$ be the action of $U(n)$, the action of $G$ is $\phi\circ\rho:G \to \End(\mbb{C}^n)$, and so
\eq{
d(\phi \circ \rho) &= d\phi \circ d\rho \\ 
\Rightarrow \xi_X^G &= \xi_{d\rho(X)}^U
}
We now need $G$ equivariance of $\mu_G$, which is given infinitesimally by the condition
\eq{
\xi_X^G \psi^G_Y &= \acomm[\psi^G_X]{\psi^G_Y} \\
\Leftrightarrow \xi^U_{d\rho(X)} \psi^U_{d\rho(Y)} &= \acomm[\psi^U_{d\rho(X)}]{\psi^U_{d\rho(Y)}}
}
This is guaranteed as $d\rho$ is a Lie algebra hom. 
\end{example}



\begin{example}
Let $G = U(n)$ act via the adjoint action (i.e matrix conjugation) on $M_n(\mbb{C}^n) \cong \mbb{C}^{n^2}$. We will use the previous example to find the moment map. We will need the following matrix identity (see \cite{Horn1991}):
\eq{
(B^T \otimes A) \Vector Z = \Vector(AZB)
}
We then note that if $U \in U(n)$, then conjugation of $Z \in M_n(\mbb{C}^n)$ gives $UZU^{-1} = UZU^\dagger$, which taking the vectorisation is equivalent to the multiplication of $\Vector Z$ by $\bar{U} \otimes U$. Now 
\eq{
(\bar{U} \otimes U)^\dagger &= (\bar{U}^\dagger \otimes U^\dagger)\\
&= (\bar{U}^{-1} \otimes U^{-1}) \\
&= (\bar{U} \otimes U)^{-1}
}
so $\bar{U} \otimes U$ is unitary. This means that
\eq{
\rho : U(n) &\to U(n^2) \\
U &\mapsto \bar{U} \otimes U
}
is a well defined map. Further $\rho$ is a representation that is faithful simply by properties of the Kronecker product. Now we have 
\eq{
d\rho : \mf{u}_n &\to \mf{u}_{n^2} \\
A &\mapsto \bar{A} \otimes I + I\otimes A
}
so 
\eq{
\psi_A^G(Z) &= \psi^U_{\bar{A}\otimes I + I\otimes A}(\Vector(Z)) \\
&= \frac{1}{2i} \Vector(Z)^\dagger \psquare{ \bar{A}\otimes I + I\otimes A} \Vector(Z) \\
&= \frac{1}{2i} \Vector(Z)^\dagger \psquare{ -A^T \otimes I + I \otimes \bar{A}} \Vector(Z)\\
&= \frac{1}{2i} \Vector(Z)^\dagger \Vector(\ad_{A}(Z))\\
&= \frac{1}{2i}\tr \pround{Z^\dagger \ad_{A}(Z)} \quad \text{(using $\Vector(A)^T\Vector(B) = \tr(A^T B)$)} \\
&= \frac{1}{2i}\tr(\comm[Z]{Z^\dagger}A) \quad \text{(cycling)}
}
Recalling the inner product on $\mbb{C}^n$
\eq{
\pangle{z,w} = z^\dagger w 
}
we note 
\eq{
\pangle{\Vector(Z),\Vector(W)} = \tr(Z^\dagger W)
}
so 
\eq{
\psi_A^G(Z) &= \frac{1}{2i} \pangle{\comm[Z]{Z^\dagger},A} \\
\Rightarrow \mu(Z) &= \frac{1}{2i}\comm[Z]{Z^\dagger}
}
using the inner product to define how $\mf{g}^\ast \cong \mf{g}$. We can do a sense check on this result: note that when $n=1$, the conjugation action is trivial, and the moment map is 0. \\
Using that for multiplication in $\mbb{C}^n$ we had $\ev{\xi_A^U}{z} = 2\real\pangle{\bar{\del},Az}$, we get that for conjugation
\eq{
\ev{\xi_A^G}{Z} &= \ev{\xi_{\bar{A}\otimes I + I\otimes A}^U}{\Vector(Z)} \\
&= 2\real\pangle{\Vector{\bar{\del}},\Vector(\ad_A(Z))} \\
&= 2\real\tr(\ad_A(Z)\del^T) 
}
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poisson actions}

We now want to generalise to $G \lact (P,\Pi)$ a Poisson manifold. We can say that $G$ preserves the Poisson structure if either:
\begin{enumerate}
    \item $(\phi_g)_\ast \Pi = \Pi$
    \item The map 
    \eq{
    \phi_g^\ast : C^\infty(P) &\to C^\infty(P) \\
    f &\mapsto f \circ \phi_g
    }
    is a Poisson morphism, i.e $\acomm[\phi_g^\ast f_1]{\phi_g^\ast f_2} = \phi_g^\ast \acomm[f_1]{f_2}$
\end{enumerate}

\begin{prop}
The two conditions are equivalent
\end{prop}
\begin{proof}
\eq{
\acomm[\phi_g^\ast f_1]{\phi_g^\ast f_2} &= \Pi(d(\phi_g^\ast f_1), d(\phi_g^\ast f_2)) \\
&= \Pi(\phi_g^\ast df_1, \phi_g^\ast df_2) \\
&= \phi_g^\ast \pround{(\phi_g)_\ast \Pi}(df_1, df_2) \\
&= \phi_g^\ast \Pi(df_1,df_2) \\
&= \phi_g^\ast \acomm[f_1]{f_2}
}
\end{proof}
 Now $G$ preserving $\Pi$ gives 
 \eq{
 \forall X \in \mf{g}, \, \mc{L}_{\xi_X} \Pi = 0
 }
 or equivalently (this can also be obtained by taking the derivative of $\acomm[\phi_g^\ast f_1]{\phi_g^\ast f_2} = \phi_g^\ast \acomm[f_1]{f_2}$)
 \eq{
 \mc{L}_{\xi_X} \acomm[f_1]{f_2} = \acomm[\mc{L}_{\xi_X} f_1]{f_2} + \acomm[f_1]{\mc{L}_{\xi_X} f_2}
 }
 That the Poisson bracket satisfies the Jacobi identity gives that $\acomm[f]{\cdot}$ is a derivation. 
 
 \begin{definition}
 $\mc{L}_{\xi_X}$ is \bam{inner} if $\mc{L}_{\xi_X} = \acomm[\psi_X]{\cdot}$.
 \end{definition}

\begin{definition}
If $\mc{L}_{\xi_X}$ is linear and $\acomm[\psi_X]{\psi_Y} = \psi_{\comm[X]{Y}}$ then the $G$ action is \bam{Poisson}.
\end{definition}

\begin{ex}\label{ex:CQIS:KKSPoisson}
Recall on $\mf{g}^\ast$ we have $\Pi_{KKS}$. For $X \in \mf{g}$ we have $l_X \in C^\infty(\mf{g}^\ast)$ by 
\eq{
l_X(\alpha) = \alpha(X)
}
Then $\acomm[l_X]{l_Y} = l_{\comm[X]{Y}}$. Then show $G \lact (\mf{g}^\ast, \Pi_{KKS})$ is Poisson and we have \eq{
-\ad_X^\ast = \mc{L}_{\xi_X} = \acomm[l_X]{\cdot} 
}
\end{ex}

\begin{fact}
Poisson manifolds are foliated by symplectic submanifolds, called the symplectic leaves (See paper by Weinstein \hl{what paper?!})
\end{fact}

\begin{example}\label{example:CQIS:CoadjointOrbitSymplectic}
Let $P = \mf{g}^\ast$ with $\Pi_{KKS}$. We claim that the symplectic leaves are coadjoint orbits. To show this we need a candidate symplectic form on each coadjoint orbit. Take $\alpha \in \mf{g}^\ast$ and let $B_\alpha \in \Lambda^2 \mf{g}^\ast$ by defined by 
\eq{
B_\alpha(X,Y) = \alpha(\comm[X]{Y})
}
Suppose $X \in \mf{g}$ is s.t 
\eq{
\forall Y, \, B_\alpha(X,Y) = 0 &\Rightarrow \alpha(\comm[X]{Y}) = 0 \\
&\Rightarrow \alpha(\ad_X Y) = 0 \\
\Rightarrow -(\ad_X^\ast \alpha)(Y) = 0
}
i.e. $X \in \mf{g}_\alpha = \Lie(G_\alpha)$. Note for $X \in \mf{g}$, letting $\bar{X} = [X] \in \faktor{\mf{g}}{\mf{g}_\alpha}$, we have
\eq{
B_\alpha (\bar{X},\bar{Y}) = \alpha(\comm[X]{Y})
}
is non-degenerate, and clearly antisymmetric, so $B_\alpha$ is a symplectic form on the vector space $\faktor{\mf{g}}{\mf{g}_\alpha}$. 
Now using injection $\mf{g}_\alpha \hookrightarrow \mf{g}$ and projection $\mf{g} \twoheadrightarrow T_\alpha \mc{O}_\alpha$, where $\mc{O}_\alpha$ is the coadjoint orbit, we get the SES
\eq{
0 \to \mf{g}_\alpha \to \mf{g} \to T_\alpha \mc{O}_\alpha \to 0 \\ 
\Rightarrow \faktor{\mf{g}}{\mf{g}_\alpha} \cong T_\alpha \mc{O}_\alpha
}
We immediately know from this that $\dim\mc{O}_\alpha$ is even, as $T_\alpha \mc{O}_\alpha$ is, because as a vector space it has a symplectic form. \\
We can also show that $B_\alpha$ is invariant under the linear isotropy rep of $G_\alpha$, where the action on $B_\alpha$ is given by the dual of the induced action on $T_\alpha \mc{O}_\alpha\oplus T_\alpha \mc{O}_\alpha$, as $\forall h \in G_\alpha$
\eq{
(h \cdot B_\alpha)(\bar{X},\bar{Y}) &= B_\alpha(h^{-1}\cdot \bar{X}, h^{-1} \cdot \bar{Y}) \\
&= B_\alpha(\bar{h^{-1}\cdot X}, \bar{h^{-1} \cdot Y}) \\
&= \alpha(\comm[h^{-1} \cdot X]{h^{-1} \cdot Y}) \\
&= \alpha(h^{-1} \comm[X]{Y}) \\
&= (h \cdot \alpha)(\comm[X]{Y}) \\
&= \alpha(\comm[X]{Y}) = B_\alpha(\bar{X},\bar{Y})
}
Frobenius reciprocity then says that $B_\alpha$ induces a $G$-invariant non-degenerate 2 form $\omega$ on $\mc{O}_\alpha$ s.t. 
\eq{
\omega_\alpha(\xi_X,\xi_Y) = B_\alpha(\bar{X},\bar{Y})
}
Hence to show $\mc{O}_\alpha$ is a $G$-homogeneous symplectic manifold is suffices to show $d\omega=0$, and as the fundamental vector fields span it is sufficient then to show 
\eq{
\forall X,Y,Z \in \mf{g}, \, d\omega(\xi_X,\xi_Y,\xi_Z) = 0
}. 
Using Cartan's identity and the results of exercise \ref{ex:CQIS:KKSPoisson} we know 
\eq{
d\omega(\xi_X,\xi_Y,\xi_Z) = \xi_X \omega(\xi_Y,\xi_Z) - \omega(\comm[\xi_X]{\xi_Y},\xi_Z) + (\text{cycles})
}
As 
\eq{
\xi_X\omega_\alpha(\xi_Y,\xi_Z) = (\ad_X^\ast \alpha)(\comm[Y]{Z}) = -\alpha(\comm[X]{\comm[Y]{Z}})
}
and 
\eq{
\omega_\alpha(\comm[\xi_X]{\xi_Y},\xi_Z) = \omega_\alpha(\xi_{\comm[X]{Y}},\xi_Z) = \alpha(\comm[{\comm[X]{Y}}]{Z})
}
we see that including the cyclic terms we are done by Jacobi's identity. 
\end{example}

\begin{remark}
	These orbits come as part of the 'orbit method' developed by Kirillov (c.f. \cite{Kirillov1999}) which is useful in representation theory. 
\end{remark}

\begin{example}
	Let $G = GL_n(\mbb{R})$. We can identify $\mf{g}^\ast$ with $\mf{g}$ by writing $\mc{B}\in \mf{g}^\ast$ as 
	\eq{
		\mc{B}(A) = \pangle{A,B}= \tr(AB)
	}
	for some $B \in \mf{g}$. As such we can identify the adjoint and coadjoint representations. As such $\mc{O}_{\mc{B}} = \pbrace{\text{Matrices similar to }B}$. Thus if $\alpha$ has distinct eigenvalues then $\mc{O}_{\mc{B}} = \pbrace{\text{Matrices isospectral to }B}$. As we are in a matrix group 
	\eq{
		ad(X)Y = \comm[X]{Y}
	}
	the standard matrix commutator, and we have the differential form given by 
	\eq{
		\omega_{\mc{A}}(\comm[B_1]{\cdot},\comm[B_2]{\cdot}) = \tr(A\comm[B_1]{B_2})
	}
where we are identifying $\comm[B]{\cdot} = \xi_B$ for $B \in \mf{gl}_n(\mbb{R})$.
\end{example}


It turns out that $G \lact (P,\Pi)$ is a special case of  $(G,\Pi_G) \lact (P,\Pi_P)$ where $(G,\Pi_G)$ is called a \bam{Poisson Lie group}. \\
Recall in a Lie group we have 
\eq{
G \times G &\overset{\mu}{\to} G \\
(g_1 , g_2) &\mapsto g_1 g_2
}
We want to give $G$ a Poisson structure $\Pi$ such that $\mu$ is a Poisson morphism. This requires that we have a way to take get a Poisson structure on a product manifold:
\begin{prop}
Given $(P_i,\Pi_i)$, $i=1,2$, $\exists! \,  (P_1 \times P_2, \Pi)$ a Poisson structure s.t. 
\begin{enumerate}
    \item The projections $pr_i : P_1 \times P_2 \to P_i$ have $pr_i^\ast $ Poisson morphisms 
    \item $\forall f_i \in C^\infty(P_i), \, \acomm[pr_1^\ast f_1]{pr_2^\ast f_2} = 0$
\end{enumerate}
\end{prop}
\begin{proof}
We will show here existence, but not uniqueness. \\
Note that for $b \in P_2$ we have the map 
\eq{
i_b : P_1 &\to P_1 \times P_2 \\
a &\mapsto (a,b)
}
and likewise for $a \in P_1$ we have 
\eq{
j_a : P_2 &\to P_1 \times P_2 \\
b &\mapsto (a,b)
}
We then define 
\eq{
\Pi_{(a,b)} = (j_a)_\ast (\Pi_2)_b + (i_b)_\ast (\Pi_1)_a
}
Then 
\eq{
\acomm[f]{g}(a,b) = \acomm[j_a^\ast f]{j_a^\ast g}_2(b) + \acomm[i_b^\ast f]{i_b^\ast g}_1(a) 
}
To see this satisfies the conditions we want, note:
\begin{enumerate}
    \item For example, $pr_1 \circ i_b = \id_{P_1}$, the identity, and  $pr_1\circ j_a = c_a$, the constant map with value $a$. As such $(pr_1)_\ast (j_a)_\ast =0, \, (pr_1)_\ast (i_b)_\ast = \id$. As such 
    \eq{
    (pr_1)_\ast \Pi_{(a,b)} = (\Pi_1)_a 
    }
    \item Similarly 
    \eq{
    \acomm[pr_1^\ast f_1]{pr_2^\ast }(a,b) &= \acomm[j_a^\ast pr_1^\ast f_1]{j_a^\ast pr_2^\ast f_2}_2(b) + \acomm[i_b^\ast pr_1^\ast f_1]{i_b^\ast pr_2^\ast f_2}_1(a) \\
    &= \acomm[c_a^\ast f_1]{\id^\ast f_2}_2(b) + \acomm[\id^\ast f_1]{c_b^\ast f_2}_1(a) \\
    &= 0
    }
    as from the perspective of $\acomm[\cdot]{\cdot}_1$, $c_b^\ast f_2 = f_2(b)$ is a constant and likewise.
\end{enumerate}
\end{proof}
Hence we make the def
\begin{definition}
$(G,\Pi)$ is a \bam{Poisson Lie group} if 
\begin{itemize}
    \item $G$ is a Lie group
    \item $\Pi$ is a Poisson bivector
    \item $\mu : G \times G \to G$ is a Poisson morphism. 
\end{itemize}
\end{definition}

This Poisson morphism condition can be seen as forcing, for $f,g \in C^\infty(G)$, $a,b \in G$, we have 
\eq{
\mu^\ast \acomm[f]{g}(a,b) &= \acomm[\mu^\ast f]{\mu^\ast g}(a,b) \\
&= \acomm[j_a^\ast \mu^\ast f]{j_a^\ast \mu^\ast g}(a) + \acomm[i_b^\ast \mu^\ast g]{i_b^\ast \mu^\ast g}(b) 
}
And using $j_a^\ast \mu^\ast = (\mu \circ j_a)^\ast$, and that 
\eq{
(\mu \circ j_a)(g) = \mu(a,g) = ag &= L_a g \\
\Rightarrow \mu \circ j_a &= L_a
}
and similarly $\mu \circ i_a = R_a$, so 
\eq{
\acomm[f]{g}(ab) = \mu^\ast \acomm[f]{g}(a,b) = \acomm[L_a^\ast f]{L_a^\ast g}(a) + \acomm[R_a^\ast f]{R_a^\ast g}(b)
}
so 
\eq{
\acomm[f]{g}(e) = 0
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson Lie groups and Lie bialgebras}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Review of Vector Fields}
We will think of vector fields both as derivations, i.e. $\mf{X}(M) \ni \xi : C^\infty(M) \to C^\infty(M)$, and as sections on the manifold, i.e. $\xi \in \Gamma(TM)$. The need for this will come from the concept of \bam{F-relatedness}

\begin{definition}
If $F:M \to N$ is a smooth manifold map that is not necessarily a diffeomorphism, then $\xi,\eta \in \mf{X}(M)$ are \bam{F-related} if $\forall f \in C^\infty(M)$
\begin{itemize}
    \item $\xi(f \circ F) = (\eta f) \circ F$
    \item $F^\prime_m \xi_m = \eta_{F(m)}$
\end{itemize}
\end{definition}

\begin{notation}
Note that we will use the notation $F^\prime_m:T_mM \to T_{F(m)}N$ for the differential of $F:M \to N$, but this will all the time be mixed in with notation such as $dF, F_\ast$. Be on your toes. Moreover, I will use both $(\cdot, \cdot)$ and $\pangle{\cdot, \cdot}$ to be inner products. \hl{Hopefully I will sometime make this uniform}. If in doubt know I prefer $\pangle{\cdot, \cdot}$
\end{notation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lie bivector of a Poisson-Lie group}

Recall now the def 
\begin{definition}
A \bam{Poisson bivector} is $\Pi \in \Gamma(\Lambda^2 TM)$ satisfying some properties (antisym, Jacobi, Leibniz), and from this we get a \bam{Poisson bracket} by 
\eq{
\acomm[f]{g}(m) &= \pangle{(df_m \otimes dg_m , \Pi_m} \\
&= \pround{\Pi (f \otimes g)}(m)
}
\end{definition}

\begin{definition}
Given a Poisson bivector on a Lie group, we can define the \bam{right-translated Poisson vector} $\Pi^R : G \to \mf{g} \otimes \mf{g}$ by 
\eq{
\Pi^R_g = (R_{g^{-1}})^\prime_g \otimes (R_{g^{-1}})^\prime_g \Pi_g
}
\end{definition}

\begin{lemma}
If $G$ is a Lie group then TFAE 
\begin{enumerate}
    \item $\acomm[f_1]{f_2}(gh) = \acomm[f_1 \circ L_g]{f_2 \circ L_g}(h) + \acomm[f_1 \circ R_h]{f_2 \circ R_h}(g)$ (i.e. it is a Poisson-Lie group)
    \item The Poisson bivector satisfies 
    \eq{
    \Pi_{gh} = (L_g)^\prime_h \otimes (L_g)^\prime_h \Pi_h + (R_h)^\prime_g \otimes (R_h)^\prime_g \Pi_g
    }
    \item $\Pi^R$ satisifes the \bam{cocycle condition} 
    \eq{
    \Pi^R_{gh} = (\Ad_g \otimes \Ad_g) \Pi^R_h + \Pi^R_g
    }
\end{enumerate}
\end{lemma}
\begin{proof}
We will only prove $3 \Rightarrow 2$, as $1 \Leftrightarrow 2$ was completed when we introduced Poisson-Lie groups. 
\eq{
\Pi_{gh} &= (R_{gh})^\prime_e \otimes (R_{gh})^\prime_e \Pi^R(gh) \\
&=  (R_{gh})^\prime_e \otimes (R_{gh})^\prime_e \pround{\Ad_g \otimes \Ad_g \Pi^R(h) + \Pi^R(g)} \\
&= (R_{gh})^\prime_e \otimes (R_{gh})^\prime_e \psquare{\Ad_g \otimes \Ad_g  (R_{h^{-1}})^\prime_h \otimes (R_{h^{-1}})^\prime_h \Pi_h + (R_{g^{-1}})^\prime_g \otimes (R_{g^{-1}})^\prime_g \Pi_g}
}
We use that $R_{gh}R_{g^{-1}} = R_h R_g R_{g^{-1}} = R_h $ to say $(R_{gh}R_{g^{-1}})^\prime_g = (R_{gh})^\prime_e (R_{g^{-1}})^\prime_g = (R_h)^\prime_g$. Moreover $R_{gh} L_g R_{g^{-1}} R_{h^{-1}} = L_g$ and substituting all this in gives 
\eq{
\Pi_{gh} = (L_g)^\prime_h \otimes (L_g)^\prime_h \Pi_h + (R_h)^\prime_g \otimes (R_h)^\prime_g \Pi_g
}
$2 \Rightarrow 3$ follows from doing the above calculation in reverse. 
\end{proof}

\begin{corollary}
$\Pi_e = \Pi^R(e) = 0 $
\end{corollary}

\begin{aside}[Comments on group cohomology]
If $\Pi^R : G \to V$ is any map where $V$ is a representation space of $G$, and then we can create a boundary map which acts as 
\eq{
(\del \Pi^R)(g,h) = \rho(g) \Pi^R(h) - \Pi^R(gh) + \Pi^R(g)
}
Letting $\rho$ be the adjoint rep, we have constructed $\Pi^R$ such that it is a cocycle, i.e. it is closed 
\end{aside}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lie Bialgebras}

\begin{definition}
Let $G$ be a Poisson Lie (PL) group, $\mf{g}$ its Lie algebra. Take $\alpha_1, \alpha_2 \in \mf{g}^\ast$ and $f_1,f_2 \in C^\infty(G)$ s.t $\alpha_i = (df_i)_e$. Then define
\eq{
\comm[\alpha_1]{\alpha_2}_{\mf{g}^\ast} = (d\acomm[f_1]{f_2})_e
}
This makes $\mf{g}^\ast$ into a Lie algebra. 
\end{definition}

\begin{lemma}
The above is well defined. 
\end{lemma}
\begin{proof}
We show $\comm[\alpha_1]{\alpha_2}_{\mf{g}^\ast} = \delta^\ast (\alpha_1 \otimes \alpha_2)$ where $\delta = (\Pi^R)^\prime_e : \mf{g} \to \mf{g}\otimes \mf{g}$. Then $\delta^\ast : \mf{g}^\ast \otimes \mf{g}^\ast \to \mf{g}^\ast$ is s.t. $\forall \alpha,\beta\in\mf{g}^\ast, X \in \mf{g}$.
\eq{
\pangle{\delta^\ast (\alpha\otimes\beta),X} = \pangle{\alpha \otimes \beta,\delta(X)} = \pangle{\alpha \otimes \beta, (\Pi^R)^\prime_e (X)} 
}
Now 
\eq{
(d\acomm[f_1]{f_2})_e &= \pround{d\pangle{\Pi_g, (df_1)_g \otimes (df_2)_g}}_e \\
&= \pround{d \pangle{(R_g)^\prime_e \otimes (R_g)^\prime_e \Pi^R_g, (df_1)_g \otimes (df_2)_g }} \\
\Rightarrow \pangle{X,(d\acomm[f_1]{f_2})_e}&= \pangle{\delta(X), \alpha_1 \otimes \alpha_2 } \\
&= \pangle{X,\delta^\ast(\alpha_1 \otimes \alpha_2)}
}
\end{proof}

\begin{ex}
Show that the cocycle property of $\Pi^R$ gives 
\eq{
\delta(\comm[X]{Y}) = (\ad_X \otimes 1 + 1 \otimes \ad_X )\delta(Y) - (\ad_Y \otimes 1 + 1 \otimes \ad_Y )\delta(X)
}
This is the cocycle condition for $\delta$
\end{ex}

\begin{definition}
$\mf{g}$ is a \bam{Lie bialgebra} if it is a LA with a map $\delta_{\mf{g}} : \mf{g} \to \mf{g} \otimes \mf{g}$ s.t. 
\begin{itemize}
    \item $\forall X \in \mf{g}, \, \delta_{\mf{g}}(X) \in \mf{g} \wedge \mf{g}$ 
    \item $\delta_{\mf{g}}^\ast : \mf{g}^\ast \otimes \mf{g}^\ast \to \mf{g}^\ast$ defines a Lie bracket on $\mf{g}^\ast$ 
    \item the cocycle condition holds
\end{itemize}
We call $\delta_{\mf{g}}$ the \bam{co-commutator}
\end{definition}

\begin{remark}
What we have shown above is that the Lie algebra of a Poisson Lie group can always be made into a Lie bialgebra. The converse holds, that for any simply connected Lie group $G$, a Lie bialgebra structure on $\mf{g}$ can be integrated to a unique Poisson-Lie structure. 
\end{remark}

\begin{example}
Take $\mf{g} = \mf{su}(2)$ with $\comm[J_a]{J_b} = \eps_{abc}J_c$. We may dualise to consider $\mf{g}^\ast \cong \mbb{R}^3$ with generators $P_a$ dual to $J_a$ and then we get 
\eq{
\pangle{\delta(P_a), J_b \otimes J_c} &= \pangle{P_a , \delta^\ast(J_b \otimes J_c)} \\
&= \pangle{P_a, \comm[J_b]{J_c}} \\
&= \pangle{P_a, \eps_{bca} J_a} \\
&= \eps_{bca} = \eps_{abc} \\
\Rightarrow \delta(P_a) &= \eps_{abc} P_b \otimes P_c 
}
Using this procedure for more general co-commutators, as we may consider $\mf{g}$ with basis $\pbrace{X_a \, | \, a=1, \dots, n}$. Then $\mf{g}^\ast$ has dual basis $\pbrace{\alpha^a \, | \, a=1, \dots, n}$. Then $\mf{g}^\ast\cong\mbb{R}^n$ as $\mbb{R}$-modules (namely $\mf{g}^\ast$ is abelian), but taking $\delta_{\mf{g}^\ast}^\ast : \mf{g} \otimes \mf{g} \to \mf{g}$ to be the standard commutator on $\mf{g}$, we get a non-trivial bracket on $\mf{g}^\ast$ through the co-commutator. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsection{Manin Triples}
Recall that given two Lie algebras $\mf{g}, \mf{h}$, we have the natural Lie bracket on $\mf{g} \oplus \mf{h}$ given by 
\eq{
\comm[(X,A)]{(Y,B)}_{\mf{g} \oplus \mf{h}} = (\comm[X]{Y}_{\mf{g}},\comm[A]{B}_{\mf{h}})
}
Consider $d(\mf{g}) = \mf{g} \oplus \mf{g}^\ast$. We have non-degenerate symmetric bilinear pairing 
\eq{
\forall \alpha,\beta \in \mf{g}^\ast, \, (\alpha,\beta) &= 0 \\
\forall X,Y \in \mf{g}, \, (X,Y) &= 0 \\
(\alpha,X) &= \alpha(X)
}
We can then ask whether the natural bracket on $d(\mf{g})$ is s.t. $(\cdot,\cdot)$ is invariant, i.e. $\forall X,Y,Z \in \mf{g}, \, g \in G$
\eq{
(\Ad_gX,\Ad_gY) &= (X,Y) \\
 \Rightarrow (\comm[Z]{X},Y) + (X,\comm[Z]{Y}) &= 0 
}
and if not whether such brackets exist. 

\begin{prop}
There is a unique bracket with this property 
\end{prop}
\begin{proof}
Let $f^c_{ab}, \gamma^{ab}_c$ be the structure constants of $\mf{g},\mf{g}^\ast$, so 
\eq{
\comm[X_a]{X_b} &= f_{ab}^c X_c \\
\comm[\alpha^a]{\alpha^b} &= \gamma^{ab}_c
 \alpha^c
 }
 where $X_a,\alpha^a$ are dual bases of $\mf{g},\mf{g}^\ast$ respectively. Then we need
 \eq{
 (X_c, \comm[\alpha^a]{X_b}) &= -(\comm[X_c]{X_b}, \alpha^a) \\
 &= -f_{cb}^a \\
(\comm[\alpha^a]{X_b},\alpha^c) &= -(X_b,\comm[\alpha^a]{\alpha^c}) \\
&= -\gamma^{ac}_b 
 }
 and hence 
 \eq{
 \comm[\alpha^a]{X_b} = f^a_{bc} \alpha^c - \gamma^{ac}_b X_c
 }
\begin{ex}
Shown that this bracket satisfies the Jacobi identity, and is independent of the choice of basis.
\end{ex}
\end{proof}
We make the definition
\begin{definition}
A \bam{Manin triple} is a Lie algebra $\mf{p}$ with subalgebra $\mf{p}^\pm$ s.t $\mf{p} = \mf{p}^+ \oplus \mf{p}^-$ with invariant pairing such that 
\eq{
\pangle{\mf{p}^+, \mf{p}^+} = 0 = \pangle{\mf{p}^-,\mf{p}^-}
}
\end{definition}
We have shown that Manin triples biject with $d(\mf{g})$ for a Lie bialgebra, or in other words Lie bialgebras with $\mf{g} = \mf{p}^+$. 

\begin{example}
If $\mf{g} = \mbb{R}^3, \mf{g}^\ast = \mf{su}(2)$. We have 
\eq{
\comm[J_a]{J_b}_{\mf{g}^\ast } &= \eps_{abc}J_c \\
\comm[P_a]{P_b}_{\mf{g}} &= 0\\
}
and so we get  
\eq{
\comm[J_a]{P_b} &= \eps_{abc} P_c
}
This gives us the double algebra as the Lie algebra of the Euclidean group. 
\end{example}

\begin{example}[Lorentz group in 3d]
The Lorentz group is double covered by $SL(2,\mbb{C})$, and has lie algebra $\mf{sl}(2,\mbb{C})_\mbb{R}$. It has the basis $J_a = -\frac{i}{2}\sigma_a$, where the $\sigma_a$ are the Pauli matrices, which gives the commutation relations $\comm[J_a]{J_b} = \eps_{abc}J_c$. We now define $K_a = \frac{1}{2}\sigma_a$ to fill up the rest of the basis. this further gives us relations 
\eq{
\comm[J_a]{J_b} &= \eps_{abc} J_c \\
\comm[J_a]{K_b} &= \eps_{abc} K_c \\
\comm[K_a]{K_b} &= - \eps_{abc} J_c
}
making the redefintions $T_3 = K_3, T^+  = K_1 + J_2, T^- = K_2 - J_1$, these form a subalgebra, i.e. $\comm[\mf{t}]{\mf{t}} = \mf{t}$

\begin{ex}
Show this above statement is true.
\end{ex}

Moreover we can find use the pairing 
\eq{
\pangle{J_a,K_b} &= \delta_{ab} \\
\pangle{J_a, J_b} &= 0 = \pangle{K_a,K_b} 
}
which gives $\pangle{J_a,T_b} = \delta_{ab}$, where we have called $T_1 = T^+, T_2 = T^-$. As a results we get the Manin triple $\mf{p} = \mf{sl}(2,\mbb{C})_\mbb{R}, \, \mf{p}^+ = \mf{su}(2), \, \mf{p}^- = \mf{t}$. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Duals and doubles}
The formulation of the Lie bialgebra structure on $\mf{g}$ as a Manin triple $(\mf{g},\mf{g}^\ast,d(\mf{g}))$ is symmetric between $\mf{g}, \mf{g}^\ast$. 
\begin{prop}\label{prop:CQIS:naturalr}
Let $r=\id : \mf{g} \to \mf{g}$ considered as an element of $\mf{g} \otimes \mf{g}^\ast$, i.e. $r = \sum_a X_a \otimes \alpha^a$ where $X_a, \alpha^a$ are dual bases. Viewing $r\in d(\mf{g}) \otimes d(\mf{g})$ instead then 
\eq{
\delta(u) = (\ad_u \otimes 1 + 1 \otimes \ad_u)r
}
defines a co-commutator, i.e. $d(\mf{g})$ has a natural bialgebra structure. 
\end{prop}
\begin{proof}
That $\delta^\ast$ is a commutator will follow from a quadratic relation called the Classical Yang Baxter Equation (CYBE) (see definition \ref{def:CQIS:YBE}) for $r$. 
\end{proof}

The Lie bialgebra structure on $\mf{g}^\ast$ defines a unique simply connected Poisson-Lie structure on $G^\ast$, called the \bam{dual Poisson-Lie group} to $G$. Moreover, $d(\mf{g})$ defines a unique imply connected Lie group $d(G)$ called the \bam{classsical double} of $G$. 

\begin{fact}
$d(G) \cong G \times G^\ast$ as manifolds 
\end{fact}

\begin{example}
If $G = SU(2)$ with trivial Poisson structure, and $G^\ast = \mbb{R}^3$ with the KKS structure, then $d(G) = SU(2) \ltimes \mbb{R}^3$ is the double cover of the Euclidean group. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extended example: Coadjoint orbits of semidirect products}


Let $H$ be a Lie group with a representation $\rho:H \to \End(V)=GL(V)$ for some f.d. $\mbb{R}$-rep space $V$. For notation sake take $\dim V = n$. As $V$ is an abelian group under addition we may consider the semidirect product $G =H\ltimes_\rho V$ with group actions given by 
\eq{
(a,v) \ast (b,w) = (ab,v + \rho(a)(w))
}
for $v,w \in V, \, a,b \in H$. Note we can see 
\eq{
(a,v)^{-1} = (a^{-1}, -\rho(a^{-1})(v))
}
Our aim will be to understand the coadjoint orbits of a semidirect product. As we go through this example it is important to have a test case in our head, which for us will be $H=SO(3), \, V = \mbb{R}^3$. Then the semi-direct product represents the orientation-preserving Euclidean transforms. \\
Let $\mf{g} = \Lie(G), \, \mf{h} = \Lie(H)$. Then noting $\mf{g}\cong \mf{h} \oplus V$ we cant write a general element of $\mf{g}$ as $(A,x)$ for $A \in \mf{h}, \, x \in V$. Similarly we can write a general element of $\mf{g}^\ast$ as $(\alpha,p)$ for $\alpha \in \mf{h}^\ast, \, p \in V^\ast$, s.t. \eq{
\pangle{(\alpha,p),(A,x)} = \pangle{\alpha,A} + \pangle{p,x}
}
Note here this bracket is not an inner product, but used to represent the element of $\mf{g}^\ast$ acting on $\mf{g}$. \\
We have the map $\rho_\ast : \mf{h} \to \mf{gl}_n $ and so viewing $\mf{gl}_n \cong V \otimes V^\ast$ we have a dual map 
\eq{
\rho^\ast : V^\ast \otimes V &\to \mf{h}^\ast \\
p \otimes v &\to p\odot v 
}
where $\forall A \in \mf{h}$
\eq{
\rho^\ast(p\otimes v)(A) &= (p \otimes v)(\rho_\ast(A)) \\
\Rightarrow \pangle{p \odot v, A} &= \pangle{p,\rho_\ast(A)v}
}
Further, as $\forall a \in H, \, A \in \mf{h}, \, \rho(a),\rho_\ast(A):V \to V$ we can get dual maps
\eq{
a^\ast = \rho(a)^\ast : V^\ast &\to V^\ast \\
p &\mapsto (v \mapsto \pangle{p,\rho(a)v}) \\
&\phantom{=} \\
A^\ast = (\rho_\ast(A))^\ast : V^\ast &\to V^\ast \\
p &\mapsto (v \mapsto \pangle{p,\rho_\ast(A)v})
}


\begin{prop}
The coadjoint orbits are given by 
\eq{
\Ad_{(a,v)}^\ast (\alpha,p) &= (\Ad_a^\ast \alpha + (a^{-1})^\ast p \odot v, (a^{-1})^\ast p)
}
that is the $G$-orbits of $\mf{g}^\ast$ are fibres over the $H$-orbits of $V^\ast$
\end{prop}
\begin{proof}
Let $\phi:G \times G \to G$ be the conjugation action of $G$ on $G$, that is 
\eq{
\phi_{(a,v)}(b,w) &= (a,v)(b,w)(a,v)^{-1} \\
&= (a,v)(b,w)(a^{-1},-\rho(a^{-1})v) \\
&= (ab,v+ \rho(a)w)(a^{-1},-\rho(a^{-1})v) \\
&= (aba^{-1},v+ \rho(a)w - \rho(ab)\rho(a^{-1})v) \\
&= (aba^{-1}, v+ \rho(a)w - \rho(aba^{-1})v)
}
As conjugation preserves the identity we have that $d\pround{\phi_{(a,v)}} = \Ad_{(a,v)}: \mf{g} \to \mf{g}$ with 
\eq{
\Ad_{(a,v)}(A,x) &= (aAa^{-1}, \rho(a)x - \rho_\ast(aAa^{-1})v) \\
&= (aAa^{-1},\rho(a)x - \rho(a)\rho_\ast(A) \rho(a^{-1})v)
}
so 
\eq{
\pangle{\Ad_{(a,v)}^\ast (\alpha,p),(A,x)} &= \pangle{(\alpha,p),\Ad_{(a,v)^{-1}}(A,x)} \\
&= \pangle{(\alpha,p),(a^{-1}Aa, \rho(a^{-1})x + \rho(a^{-1}) \rho_\ast(A)v)} \\
&= \pangle{\alpha,a^{-1}Aa} + \pangle{p,\rho(a^{-1})x} + \pangle{p,\rho(a^{-1})\rho_\ast(A)v}
}
Using our definitions we see
\eq{
\pangle{\Ad_{(a,v)}^\ast (\alpha,p),(A,x)} &= \pangle{\Ad_a^\ast \alpha,A} + \pangle{(a^{-1})^\ast p,x} + \pangle{(a^{-1})^\ast p \odot v, A}  \\
\Rightarrow \Ad_{(a,v)}^\ast (\alpha,p) &= (\Ad_a^\ast \alpha + (a^{-1})^\ast p \odot v, (a^{-1})^\ast p)
}
An alternative way to think of this is to  view elements of $G$ as $(n+1) \times (n + 1)$ matrices of the form 
\eq{
M(a,v) = \begin{pmatrix} 
\rho(a) & v \\
0 & 1
\end{pmatrix}
}
as 
\eq{
M(a,v)M(b,w)& =\begin{pmatrix} \rho(a) & v \\0 & 1\end{pmatrix}\begin{pmatrix} \rho(b) & w \\0 & 1\end{pmatrix} \\ 
&= \begin{pmatrix} \rho(a)\rho(b) & v+\rho(a)w \\0 & 1\end{pmatrix}\\ 
&= \begin{pmatrix} \rho(ab) & v+\rho(a)w \\0 & 1\end{pmatrix} = M((a,v)\ast(b,w))
}
Note, $M$ is now a representation of $H \ltimes_\rho V$ with rep space $\mbb{R}^{n + 1 }$. Then 
\eq{
dM : \mf{g} &\to \mf{gl}_{n+1} \\
(A,x) &\mapsto \begin{pmatrix} \rho_\ast(A) & x \\ 0 & 0 \end{pmatrix}
}
We may then run through similar calculations as before, finding 
\eq{
\Ad_{\begin{psmallmatrix} \rho(a) & v \\ 0 & 1\end{psmallmatrix}}\begin{pmatrix} \rho_\ast (A) & x \\ 0 & 0 \end{pmatrix} &= \begin{pmatrix} \rho(a) & v \\ 0 & 1 \end{pmatrix}\begin{pmatrix} \rho_\ast (A) & x \\ 0 & 0 \end{pmatrix}\begin{pmatrix} \rho(a) & v \\ 0 & 1 \end{pmatrix}^{-1} \\
&= \begin{pmatrix} \rho(a) & v \\ 0 & 1 \end{pmatrix}\begin{pmatrix} \rho_\ast (A) & x \\ 0 & 0 \end{pmatrix}\begin{pmatrix} \rho(a^{-1}) & -\rho(a^{-1})v \\ 0 & 1 \end{pmatrix} \\
&= \begin{pmatrix} \rho(a) \rho_\ast (A) \rho(a^{-1}) & \rho(a)x-\rho(a) \rho_\ast(A) \rho(a^{-1})v \\ 0 & 0 \end{pmatrix}
}
and
\eq{
\pangle{\Ad_{\begin{psmallmatrix} \rho(a) & v \\ 0 & 1\end{psmallmatrix}}^\ast(\alpha,p), \begin{pmatrix} \rho_\ast(A) & x \\ 0 & 0 \end{pmatrix}}  &= \pangle{(\alpha,p),\Ad_{\begin{psmallmatrix} \rho(a^{-1}) & -\rho(a^{-1})v \\ 0 & 1\end{psmallmatrix}}\begin{pmatrix} \rho_\ast(A) & x \\ 0 & 0 \end{pmatrix}} \\
&= \pangle{(\alpha,p),\begin{pmatrix} \rho(a^{-1}) \rho_\ast (A) \rho(a) & \rho(a^{-1})x+\rho(a^{-1}) \rho_\ast(A) v \\ 0 & 0 \end{pmatrix}} \\
&= \pangle{\alpha,a^{-1}Aa} + \pangle{p,\rho(a^{-1})x} + \pangle{p,\rho(a^{-1})\rho_\ast(A)v}
}
where we have used $\rho_\ast(a^{-1}Aa)= \rho(a^{-1})\rho_\ast(A) \rho(a)$. For reasons that will become clearer later let us write this as 
\eq{
\begin{pmatrix} 
\rho(a) & v \\
0 & 1
\end{pmatrix} \cdot (\alpha,p) = (\Ad_a^\ast \alpha + (a^{-1})^\ast p \odot v, (a^{-1})^\ast p)
}

\end{proof}
To make progress in understanding these coadjoint orbits, pick $p \in V^\ast$, consider $\mc{O}_p^H \subset V^\ast, \, H_p \subset H$, and the corresponding Lie algebra of the stabiliser $\mf{h}_p = \Lie(H_p) \subset \mf{h}$. 

\begin{lemma}
$B \in \mf{h}_p  \Leftrightarrow B^\ast p = 0$. (\hl{certainly I have $\Rightarrow$})
\end{lemma}
\begin{proof}
Let $b = \exp(B) \in H_p$. Then 
\eq{
b^\ast p = p &\Rightarrow \forall v \in V, \, \pangle{p,\rho(b)v} = \pangle{p,v} \\
&\Rightarrow \forall v \in V, \, \pangle{p,\rho_\ast(B)v} = 0 \quad \text{(taking the derivative)} \\
&\Rightarrow \forall v \in V, \, \pangle{B^\ast p,v} = 0 \\
&\Rightarrow b^\ast p = p \quad \text{\hl{(I think this is valid but can I present it better?)}}
}
\end{proof}

We let $\pi_p : H \twoheadrightarrow H_p$ be the projection onto the subgroup, giving $(\pi_p)_\ast: \mf{h} \twoheadrightarrow \mf{h}_p $ the projection onto the subspace. Hence $\pi_p^\ast : \mf{h}_p^\ast \to \mf{h}^\ast$, which we can see acts as $\pi_p^\ast \pround{\ev{\alpha}{\mf{h}_p}} = \alpha$. Hence we can state the following useful lemma:

\begin{lemma}
For $\alpha,\beta \in \mf{h}^\ast$, 
\eq{
\exists v \in V \, \text{ s.t. } \, \alpha - \beta = p \odot v \Leftrightarrow  (\pi_p^\ast)^{-1}\alpha = (\pi_p^\ast)^{-1} \beta
}
\end{lemma}
\begin{proof}
Recall for $A \in \mf{h}$, $\pangle{p \odot v, A} = \pangle{p,\rho_\ast(A)v}$. If $A \in \mf{h}_p$, it must be that $\forall v \in V, \, \pangle{p,\rho_\ast(A)v}=0$. Hence 
\eq{
\exists v \in V \, \text{ s.t. } \, \alpha - \beta = p \odot v \Rightarrow \forall A \in \mf{h}_p, \, \pangle{\alpha,A} &= \pangle{\beta,A} \\
\Leftrightarrow \qquad \qquad \quad \ev{\alpha}{\mf{h}_p} &= \ev{\beta}{\mf{h}_p}
}
Hence we see $p \odot V \leq \pi_p^\ast(0) $. By counting dimensions we see $\dim \pi_p^\ast(0) = \dim\mf{h}^\ast - \dim \mf{h}_p^\ast$. Furthermore, from the previous lemma, we now know we $\dim \mf{h} = \dim\mf{h}_p + \dim\image(B \mapsto B^\ast p)$. Hence 
\eq{
\dim \pi_p^\ast(0) = \dim\image(B \mapsto B^\ast p)
}
Now 
\eq{
\dim\image(B \mapsto B^\ast p) &= \dim\pbrace{B^\ast p \, | \, B \in \mf{h}} \\
&= \dim \pbrace{v \in V \, | \, \exists B \in \mf{h}, \, \pangle{B^\ast p, v} \neq 0} \\
&= \dim \pbrace{v \in V \, | \, \exists B \in \mf{h}, \, \pangle{p,\rho_\ast(B) v} \neq 0} \\
&= \dim \pbrace{v \in V \, | \, \exists B \in \mf{h}, \, \pangle{p\odot v,B} \neq 0} \\
&= \dim \pbrace{v \in V \, | \, p \odot v \neq 0} \\
&= \dim(p \odot V)
}
Hence by matching of dimensions $p \odot V = \pi_p^\ast(0)$ and we are done. 
\end{proof}

The above lemma means that the orbit of $(\alpha,p)$ over $\mc{O}_p^H$ is the same as that of $((\pi_p)_\ast \alpha,p)$. Hence in order to classify the orbits in $\mf{g}^\ast$ over $\mc{O}_p^H$ it is sufficient to classify orbits in $\mf{h}_p$ under $H_p$. 

\begin{example}
Consider our main example $H=SO(V)$, where $(V,\pangle{\cdot, \cdot})$ is an inner product space. The representation of $H$ acts via matrix multiplication. We may use $\pangle{\cdot, \cdot}$ to identify $V$ with $V^\ast$ as is natural, that is 
\eq{
V &\to V^\ast \\
x &\mapsto \pangle{x, \cdot}
}
This leads to the rather confusing notation, \hl{which you will have to learn to deal with until I come up with something better}, that 
\eq{
\pangle{\underbrace{p}_{\in V^\ast},\underbrace{v}_{\in V}} = \pangle{\underbrace{p}_{\substack{{\in V} \\ {\text{under}} \\ {\text{identification}}}}, \underbrace{v}_{\in V}}
}
Further, noting $\mf{h} = \pbrace{A \in M_3(\mbb{R}) \, | \, A^T + A = 0}$, we have a the identification 
\eq{
\wedge^2 V &\to \mf{h} \\
u \wedge w &\mapsto (A_{u\wedge w} : v \mapsto \pangle{w,v}u - \pangle{v,u}w)
}
In this case we see 
\eq{
\pangle{p,A_{u \wedge w}v} &= \pangle{p,\pangle{w,v}u - \pangle{v,u}w} \\
&= \pangle{p,u} \pangle{w,v} - \pangle{p,w} \pangle{v,u} \\
&= \det \begin{pmatrix} \pangle{p,u} & \pangle{p,w} \\ \pangle{v,u} & \pangle{v,w} \end{pmatrix} \\
\Rightarrow \pangle{p \odot v, A_{u \wedge w}}&= \pangle{p\wedge v, u \wedge w}
}
using the inner product induced on $\wedge^2V$ on the RHS. Under the identification of $\wedge^2 V, \mf{h}$, we get an induced identification 
\eq{
\mf{h}^\ast &\to (\wedge^2 V)^\ast \\
\alpha &\mapsto (\beta_\alpha:u \wedge w \mapsto \pangle{\alpha,A_{u \wedge w}})
}
and combining this with the identification of $(\wedge^2 V)^\ast, \wedge^2 V$, we get 
\eq{
\beta_{p \odot v} = p \wedge v
}
We note that that under the identification of $V,V^\ast $
\eq{
\pangle{(a^{-1})^\ast p,v} = \pangle{p,a^{-1}v} = \pangle{ap,v}
}
so finally seeing 
\eq{
(a^{-1} A_{u \wedge w} a)v &= \pangle{w,av}a^{-1}u - \pangle{av,u} a^{-1}w \\
&= \pangle{a^{-1}w,v}a^{-1}u - \pangle{v,a^{-1}u}a^{-1}w \quad \text{(using $a^T = a^{-1}$)}\\
&= A_{a^{-1}(u \wedge w)}v
}
and taking $\alpha \in \mf{h}^\ast$ we have 
\eq{
\pangle{\Ad_a^\ast\alpha,A_{u \wedge w}} &= \pangle{\alpha,a^{-1} A_{u \wedge w} a} \\
&= \pangle{\alpha, A_{a^{-1}(u \wedge w)}} \\
\Rightarrow \pangle{\beta_{\Ad_a^\ast\alpha}, u \wedge w} &= \pangle{\beta_\alpha,a^{-1}(u \wedge w)} \\
&= \pangle{a \beta_\alpha, u \wedge w} \\
\Rightarrow \beta_{\Ad_a^\ast\alpha} &= a \beta_\alpha
}
Hence using the identification of $A_{u \wedge w}, u \wedge w$, the coadjoint action of $SO(V)$ on $\mf{so}(V)^\ast$ becomes the standard action on $(\wedge^2 V)^\ast$. \\
Finally, 
Hence we can write the coadjoint action as 
\eq{
\begin{pmatrix} 
a & v \\
0 & 1
\end{pmatrix} \cdot \underbrace{(\alpha,p)}_{\in \mf{h}^\ast \oplus V^\ast} &= (\Ad_a^\ast \alpha + (a^{-1})^\ast p \odot v, (a^{-1})^\ast p) \\
\Rightarrow \begin{pmatrix} 
a & v \\
0 & 1
\end{pmatrix} \cdot \underbrace{(\beta_\alpha,p)}_{\in (\wedge^2 V) \oplus V} &= (\beta_{\Ad_a^\ast \alpha} + (a^{-1})^\ast \beta_{p \odot v}, (a^{-1})^\ast p) \\
&= (a \beta_\alpha + a p \wedge v , ap) 
}
\end{example}

\begin{example}
We may now consider the specific example of $V = \mbb{R}^3$ with the standard inner product from above, where we now get $G= E(3)$ is the group of Euclidean symmetries. For any $p \in V$ we can find an $a \in SO(3)$ s.t. $ap = k e_3$ where $k = \norm{p}$. Let us think about the possible cases:\\
\newline
$\underline{\bm{k=0}}$: The vector $p=0$, and so $\beta\mapsto a\beta$. This again splits into two cases\\
$\bm{\beta=0}$, and the coadjoint orbit is the zero dimensional orbit $\pbrace{(0,0)}$. \\
$\bm{\beta \neq 0}$, and the orbit through $(\beta,0)$ is the 2d sphere $S^2_{\norm{\beta}}\times\pbrace{0}$. (Hence there is a 1-parameter family of these orbits) \\
\newline 
$\underline{\bm{k\neq0}}$: We have $(\beta,p) \mapsto (\gamma,ke_3)$. Write
\eq{
v &= ae_1 + be_2 + ce_3  \\ 
\Rightarrow p \wedge v &= k(ae_3 \wedge e_1 + be_3 \wedge e_2) \\
\phantom{=} \\
\gamma &= xe_1 \wedge e_3 + ye_2 \wedge e_3 + s e_1 \wedge e_2 \\
\Rightarrow \gamma + p \wedge v &= (x-ka)e_1 \wedge e_3 + (y-kb) e_2 \wedge e_3 + s e_1 \wedge e_2
}
For any $\gamma,k$ we can then choose $x=ka, \, y=kb$ s.t. $\gamma + p \wedge v = s e_1 \wedge e_2$. Hence the coadjoint orbits for $p \neq 0$ are 4d (two rotations, and two translations non-parallel to $p$) and uniquely labelled by a point $(se_1 \wedge e_2, ke_3)$ (hence such orbits form a 2-parameter family). This point is preserved only by transformations $a=I, \, v = le_3$ for $l \in \mbb{R}$. We call the group of such transformations preserving the z-axis $E(3)_z$, and then the orbit is homeomorphic to $\faktor{E(3)}{E(3)_z}$. \hl{It turns out this looks like the space of straight lines in $\mbb{R}^3$}. 
\begin{remark}
Note that we can find $k$ as a function of $(\beta,p)$ as
\eq{
k(\beta,p) = \norm{p}
}
We can do the same for $s$. By observing 
\eq{
(a\beta + ap\wedge v) \wedge ap &= a\beta \wedge ap \\
&= \det(a) \beta \wedge p \\
&= \beta \wedge p \quad \text{(as $\det a = 1$)}
}
and noting $se_1 \wedge e_2 \wedge ke_3 = sk e_1 \wedge e_2 \wedge e_3$ we get 
\eq{
\beta \wedge p = s(\beta,p) k(\beta,p) e_1\wedge e_2\wedge e_3
}
\hl{This makes me think of arc length and curvature, is there any connection here? }
\end{remark}
Now recall back to example \ref{example:CQIS:CoadjointOrbitSymplectic} where we calculated the symplectic form on generic coadjoint orbits. We defined for $X \in \mf{g}$ the map $l_X\in \mf{g}^\ast$ s.t. for $\alpha \in \mf{g}^\ast, \, l_X(\alpha) = \alpha(X)$. If we look at the $p=0, \beta \neq 0$ orbit, we see we reduce to coadjoint orbits of $H$. \\
Let us call the three generators of $\mf{h} = \mf{so}_3$ $L_i$ so the algebra is $\comm[L_i]{L_j} = \eps_{ijk}L_k$, and an inner product is given by $\pangle{L_i,L_j} = \delta_{ij}$. Under the identification of $\mf{h}, \wedge^2 V$ we see $L_1 \leftrightarrow e_2 \wedge e_3$, etc. If we pick the orbit through $re_3 \wedge e_1$, we see the Lie algebra of the stabilising subgroup is $\spn L_2$. Choosing coordinates on the orbit s.t. $\del_\theta = \xi_{L_1}, \, \del_\phi = \xi_{L_3}$ we get 
\eq{
\omega(\del_\theta,\del_\theta) &= \pangle{rL_2, \comm[L_1]{L_1}} = 0 \\
\omega(\del_\phi,\del_\phi) &= \pangle{rL_2, \comm[L_3]{L_3}} = 0 \\
\omega(\del_\phi,\del_\theta) &= \pangle{rL_2,\comm[L_3]{L_1}} = r
}
These combined mean 
\eq{
\omega = rd\phi \wedge d\theta 
}
\hl{Do this for the 4d orbit too, and formalise what is above. Note we need to be using the cocommutator which is what this section is about}
\end{example}

The above example demonstrates in practice a useful result:

\begin{theorem}[Kostant-Souriau]
If $H^1(\mf{g}) = 0 = H^2(\mf{g})$, then up to coverings every homogeneous symplectic manifold for a group $G$ is an orbit of $G$ acting on $\mf{g}^\ast$. 
\end{theorem}

The parameters of the orbits will then correspond to parameters of the possible symplectic structures on the underlying manifold. 

\begin{example}
To continue on the above example let us now consider $N$ classical particles in $\mbb{R}^3=V$. Each particle has a position $q_i\in \mbb{R}^3$ and a momentum $p_i\in \mbb{R}^3$. We represent the state of this system as an element of the phase space $z=(q_1, \dots, q_N,p_1, \dots, p_N) \in \mbb{R}^{6N}=M$. \\
As seen previously this has a natural action from $E(3)$ induced from the action on the configuration space as 
\eq{
\begin{pmatrix} a & v \\ 0 & 1 \end{pmatrix} \cdot z = (aq_1 + v, \dots, aq_N + v,ap_1, \dots, ap_N)
}
Let us define the linear momenta $p: M \to \mbb{R}$ and angular momenta $\beta: M \to \mbb{R}$ by 
\eq{
p(z) &= p_1 +\dots+ p_N \\
\beta(z) &= p_1 \wedge q_1 +\dots+ p_N \wedge q_N 
}
With this we may define a map $\Phi:M \to (\wedge^2 V) \oplus V$ (where we will want to identify this image with $\mf{e}_3^\ast$).
\eq{
\Phi(z) = (\beta(z),p(z))
}
By the previous example we have, calling $b=\begin{psmallmatrix} a & v \\ 0 & 1 \end{psmallmatrix}$,
\eq{
b \cdot \Phi(z) &= (a\beta(z) + ap(z)\wedge v,ap(z)) \\
&= \pround{ap_1 \wedge aq_1 + \dots ap_N \wedge aq_N + ap_1\wedge v + \dots +ap_N\wedge v,ap_1 + \dots ap_N} \\
&= (ap_1 \wedge (aq_1 + v) + \dots +ap_N\wedge (aq_N + v), ap_1 + \dots ap_N) \\
&=(\beta(a\cdot z),p(a \cdot z)) = \Phi(a \cdot z)
}
\begin{remark}
Note what we have just done is constructed the moment map, and then shown its equivariance. 
\end{remark}

Now restrict to the case $N=1$. Since $\norm{p}$ is preserved under the action of $E(3)$ the orbit of the point $(q,p)$ with $p \neq 0$ is 5-dimensional. By the equivariance of $\Phi$ is must be that $\Phi(G\cdot(q,p))$ is one orbit in $\mf{e}_3^\ast$ and we calculate 
\eq{
k = \norm{p} &\neq 0 \\
0 = (p \wedge q) \wedge q = sk e_1 \wedge e_2 \wedge e_3 \Rightarrow s &= 0
}
to find it. For a point in phase space $(q^\prime,p^\prime)$ to lie in the same orbit as $(q,p)$, it must then be that 
\eq{
\Phi(q^\prime,p^\prime) = \Phi(q,p) 
&\Rightarrow p=p^\prime \quad \text{and} \quad p \wedge q = p \wedge q^\prime  \\
&\Rightarrow q-q^\prime \parallel p \\
\phantom{=} \\
&\Rightarrow \Phi^{-1}(\Phi(q,p)) = \pbrace{(q+tp,p) \,| \, t \in \mbb{R}}
}
As such we interpret these as trajectories of a particle with momentum $p$, and we see the coadjoint orbits in $\mf{e}_3^\ast$ as being the space of all possible trajectories. Because this is a classical particle, it has no internal degree of freedom, and more specifically it has no spin. The existence of $s \neq 0$ should suggest that particles with intrinsic angular momentum could exists, and indeed they do. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Co-boundary Poisson Lie groups}
Let $\mf{g}$ be a Lie algebra, and we want to construct the co-commutator. Let 
\eq{
\delta(X) = (\ad_X \otimes 1 + 1 \otimes \ad_X)r
}
for some $r \in \mf{g} \otimes \mf{g}$. Write this as $r = \sum_a X_a \otimes Y_a$, and then 
\eq{
\delta(\comm[X]{Y}) &= (\ad_{\comm[X]{Y}} \otimes 1 + 1 \otimes \ad_{\comm[X]{Y}})r \\
&= \comm[{\comm[X]{Y}}]{X_a} \otimes Y_a + X_a \otimes \comm[{\comm[X]{Y}}]{Y_a} \\
&= (\comm[{\comm[X]{X_a}}]{Y} + \comm[X]{\comm[Y]{X_a}}) \otimes Y_a + X_a \otimes (\comm[{\comm[X]{Y_a}}]{Y} + \comm[X]{\comm[Y]{Y_a}} \\
&= (\ad_X \otimes 1 + 1 \otimes \ad_X)\delta(Y) - (\ad_Y \otimes 1 + 1 \otimes \ad_Y)\delta(X)
}
so we can see that $\delta$ automatically satisfies the co-cycle condition.
\begin{idea}
If this form of $\delta$ automatically solves the cocycle condition, it may be a useful approach to finding the Lie bialgebra structure to look for certain kinds of these $r$. As saw in prop \ref{prop:CQIS:naturalr} such an $r$ which makes $\delta$ satify the other properties can always be found in a double algebra.
\end{idea}
Now let $r^\prime = \sum_a Y_a \otimes X_a$, and then write 
\eq{
\comm[[r]{r]} = \comm[r_{12}]{r_{13}} + \comm[r_{12}]{r_{23}} + \comm[r_{13}]{r_{23}}
}
where 
\eq{
\comm[r_{12}]{r_{13}} &= \sum_{a,b} \comm[X_a]{X_b} \otimes Y_a \otimes Y_b \\
\comm[r_{12}]{r_{23}} &= \sum_{a,b} X_a \otimes \comm[Y_a]{X_b} \otimes Y_b \\
\comm[r_{13}]{r_{23}} &= \sum_{a,b} X_a \otimes X_b \otimes \comm[Y_a]{Y_b}
}
This means we can think schematically (or more precisely as an element of the universal enveloping algebra) of these as 
\eq{
r_{12} &= \sum_a X_a \otimes Y_a \otimes 1 \\
r_{13} &= \sum_a X_a \otimes 1 \otimes Y_a \\
r_{23} &= \sum_a 1 \otimes X_a \otimes Y_a 
}

\begin{lemma}
$\delta(X) = (\ad_X \otimes 1 + 1 \otimes \ad_X) r$ defines a co-commutator of a Lie bialgebra iff
\begin{enumerate}
    \item $r + r^\prime$ is an invariant element in $\mf{g} \otimes \mf{g}$
    \item $\comm[[r]{r]}$ is an invariant element in $\mf{g} \otimes \mf{g} \otimes \mf{g}$. 
\end{enumerate}
\end{lemma}
Here, invariant mean $\Ad$ invariants, in the sense that if we call $\comm[[r]{r]} = -\omega$, then we need $\forall g \in G,\, (\Ad_g \otimes \Ad_g \otimes \Ad_g) \omega = \omega$. We may differentiate to get the condition 
\eq{
\pround{\ad_X \otimes 1 \otimes 1 + 1 \otimes \ad_X \otimes 1 + 1 \otimes 1 \otimes \ad_X}\omega = 0
}

\begin{aside}
We might at this point be asking how we could remember this formula. We will use ideas developed in the quantum part to develop this. 
\begin{center}
\begin{tikzpicture}
\draw (0,4.5) node[anchor = south east] {2} -- (0,-0.5) ;
\draw (-1,4) node[anchor = south east] {1} -- (2,0) ; 
\draw (-1,0) -- (2,4) node[anchor = south east] {3} ;
\draw (3,2) node {=};
\draw (6,4.5) node[anchor = south east] {2} -- (6,-0.5) ;
\draw (4,4) node[anchor = south east] {1} -- (7,0) ; 
\draw (4,0) -- (7,4) node[anchor = south east] {3} ;
\end{tikzpicture}
\end{center}
The diagram represets the QYBE equation, and says (ignoring spectral parameters)
\eq{
R_{12}R_{13}R_{23} = R_{23}R_{13}R_{12}
}
where what we have done is read the intersections from top to bottom, and put the index of the left line first. If we let $R = 1 + \hbar r$, where $\hbar$ is a (suspiciously physics-y) formal parameter, then the QYBE gives $\dcomm[r]{r} = 0 $  
\end{aside}

\begin{definition}[Yang-Baxter equation]\label{def:CQIS:YBE}
The equation $\comm[[r]{r]}=0$ is called the \bam{Classical Yang Baxter Equation (CYBE)}. The equation $\comm[[r]{r]}=-\omega$, where $\omega$ is $\ad$-invariant, is called the \bam{Modified CYBE (MCYBE)}. A bialgebra structure obtained from a solution of the (M)CYBE is called \bam{(quasi)}-\bam{triangular},  
\end{definition}

\begin{lemma}
If $r \in \mf{g} \otimes \mf{g}$ is antisymmetric and $s \in \mf{g} \otimes \mf{g}$ is $\ad$-invariant then 
\eq{
\comm[[r+s]{r+s]} &= \dcomm[r]{r} + \dcomm[s]{s} \\
\dcomm[s]{s} &\in \mf{g} \otimes \mf{g} \otimes \mf{g} \text{ is $\ad$-invariant}
}
\end{lemma}
\begin{corollary}
$r \in \mf{g} \otimes \mf{g}$ solves CYBE and $r + r^\prime$ is ad-invariant iff $\frac{1}{2}(r^\prime-r)$ solves MCYBE and is antisymmetric.  
\end{corollary}
\begin{proof}
Let $\tilde{r} = \frac{1}{2}(r^\prime - r), \, s = \frac{1}{2}(r^\prime + r)$. Then $\tilde{r}$ is antisymmetric, and $\tilde{r}+s = r$. 
\end{proof}

\begin{definition}
Given $\mf{g}$ with a non-degenerate, invariant, symmetric inner product, with basis $X_a$, the $\ad$-invariant symmetric element 
\eq{
t = t^{ab} X_a \otimes X_b
}
where $t_{ab} = (X_a, X_b)$ is called a \bam{Casimir}. 
\end{definition}

\begin{example}
Recall $d(\mf{su}_2) = \Lie(E_3)$ with trivial $\delta$. We has algebra 
\eq{
\comm[J_a]{J_b} &= \eps_{abc} J_c \\
\comm[P_a]{P_b} &= 0 \\
\comm[J_a]{P_b} &= \eps_{abc} P_c 
}
We had pairing $\pangle{P_a,J_b} = \delta_{ab}$, so these form a dual basis, and so we can take $r = P_a \otimes J_a$, and it turns out 
\begin{enumerate}
    \item $r + r^\prime = P_a \otimes J_a + J_a \otimes P_a$ is ad-invariant (Note that $J$ invariance should be clear as $P,J$ transform the same way under it.
    \item $r$ is a solution of the CYBE. 
\end{enumerate}
We can show 2: 
\eq{
\comm[r_{12}]{r_{13}} &= 0 \\
\comm[r_{12}]{r_{23}} &= \sum_{a,b} P_a \otimes \underbrace{\comm[J_a]{P_b}}_{\eps_{abc}P_c} \otimes J_b = \sum_{a,b,c} \eps_{abc} P_a \otimes P_c \otimes J_b \\
\comm[r_{13}]{r_{23}} &= \sum_{a,b} P_a \otimes P_b \otimes \underbrace{\comm[J_a]{J_b}}_{\eps_{abc}J_c} = \sum_{a,b,c} \eps_{acb} P_a \otimes P_c \otimes J_b \\
}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Skylanin bracket}

\begin{definition} The poisson bracket given by the PL structure $\Pi^R_g = (\Ad_g \otimes \Ad_g)r - r$  on $G$ if $r+ r^\prime, \dcomm[r]{r}$ are ad-invariant is called the \bam{Skylanin bracket}.  
\end{definition}

\begin{ex}
Show this satisfies the co-cycle conditions 
\end{ex}

\begin{prop}
The Skylanin bracket of $f_1,f_2 \in C^\infty(G)$ is 
\eq{
\acomm[f_1]{f_2}(g) &= \sum_{a,b} r^{ab}\pround{\xi_a^L f_1 \xi^L_b f_2 - \xi^R_a f_1 \xi^R_b f_2}(g)
}
where $\pbrace{X_a}$ is a basis, $r = \sum_{a,b} r^{ab} X_a \otimes X_b$, and 
\eq{
(\xi^L_a f)(g) &= \ev{\frac{d}{dt} f(ge^{tX_a})}{t=0} \\
(\xi^R_a f)(g) &= \ev{\frac{d}{dt} f(e^{-tX_a}g)}{t=0}
}
\end{prop}
\begin{proof}
Recall 
\eq{
\Pi_g &= (R_g)^\prime_e \otimes (R_g)^\prime_e \Pi^R_g \\
&=(R_g)^\prime_e \otimes (R_g)^\prime_e \pround{(\Ad_g \otimes \Ad_g)r - r} 
}
As $\Ad_g = (R_{g^{-1}} L_g)^\prime_e = (R_{g^{-1}})^\prime_g (L_g)^\prime_e$ we get 
\eq{
\Pi_g &= (L_g)^\prime_e \otimes (L_g)^\prime_e r - (R_g)^\prime_e \otimes (R_g)^\prime_e r \\
&= r^{ab} \psquare{(L_g)^\prime_e X_a \otimes (L_g)^\prime_e X_b - (R_g)^\prime_e X_a \otimes (R_g)^\prime_e X_b}
}
and this yields the correct bracket. 
\end{proof}

\begin{example}
Consider $G = GL_n(\mbb{R})$. Then $t_{ab} : G \to \mbb{R}, \, A \mapsto A_{ab}$. Then 
\eq{
\acomm[t_{ij}]{t_{kl}} = \sum_{a,b} r^{ajbl} t_{ia} t_{kb} - r^{iakb}t_{aj}t_{bl}
}
Here we have written $r$ in terms of the basis $\pbrace{X_{ij}}$ where $(X_{ij})_{kl} = \delta_{ik}\delta_{jl}$ s.t. $r = \sum r^{ijkl} X_{ij} \otimes X_{kl}$. This is as 
\eq{
\xi^L_{ab} t_{ij}(g) = t_{ij}(gX_{ab})
}
\end{example}

Often we will use the shorthand notation for the bracket $\acomm[T]{T} = \comm[T\otimes T]{r}$, where $T = (t_{ij})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Integrable systems from r-matrices}

\begin{definition}
A \bam{Lax pair} for a Hamiltonian system $(M,\omega,H)$ is a pair of functions $P,L : M \to \mf{g}$ s.t.
$ \acomm[L]{H} = \comm[L]{P}$. \\
If we consider $H$ as generating flow with parameter $s$ on $M$, we write this as $\frac{dL}{ds} = \comm[L]{P}$. 
\end{definition}

\begin{prop}
The Lax equation has the formal solution 
\eq{
L(s) = \exp(-sP) L(0) \exp(sP)
}
\end{prop}

Note that ad-invariant functions $f: \mf{g} \to \mbb{R}$ give conserved quantities via $f \circ L$. We now want to ask what makes these in involution. Define a map $\rho : \mf{g} \to \mf{g}$ by 
\eq{
\forall X, Y \in \mf{g}, \, (\rho(X),Y) = (r, X \otimes Y)
}
Then 
\eq{
\comm[X]{Y}_r \equiv \comm[\rho(X)]{Y} + \comm[X]{\rho(Y)}
}
defines a Lie bracket. 

\begin{ex}
Note $\mf{sl}_2(\mbb{R}) \cong \mf{su}_{2,1}$. Take the algebra 
\eq{
\comm[J_a]{J_b} = \eps_{abc} J^c
}
where $\eta^{ab} = \diag(1,-1,-1)$. Then $(J_a,J_b) = \eta_{ab}$ is a Lorentzian inner product. We then have a solution to the CYBE 
\eq{
r = J_0 \otimes J_2 - J_2 \otimes J_0
}
and we get 
\eq{
\comm[J_0]{J_2}_r &= 0 \\
\comm[J_1]{J_0}_r &= J_0 \\
\comm[J_1]{J_2}_r &= J_2
}
\end{ex}

We now have two Poisson brackets on $\mf{g}^\ast$: 
\eq{
\acomm[f_1]{f_2}(\alpha) &= \pangle{\comm[(df_1)_\alpha]{(df_2)_\alpha},\alpha} \\
\acomm[f_1]{f_2}_r(\alpha) &= \pangle{\comm[(df_1)_\alpha]{(df_2)_\alpha}_r,\alpha}
}
which are both KKS brackets. 

\begin{prop}
$\Ad^\ast$-invariant functions on $\mf{g}^\ast$ are in involution wrt both brackets. 
\end{prop}
\begin{proof}
We will take the convention $\pangle{\ad_X^\ast \alpha,Y} = -\pangle{\alpha,\ad_X Y}$. \hl{This differs from that taken in the typed notes, but does not affect the proof}. Then 
\eq{
\pangle{\comm[(df_1)_\alpha]{(df_2)_\alpha},\alpha} &= - \pangle{(df_2)_\alpha, \ad^\ast_{(df_1)_\alpha}\alpha} \\
&= 0 \text{ if $f_2$ is ad-invariant}
}
This is as $\Ad^\ast$-invariance of a function $f:\mf{g}^\ast \to \mbb{R}$ means 
\eq{
\forall g \in G, \, \forall \alpha \in \mf{g}^\ast, \, f(\Ad_g^\ast \alpha) &= f(\alpha) \\
\Rightarrow \text{for } g = \exp(tX), \, \pangle{df_\alpha, \ad_X^\ast \alpha} &= 0
}
Hence $\ad^\ast$-invariant functions Poisson commute with all functions wrt $\acomm[\cdot]{\cdot}$. Similarly 
\eq{
\acomm[f_1]{f_2}_r &=  \pangle{\comm[(df_1)_\alpha]{(df_2)_\alpha}_r,\alpha} \\
&= \pangle{\pround{\comm[\rho((df_1)_\alpha)]{(df_2)_\alpha} + \comm[(df_1)_\alpha]{\rho((df_2)_\alpha)}},\alpha} \\
&= - \pangle{(df_2)_\alpha, \ad^\ast_{\rho((df_1)_\alpha)}\alpha} + \pangle{(df_1)_\alpha, \ad^\ast_{\rho((df_2)_\alpha)}\alpha} \\
&= 0 \quad \text{ if both $f_1,f_2$ $\Ad^\ast$-invariant}
}
\end{proof}

\begin{remark}
A corollary of the proof of the above is that if $H$ is $\Ad^\ast$-invariant, generates trivial evolution wrt the bracket $\acomm[\cdot]{\cdot}$, but wrt $\acomm[\cdot]{\cdot}_r$, for which 
\eq{
\dot{f} = \acomm[f]{H}_r = \pangle{df_\alpha, \ad^\ast_{\rho(dH_\alpha)}\alpha}
}
\end{remark}

\begin{prop}
Let $L,P: \mf{g}^\ast \to \mf{g}$ be given by 
\eq{
L(\alpha) &= (\alpha \otimes 1)(t) = \sum_{a,b} t^{ab} \alpha(X_a)X_b \\
P(\alpha) &= \rho((dH)_\alpha)
}
where $t$ is the standard Casimir. Then $L,P$ are a Lax pair.  
\end{prop}
\begin{proof}
Note that for $\alpha \in \mf{g}^\ast$, $\dot{\alpha} = \pangle{d(\id)_\alpha, \ad^\ast_{\rho(dH_\alpha)}\alpha} = \ad_{P(\alpha)}^\ast(\alpha)$. Then 
\eq{
\dot{L}(\alpha) &= (\dot{\alpha} \otimes 1)(t) \\
&= (\ad_{P(\alpha)}^\ast \otimes 1)(t) \\
&= (\alpha \otimes 1)\comm[P(\alpha) \otimes 1]{t}
}
By $\ad$-invariance of $t$, we know 
\eq{
\comm[P(\alpha) \otimes 1]{t} = -\comm[1 \otimes P(\alpha)]{t} = \comm[t]{1 \otimes P(\alpha)}
}
and so 
\eq{
\dot{L}(\alpha) = (\alpha \otimes 1)\comm[t]{1 \otimes P(\alpha)} = \sum_{a,b} t^{ab} \alpha(X_a) \comm[X_b]{P(\alpha)} = \comm[L]{P}(\alpha)
}
\end{proof}

\begin{remark}
Once again we hit a point where we have developed a nice large amount of theory and want to know why. Having a Lax pair is a very useful property because of the following lemma:

\begin{prop}
$\forall k \geq 1, \, \tr (L^k)$ is a conserved quantity. 
\end{prop}
\begin{proof}
\eq{
\frac{d}{dt} \tr(L^k) &= \tr(\dot{L} L^{k-1}) \\
&= \tr(\comm[L]{P}L^{k-1}) \\
&= 0 \quad \text{by trace properties}
}
\end{proof}
This is called the \bam{isospectral property}. It gives you a tower of independent conserved quantities. It is also very useful why solving PDEs which have a Lax pair, by the method of \bam{inverse scattering} (which I may write about later). Moreover, if your Lax pair has an addition degree of freedom in the form of a spectral parameter, you can construct what is called a \bam{spectral curve}, which can allow you to construct your integrable system in reverse. To learn more about this area I can recommend Babelon's Introduction to Classical Integrable System \cite{Babelon2003}. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Quantum}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
We want to now go from classical mechanics to quantum mechanics. This will look like:

\begin{center}
    \begin{tabular}{c|c}
    Classical & Quantum \\ \hline \hline
    Phase space $M$, a symplectic manifold & State space,  $\mc{H}$ a Hilbert space \\
    \hline 
    Observables $A:M \to \mbb{R}$ & Observables $A \in \End(\mc{H})$ \\ \hline
    Evolution $\dot{A} = \acomm[A]{H}$ & Evolution $\dot{A} = \frac{-i}{\hbar}\comm[A]{H}$
    \end{tabular}
\end{center}

\begin{remark}
\hl{Maybe I want to mention how people tried to introduce canonical quantisation by upgrading the Poisson bracket, and the corresponding Groenewold's theorem}
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The corresponding structures that give integrability are 

%\begin{center}
%    \begin{tabular}{c|c}
%    Classical Integrability & Quantum Integrability \\ \hline \hline 
%        Lax pair & RTT relation \\ \hline 
        
%    \end{tabular}
%\end{center}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Algebraically this upgrading occurs as 

\begin{center}
    \begin{tabular}{c|c}
    Classical & Quantum \\ \hline \hline
    Lie algebra $\mf{g}$ & Universal enveloping algebra $\mc{U}(\mf{g})$ \\ \hline 
    Lie bialgebra with CYBE & quasi-triangular Hopf algebra with QYBE
    \end{tabular}
\end{center}

An alternate way of motivating is to consider \bam{Schr\"odinger's equation}
\eq{
i \hbar \pd[\psi]{t} = \mc{H} \psi
}
for quantum Hamiltonian 
\eq{
\mc{H} = \frac{1}{2} \sum_j \pround{-i\hbar \pd{q_j}}^2 + V
}
Here $\psi$ is a state in some Hilbert space $\mc{S}$ and this system depends on the coordinates $q_j$ on manifold $M$. This system has the formal solution $\psi(t) =\mc{U}(t) =  \exp\pround{-\frac{it}{\hbar}\mc{H}}$. Then if $\mc{F}$ is another operator on $\mc{S}$ then $\mc{F}(t) = \mc{U}(t)^{-1} \mc{F} \mc{U}(t)$ satisfies 
\eq{
\dot{\mc{F}}(t) = -\frac{i}{\hbar}
	 \comm[\mc{F}(t)]{\mc{H}}
}
This setup is clearly analogous to classical Hamiltonian mechanics. To now understand the analogue of Liouville integrability, we remark that if we have a discrete eigenbasis for $\mc{H}$ then we can solve explicitly. Now if we have operator $\mc{F}$ s.t. $\comm[\mc{F}]{\mc{H}}=0$ we know that $\mc{F}$ will preserve eigenspaces of $\mc{H}$. We can then state a definition:

\begin{definition}
	A \bam{quantum integrable system} is a set of $\dim M$ operators $\mc{H}_i$ acting on $\mc{S}$ that pairwise commute and are algebraically independent. 
\end{definition}
This definition is hard to check so we need a procedure:
\begin{definition}
	The \bam{reduction} of an operator $\mc{S}$ is a function on $M\times \mbb{R}^n$ gained by replacing $-i\hbar\pd{q_j}$ with $p_j$ and letting $\hbar \to 0$. 
\end{definition}
We can now modify the condition in the definition of quantum integrable systems to ask that the reductions of the $\mc{H}_i$ are functionally independent. 

\begin{example}
	We can have $\mc{S} = L^2(\mbb{R}^n,dq)$, and take $\mc{H}_j = -i\hbar \pd{q_j}$. The corresponding reductions are $H_j = p_j$. 
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Yangians and the RTT relation}

\begin{definition}
Let $\mf{g}$ be a Lie algebra, then the corresponding \bam{polynomial current Lie algebra} is 
\eq{
\mf{g}[x] = \mf{g} \otimes_{\mbb{C}} \mbb{C}[x]
}
with bracket 
\eq{
\forall x,y \in \mf{g}, \, f,g \in \mbb{C}[x], \, \comm[x \otimes f]{y \otimes g} = \comm[x]{y} \otimes (fg)
}
\end{definition}

\begin{prop}
The polynomial current Lie algebra is indeed a Lie algebra
\end{prop}
\begin{proof}
\begin{ex}
Show this. 
\end{ex}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\secmath{\mf{gl}_n} as an RTT-algebra}

Consider $\mf{g} = \mf{gl}_N$, and take the basis $E_{ij}$ of $N \times N$-matrices having the only non-zero entry be a 1 in the $i-j$ entry. 

\begin{fact}
This basis has commutation relations 
\eq{
\comm[E_{ij}]{E_{kl}} = E_{ij}E_{kl} -E_{kl}E_{ji} = \delta_{jk}E_{il} - \delta_{il} E_{kj}
}
\end{fact}

\begin{definition}
The \bam{tensor algebra} of $\mf{g}$ is 
\eq{
\mc{T}(\mf{g}) = \bigoplus_{k \geq 0} \mf{g}^{\otimes k}
}
where we define $\mf{g}^{\otimes 0} = \mbb{C}$. We have multiplication $m:\mc{T} \otimes \mc{T} \to \mc{T}$ given simply by taking the tensor product of the elements in the algebra, that is 
\eq{
m(X,Y) = X \otimes Y 
}
\end{definition}

\begin{definition}[Universal Enveloping Algebra]
The \bam{universal enveloping algebra} of $\mf{g}$ is
\eq{
\mc{U}(\mf{g}) = \faktor{\mc{T}(\mf{g})}{I}
}
where $I$ is the 2-sided ideal generated by the elements of the form $x \otimes y - y \otimes x - \comm[x]{y}$. 
\end{definition}

\begin{remark}
This is the most general unital associative algebra containing all representations of $\mf{g}$. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The quantum Yang-Baxter equation and RTT relation}

For the rest of this section we will now set $V = \mbb{C}^N$;

\begin{definition}
The map $P = \sum_{ij}, E_{ij} \otimes E_{ji} : V \otimes V \to V \otimes V$ which acts as $ v \otimes w \mapsto w \otimes v$ is called the \bam{flip operator}
\end{definition}

Consider $R = \sum_i A_i \otimes B_i$. We define 
\eq{
R_{12} = \sum_i A_i \otimes B_i \otimes 1 \\
R_{13} = \sum_i A_i \otimes 1 \otimes B_2 \\
R_{23} = \sum_i 1 \otimes A_i \otimes B_i
}
\begin{definition}
The \bam{quantum Yang Baxter equation (QYBE)} for $R\in \End(V \otimes V) \cong (\End V)^{\otimes 2}$ is 
\eq{
R_{12}R_{13}R_{23} = R_{23} R_{13} R_{12}
}
in $\End(V \otimes V \otimes V \otimes \mbb{C}(u,v))$. 
\end{definition}

\begin{prop}
Choose a basis $V = \oplus_{a=1}^N \mbb{C} v^a$. In terms of matrix elements, writing $R v^a \otimes v^b = \sum_{c,d} R^{ab}_{cd} v^c \otimes v^d$ The QYBE is 
\eq{
\sum_{\alpha,\beta,\gamma} R^{bc}_{\beta \gamma} R^{a \gamma}_{\alpha f} R_{de}^{\alpha\beta} = \sum_{\alpha,\beta,\gamma} R^{ab}_{\alpha\beta} R^{\alpha c}_{d \gamma} R^{\beta \gamma}_{ef} 
}
\end{prop}
\begin{proof}
Write 
\eq{
R_{12}R_{13}R_{23} v^a \otimes v^b \otimes v^c = R_{23} R_{13} R_{12}v^a \otimes v^b \otimes v^c
}
and then expand out the action, using for example 
\eq{
R_{23} v^a \otimes v^b \otimes v^c = R^{bc}_{\beta \gamma} v^a \otimes v^\beta \otimes v^\gamma
} 
\end{proof}

\begin{remark}[Graphical interpretation of QYBE]
Recall I drew the somewhat cryptic picture 
\begin{center}
\begin{tikzpicture}
\draw (0,2) node[anchor = south east] {2} -- (0,0) ;
\draw (-1,2) node[anchor = south east] {1} -- (2,0) ; 
\draw (-1,0) -- (2,2) node[anchor = south east] {3} ;
\draw (3,1) node {=};
\draw (6,2) node[anchor = south east] {2} -- (6,0) ;
\draw (4,2) node[anchor = south east] {1} -- (7,0) ; 
\draw (4,0) -- (7,2) node[anchor = south east] {3} ;
\end{tikzpicture}
\end{center}
if we label the internal states, and sum over all their possible values, i.e.

\begin{center}
\begin{tikzpicture}

\draw[->] (0,2) node[anchor = south] {b} -- (0,0)node[anchor = north] {e} ;
\draw[->] (-1,2) node[anchor = south east] {a} -- (2,0) node[anchor = north west] {d};
\draw[->] (2,2) node[anchor = south east] {c} -- (-1,0) node[anchor = north east] {f};

\draw (0,1) node[anchor = east] {$\beta$};

\draw (0.25,1.2) node[anchor = south] {$\alpha$};

\draw (0.25,0.8) node[anchor = north] {$\gamma$};

\draw (3,1) node {=};

\draw[->] (6,2) node[anchor = south] {b} -- (6,0) node[anchor = north] {e} ;
\draw[->] (4,2) node[anchor = south east] {a} -- (7,0) node[anchor = north west] {d} ;
\draw[->] (7,2) node[anchor = south west] {c} -- (4,0) node[anchor = north east] {f};

\draw (6,1) node[anchor = west] {$\beta$};

\draw (5.75,1.2) node[anchor = south] {$\gamma$};

\draw (5.75,0.8) node[anchor = north] {$\alpha$};

\end{tikzpicture}
\end{center}

we get this indexed QYBE 
\end{remark}

\begin{prop}
$R(u) = 1 - u^{-1}P \in \End V \otimes \End V \otimes \mbb{C}(u)$ is a solution of the QYBE
\eq{
R_{12}(u-v) R_{13}(u) R_{23}(v) = R_{23}(v) R_{13}(u)R_{12}(u-v)
}
\end{prop}
\begin{proof}

Take $x,y,z \in \mbb{C}^N$. Then 
\eq{
R_{23}(v) (x \otimes y \otimes z) =& x \otimes y\otimes z + v^{-1} x \otimes z \otimes y \\
\Rightarrow R_{13}(u)R_{23}(v) (x \otimes y \otimes z) =&  x \otimes y\otimes z + v^{-1} x \otimes z \otimes y + u^{-1} z \otimes y \otimes x  + (uv)^{-1}y \otimes z \otimes x \\
\Rightarrow R_{12}(u-v) R_{13}(u) R_{23}(v)(x \otimes y \otimes z) =& x \otimes y\otimes z + v^{-1} x \otimes z \otimes y + u^{-1} z \otimes y \otimes x  + (uv)^{-1}y \otimes z \otimes x \\
& + (u-v)^{-1} y \otimes x \otimes z  + [(u-v)v]^{-1} z \otimes x \otimes y  + [(u-v)u]^{-1} y \otimes z \otimes x \\
&+ [(u-v)uv]^{-1} z \otimes y \otimes x \\
=& x \otimes y \otimes z + v^{-1} x \otimes z \otimes y + (u-v)^{-1} y \otimes x \otimes z + [(u-v)v]^{-1} y \otimes z \otimes x \\
&+ [(u-v)v]^{-1} z \otimes x \otimes y + (u^{-1} + [(u-v)uv]^{-1})z \otimes y \otimes x  
}
Alternatively 
\eq{
R_{12}(u-v)(x \otimes y \otimes z) =& x \otimes y \otimes z + (u-v)^{-1} y \otimes x \otimes z \\
\Rightarrow R_{13}(u) R_{12}(u-v) (x\otimes y \otimes z) =& x \otimes y \otimes z + (u-v)^{-1} y \otimes x \otimes z + u^{-1} z \otimes y \otimes x + [(u-v)u]^{-1} z \otimes x \otimes y \\
\Rightarrow R_{23}(v) R_{13}(u) R_{12}(u-v) (x \otimes y \otimes z) =& x \otimes y \otimes z + (u-v)^{-1} y \otimes x \otimes z + u^{-1} z \otimes y \otimes x + [(u-v)u]^{-1} z \otimes x \otimes y \\
& + v^{-1} x \otimes z \otimes y + [(u-v)v]^{-1} y \otimes z \otimes x + (uv)^{-1} z \otimes x \otimes y \\
&+ [(u-v)uv]^{-1} z \otimes y \otimes x \\
=& x \otimes y \otimes z + v^{-1} x \otimes z \otimes y + (u-v)^{-1}y \otimes x \otimes z + [(u-v)v]^{-1} y \otimes z \otimes x \\
&+ [(u-v)v]^{-1} z \otimes x \otimes y  + (u^{-1} + [(u-v)]uv)^{-1} z\otimes y \otimes x 
}
and hence done
\end{proof}

\begin{remark}
The proof above actually shows $R(u) = 1 + u^{-1} P$ is a solution, which saves on watching minus signs, but note they are equivalent as I can absorb the minus sign into the parameter $u$.
\end{remark}


Let $T(u) = 1 + u^{-1}\sum_{ij} E_{ij} \otimes E_{ij} \in \End V \otimes \mc{U}(\mf{gl}_N) \otimes \mbb{C}(u)$. We can think of it as an $N \times N$ matrix with non-commutative entries $t_{ij}(u) = \delta_{ij} + u^{-1}E_{ij}$. 

\begin{prop}
Set $T_1(u) = \sum_{ij}E_{ij} \otimes 1 \otimes t_{ij}(u), \, T_2(u) = \sum_{ij} 1 \otimes E_{ij} \otimes t_{ij}(u)$. Then 
\eq{
R_{12}(u-v)T_1(u) T_2(v) = T_2(v) T_1(u) R_{12}(u-v) \; \in \End V \otimes \End V \otimes \mc{U}(\mf{gl}_N) \otimes \mbb{C}(u,v)
}
 in, where $R$ solves the QYBE is equivalent to the commutation relations for the generators $E_{ij}$ of $\mc{U}(\mf{gl}_N)$. This is the \bam{RTT} equation 
\end{prop}
\begin{proof}
This is a restriction of the proof of lemma \ref{lemma:CQIS:YangianCommReln}. 
\end{proof}
To re-interpret the above, we can calculate
\eq{
R_{12} T_1 T_2 v^a \otimes v^b \otimes 1 &+ \sum_\beta R_{12}T_1 v^a \otimes v^\beta \otimes t^b_\beta \\
&= \sum_{\alpha,\beta} R_{12} v^\alpha \otimes v^\beta \otimes t^a_\alpha t^b_\beta  \\
&= \sum_{c,d,\alpha,\beta} R^{\alpha\beta}_{cd} v^c \otimes v^d \otimes  t^a_\alpha t^b_\beta \\
T_2 T_1 R_{12} v^a \otimes v^b \otimes 1 &= \sum_{\alpha,\beta} R_{\alpha\beta}^{ab} T_2 T_1 v^\alpha \otimes v^\beta \otimes 1 \\
&= \sum_{c,\alpha,\beta} T_2 v^c \otimes v^\beta \otimes t^\alpha_c \\
&= \sum_{c,d,\alpha,\beta} R^{ab}_{\alpha\beta} v^c \otimes v^d \otimes t^\beta_d t^\alpha_c \\ 
\Rightarrow \sum_{\alpha,\beta} R^{ab}_{\alpha\beta} t^\beta_d t^\alpha_c &= \sum_{\alpha,\beta} R^{\alpha\beta}_{cd} t^a_\alpha t^b_\beta
}


Note the above allows us to write 
\eq{
R_{12}R_{13}R_{23} T_1 T_2 T_3 = T_3 T_2 T_1 R_{12} R_{13} R_{23}
}
Doing the rearrangement in the above in different orders gives the QYBE. 

\begin{remark}
As we did with the QYBE, we can use a graphical mnemonic for the RTT relation, which looks as below 
\begin{center}
\begin{tikzpicture}
\draw[decorate, decoration=snake,->] (0,2)  -- (0,0) ;
\draw[->] (-1,2) node[anchor = east] {2} -- (2,0) ;
\draw[->] (-1,0)node[anchor = east] {1} -- (2,2)  ;

\draw (0.25,1.2) node[anchor = south] {$v$};

\draw (0.25,0.8) node[anchor = north] {$u$};

\draw (3,1) node {=};

\draw[decorate, decoration=snake,->] (6,2) -- (6,0) ;
\draw[->] (4,2) node[anchor = east] {2} -- (7,0) ; 
\draw[->] (4,0) node[anchor = east] {1} -- (7,2)  ;

\draw (5.75,1.2) node[anchor = south] {$u$};

\draw (5.75,0.8) node[anchor = north] {$v$};
\end{tikzpicture}
\end{center}
\end{remark}

We can now generalise our universal enveloping algebra:
\begin{definition}
Let $T(u) = \sum_{ij} E_{ij} \otimes t_{ij}(u)\in \End(V) \otimes Y(\mf{gl}_N) \otimes \mbb{C}[[u^{-1}]]$ where 
\eq{
t_{ij}(u) &= \delta_{ij} + \sum_{k \geq 1} t_{ij}^{(k)} u^{-k} \\
&= \sum_{k \geq 0} t_{ij}^{(k)} u^{-k} \quad \text{letting $t_{ij}^{(0)} = \delta_{ij}$}
}
The \bam{Yangian}, $Y(\mf{gl}_N)$, is the unital associative algebra with generators $\pbrace{t_{ij}^{(k)} \, | \, k \geq 1}$ and relations given by imposing the RTT relation.
\end{definition}

\begin{lemma}[Yangian commutation relations]\label{lemma:CQIS:YangianCommReln}
The $t_{ij}^{(k)}$ satisfy
\begin{enumerate}
    \item $\comm[t_{ik}^{(r+1)}]{t_{kl}^{(s)}} - \comm[t_{ik}^{(r)}]{t_{kl}^{(s+1)}} = t_{kj}^{(r)} t_{il}^{(s)} - t_{kj}^{(s)} t_{il}^{(r)}$
    \item $\comm[t_{ij}^{(r)}]{t_{kl}^{(s)}} = \sum_{m=1}^{\min(r,s)} \psquare{ t_{kj}^{(m-1)}t_{il}^{(r+s-m)} - t_{kj}^{(r+s-m)} t_{il}^{(m-1)}}$
\end{enumerate}
\end{lemma}
\begin{proof}
\hl{This is copied over from the answer to the assignment, and so needs a sign fix throughout.}
Defining $s_{ij}(u) = \delta_{ij} + u^{-1}E_{ji}$ we have $R(u) = \sum_{i,j} E_{ij} \otimes s_{ij}(u) $. As such 
\eq{
T_2(v)T_1(u)R_{12}(u-v) &= \pround{\sum_{k,l} 1 \otimes E_{kl} \otimes t_{kl}(v)}\pround{\sum_{i,j}E_{ij} \otimes 1 \otimes t_{ij}(u)}\pround{\sum_{m,n} E_{mn} \otimes s_{mn}(u-v) \otimes 1} \\
&= \sum_{i,j,k,l,m,n} (E_{ij}E_{mn}) \otimes (E_{kl}s_{mn}(u-v)) \otimes (t_{kl}(v)t_{ij}(u)) \\
&= \sum_{i,j,k,l,m,n} \delta_{jm}E_{in} \otimes (\delta_{mn}E_{kl} + (u-v)^{-1}\delta_{ln}E_{km})  \otimes (t_{kl}(v)t_{ij}(u)) \\
&= \sum_{i,j,k,l,n} E_{in} \otimes (\delta_{jn}E_{kl}+(u-v)^{-1}\delta_{ln}E_{kj}) \otimes (t_{kl}(v)t_{ij}(u))\\
&= \sum_{i,j,k,l} (E_{ij} \otimes E_{kl} + (u-v)^{-1}E_{il} \otimes E_{kj}) \otimes (t_{kl}(v)t_{ij}(u)) 
}
Similarly,
\eq{
R_{12}(u-v) T_1(u) T_2(v) &= \sum_{i,j,k,l,m,n} (E_{mn}E_{ij}) \otimes (s_{mn}(u-v) E_{kl}) \otimes (t_{ij}(u)t_{kl}(v)) \\
&= \sum_{i,j,k,l,m,n} \delta_{ni}E_{mj} \otimes (\delta_{mn}E_{kl} + (u-v)^{-1}\delta_{km}E_{nl}) \otimes (t_{ij}(u)t_{kl}(v)) \\
&= \sum_{i,j,k,l} (E_{ij} \otimes E_{kl} + (u-v)^{-1}E_{kj}\otimes E_{il}) \otimes (t_{ij}(u)t_{kl}(v))
}
This means we can write 
\eq{
R_{12}(u-v) T_1(u) T_2(v) - T_2(v)T_1(u)R_{12}(u-v) =& \sum_{i,j,k,l} 
E_{ij} \otimes E_{kl} \otimes \comm[t_{ij}(u)]{t_{kl}(v)} \\
&+ (u-v)^{-1} E_{ij} \otimes E_{kl} \otimes \psquare{t_{kj}(v)t_{il}(u) - t_{kj}(u)t_{il}(v)} \\
\Rightarrow \comm[t_{ij}(u)]{t_{kl}(v)} + (u-v)^{-1}\psquare{t_{kj}(v)t_{il}(u) - t_{kj}(u)t_{il}(v)} =& \, 0 
}
This is an equality of Laurent series, so we can compare terms of order $u^{-r}v^{-s}$ to get constraints. We will assume $u > v$ to expand $(u-v)^{-1}$ in a convergent Laurent series, but we could equally assume $v > u$ and follow through the calculation, getting the same result. We don't let $u=v$ in order to have $R(u-v)$ finite. Now 
\eq{
(u-v)^{-1} = u^{-1}\pround{1-\frac{v}{u}}^{-1} = u^{-1}\sum_{m \geq 0} \pround{\frac{v}{u}}^m = \sum_{m \geq 0} v^m u^{-m-1}
}
Thus 
\eq{
\sum_{r,s \geq 0}\comm[t_{ij}^{(r)}]{t_{kl}^{(s)}} u^{-r} v^{-s} &= -\sum_{m\geq 0} v^m u^{-m-1} \sum_{r,s \geq 0 } u^{-r}v^{-s}\psquare{t_{kj}^{(s)} t_{il}^{(r)} - t_{kj}^{(r)} t_{il}^{(s)} } 
}
and so 
\eq{
\comm[t_{ij}^{(r)}]{t_{kl}^{(s)}} = -\sum_{m \geq 0} \psquare{t_{kj}^{(s+m)} t_{il}^{(r-m-1)} - t_{kj}^{(r-m-1)} t_{il}^{(s+m)}}
}
Using that for $m <0, \, t_{ij}^{(m)} = 0$, we may rewrite and re-index the sum ($m \mapsto m-s-1$) as 
\eq{
\comm[t_{ij}^{(r)}]{t_{kl}^{(s)}} = -\sum_{m=1}^{\min(r,s)} \psquare{ t_{kj}^{(m-1)}t_{il}^{(r+s-m)} - t_{jk}^{(r+s-m)} t_{il}^{(m-1)}}
}
An immediate corollary of this is (wlog taking $s <r$)
\eq{
\comm[t_{ij}^{(r+1)}]{t_{kl}^{(s)}} - \comm[t_{ij}^{(r)}]{t_{kl}^{(s+1)}} =& -\left\lbrace\sum_{m=1}^{\min(r+1,s)} \psquare{ t_{kj}^{(m-1)}t_{il}^{(r+s+1-m)} - t_{kj}^{(r+s+1-m)} t_{il}^{(m-1)}} \right.\\
&- \left. \sum_{m=1}^{\min(r,s+1)} \psquare{ t_{kj}^{(m-1)}t_{il}^{(r+s+1-m)} - t_{kj}^{(r+s+1-m)} t_{il}^{(m-1)}} \right\rbrace \\
=&  \sum_{m=s+1}^{s+1} \psquare{ t_{kj}^{(m-1)}t_{il}^{(r+s+1-m)} - t_{kj}^{(r+s+1-m)} t_{il}^{(m-1)}} \\
=& t_{kj}^{(s)}t_{il}^{(r)} - t_{kj}^{(r)} t_{il}^{(s)}
}
\end{proof}

\begin{prop}
The map $\pi: t_{ij}(u) \mapsto \delta_{ij} + u^{-1}E_{ij}$ is an algebra epimorphism $Y(\mf{gl}_N) \to \mc{U}(\mf{gl}_N)$, and the map $i : E_{ij} \mapsto t_{ij}^{(1)}$ is an embedding $\mc{U}(\mf{gl}_N) \hookrightarrow Y(\mf{gl}_N)$. 
\end{prop}

\begin{remark}
It follows that any $\mf{gl}_N$-module can be extended to a $Y(\mf{gl}_N)$-module using $\pi$. (E.g. see lattice models later)
\end{remark}

An advantage of defining $Y$ using the RTT relation is that we can easily read off the transformations of $T(u)$ which lead to a new solution $\tilde{T}(u)$ of RTT and thus induce algebra (anti-)homomorphisms of $Y$. 

\begin{prop}
Let $f(u) \in \mbb{C}[[u^{-1}]]$, $f(\infty) = 1$, and $M$ a non-singluar matrix. The maps 
\begin{itemize}
    \item $T(u) \mapsto f(u) T(u)$
    \item $T(u) \mapsto T(u+a)$ 
    \item $T(u) \mapsto MT(u) M^{-1}$
\end{itemize}
are automorphisms of $Y$ and 
\begin{itemize}
    \item $T(u) \mapsto T(-u)$
    \item $T(u) \mapsto T(u)^T$
    \item $T(u) \mapsto T(u)^{-1}$
\end{itemize}
are anti-automorphisms. 
\end{prop}

\begin{remark}
As $T(u)$ is a formal power series in $u^{-1}$ with matrix coefficient, and the first term is the identity, it must be the case that $T^{-1}$ exists.
\end{remark}


\hl{insert section from Molev on quantum determinant}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hopf algebra structure}

\begin{prop}[Railroad principle]
If $T(u)$ a solution of RTT, then $\Delta (T(u)) \equiv T(u) \otimes T(u) = \pround{\sum_a t_{aj}(u) \otimes t_{ia}(u)}_{ij}$ is also a solution in $Y \otimes Y$
\end{prop}
\begin{corollary}
The map $\Delta : Y(\mf{gl}_N) \to Y(\mf{gl}_N) \otimes Y(\mf{gl}_N)$ is an algebra homomorphism
\end{corollary}

Let $A$ be an associative unital $\mbb{C}$-algebra, then the following diagram commute:

\begin{tkz}
A \otimes A \arrow[d,"m"'] & A \otimes A \otimes A \arrow[l,"m \otimes \id"'] \arrow[d,"\id \otimes m"] \\ A & A\otimes A \arrow[l,"m"]
\end{tkz}

\begin{tkz}
A \otimes A  \arrow[d,"m"'] & A \otimes \mbb{C} \arrow[l,"\id \otimes 1"'] \arrow[dl,"\sim"] \\ 
A & 
\end{tkz}
\begin{tkz}
A \otimes A  \arrow[d,"m"'] & \mbb{C} \otimes A  \arrow[l,"1 \otimes \id "'] \arrow[dl,"\sim"] \\ 
A & 
\end{tkz}

\begin{definition}
Let $A$ be a $\mbb{C}$-vector space with linear maps $\Delta : A \to A \otimes A$ (\bam{coproduct}), $\eps: A \to \mbb{C}$ (\bam{co-unit}) s.t. the co-diagrams to the above commute, i.e. 
\begin{tkz}
A \otimes A \arrow[r,"\Delta \otimes \id"] & A \otimes A \otimes A \\ 
A \arrow[u,"\Delta"] \arrow[r,"\Delta"] & A \otimes A \arrow[u,"\id \otimes \Delta"']
\end{tkz}
\begin{tkz}
A \otimes A \arrow[r,"\id \otimes \eps"] & A \otimes \mbb{C} \\
A \arrow[u,"\Delta"] \arrow[ur,"\sim"']
\end{tkz}
\begin{tkz}
A \otimes A \arrow[r,"\eps \otimes \id"] & \mbb{C} \otimes A  \\
A \arrow[u,"\Delta"] \arrow[ur,"\sim"']
\end{tkz}
Then we call $A$ a \bam{coalgebra}. If $A$ is also a unital associative algebra and $\Delta, \eps$ are algebra homomorphisms then we call $A$ a \bam{bialgebra}. 
\end{definition}

\begin{remark}
Note that being a bialgebra is equivalent to being an algebra \bam{and also} a coalgebra. This links us back to our definition of Lie bialgebra, which has the additional structure of the coalgebra also being a Lie algebra. 
\end{remark}

\begin{example}
A natural example is the group algebra $\mbb{C}G$, in which elements are formal sums of group elements. Here we have 
\eq{
\Delta : \mbb{C}G &\to \mbb{C}G \otimes \mbb{C}G \\
g &\mapsto g \otimes g \\
\eps : \mbb{C}G &\to \mbb{C} \\
g &\mapsto 1
}
Note the co-product is co-commutative. Moreover,  this algebra also has the natural map $S : \mbb{C}G \to \mbb{C}G, \, g \mapsto g^{-1}$ which is an antihomomorphism. 
\end{example}

\begin{definition}
A bialgebra $H$ is called a \bam{Hopf algebra} if it also has an antihomomorphism $S : H \to H$ (called the \bam{antipode}) s.t. 
\begin{tkz}
H \otimes H \arrow[r,"S \otimes \id"] & H \otimes H \arrow[d,"m"] \\
H \arrow[u,"\Delta"] \arrow[r,"1 \circ \eps"'] & H
\end{tkz}

\begin{tkz}
H \otimes H \arrow[r,"\id \otimes S"] & H \otimes H \arrow[d,"m"] \\
H \arrow[u,"\Delta"] \arrow[r,"1 \circ \eps"'] & H
\end{tkz}

commute
\end{definition}

\begin{example}
	As shown the group algebra is a Hopf algebra
\end{example}

\begin{example}
The tensor algebra $\mc{T}(V)$ of a vector space $V$ (over $\mbb{C}$) is a Hopf algebra. To show this let us build the necessary maps. Define $\Delta : V \to \mc{T}(V) \otimes \mc{T}(V)$ by 
\eq{
\Delta(v) &= v \otimes 1 + 1 \otimes v \\
\Delta(1) &= 1 \otimes 1
}
and extend linearly. By the universal property of the tensor product this map extends uniquely  
We take the counit simply defined by 
\eq{
\eps : V &\to \mbb{C} \\
v &\mapsto 0 \\
1 &\mapsto 1
}
and then extend it so it is an algebra homomorphism, again using the universal property of $\mc{T}$. We can see 
\eq{
(\Delta \otimes \id)(v \otimes 1 + 1 \otimes v) &= v \otimes 1 \otimes 1 + 1 \otimes v \otimes 1 + 1 \otimes 1 \otimes 1 \\
&= (\id \otimes \Delta)(v \otimes 1 + 1 \otimes v) \\
(\Delta \otimes \id) (1 \otimes 1) &= 1 \otimes 1 \otimes 1 \\
&= (\id \otimes \Delta)(1 \otimes 1)
}
So comultiplication is associative. Further 
\eq{
(\id \otimes \eps)(v \otimes 1 + 1 \otimes v) = v \otimes 1 + 1 \otimes 0 &= v \otimes 1 \\
(\id \otimes \eps)(1 \otimes 1) &= 1 \otimes 1 
}
and by symmetry the other $\eps$ diagram commutes. Hence we have multiplicative inverse. 
Finally lets introduce an antipode by 
\eq{
S : \mc{T} &\to \mc{T} \\
v &\mapsto -v \\
1 &\mapsto 1 
}
and extend to an algebra antihomoomrphism. Then 
\eq{
m((S \otimes \id)(v \otimes 1 + 1 \otimes v)) &= m(-v \otimes 1 + 1 \otimes v) \\
&= -v + v = 0 = \eps(v)  \\
m((S \otimes \id)(1 \otimes 1)) &= m(1 \otimes 1) \\
&= 1 = \eps(1) 
}
gives commutation of the first antipode diagram, and the second follows similarly. Hence we are done. 
\end{example}


\begin{theorem}
The Yangian is a Hopf algebra with 
\begin{itemize}
    \item Co-product $\Delta (t_{ij}(u)) = \sum_a t_{aj}(u) \otimes t_{ia}(u)$
    \item Co-unit $\eps(t_{ij}(u)) = \delta_{ij}$
    \item Antipode $S(t_{ij}(u)) = (T^{-1}(u))_{ij}$
\end{itemize}
\end{theorem}
\begin{proof}
The hardest part is showing that $\Delta$ is a hom, which comes from the railroad principle. 
\end{proof}

\begin{remark}
There is an alternative Hopf algebra structure on the Yangian where we instead take $\Delta^\prime(t_{ij}(u)) = \sum_a t_{ia}(u) \otimes t_{aj(u)}$
\end{remark}

\begin{prop}
The map $T(u) \to T(-u)^T $ defines an involutive automorphism of $Y(\mf{gl}_N)$, $\tau_N$, which interchanges co-product on $Y$ by $\Delta^\prime \circ \tau_N = (\tau_N \otimes \tau_N) \circ \Delta$. 
\end{prop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Yangian as a deformation of \secmath{\mc{U}(\mf{gl}_N[x])}}

Note that $Y(\mf{gl}_N)$ is a filtered algebra w.r.t. the grading $\deg t_{ij}^{(r+1)} = r$, that is if $Y_r$ is the subspace of elements of degree $\leq r$ then $Y_r Y_s \subseteq Y_{r+s}$. 

\begin{prop}
The associated graded algebra $\gr Y = \bigoplus_{r \geq 1} \faktor{Y_r}{Y_{r-1}}$ is isomorphic to $\mc{U}(\mf{gl}_N[x])$
\end{prop}
\begin{proof}
Recall the relation 
\eq{
\underbrace{\comm[t_{ij}^{(r)}]{t_{kl}^{(s)}}}_{\in Y_{r+s-2}} = \sum_{m=1}^{\min(r,s)} \psquare{ t_{kj}^{(m-1)}t_{il}^{(r+s-m)} - t_{kj}^{(r+s-m)} t_{il}^{(m-1)}} = \underbrace{\delta_{kj}t_{il}^{(r+s-1)} - \delta_{il}t_{kj}^{r+s-1}}_{\in Y_{r+s-2}} + \underbrace{\dots}_{\in Y_{r+s-3}}
}
and so the map 
\eq{
\mc{U}(\mf{gl}_N[x]) &\to \gr Y \\
E_{ij}x^{r-1} &\mapsto t_{ij}^{(r)}+ Y_{r-2}
}
is an algebra epimorphism. Since any element in $Y$ can be uniquely written as a linear combination of monomials in the generators $t_{ij}^{(r)}$, the kernel of this map must be trivial. 
\end{proof}

\begin{remark}
This map extends to an isomorphism of Hopf algebras with $\forall g \in \mf{gl}_N[x], \, \Delta(g) = g \otimes 1 + 1 \otimes g$, $\eps(g) = 0$, $S(g) = -g$
\end{remark}

\begin{remark}
The statement that "any element in $Y$ can be uniquely written as a linear combination of monomials in the generators $t_{ij}^{(r)}$" is not entirely trivial, but is a consequence of the Poincar\'e-Birkhoff-Witt theorem. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lattice models in statistical mechanics}

Let $G$ be an $m \times n$ square lattice. Denote by $E(G)$ the set of its edges and $V(G)$ the set of its vertices $v = \pangle{i,j}$. Given a vertex, denote by $N,E,S,W$ its adjacent north, east, south, and west edges respectively. Fix some $\bm{\lambda} = (\lambda_1, \dots, \lambda_m)\in \mbb{C}^m, \, \bm{\mu} = (\mu_1, \dots, \mu_n)\in \mbb{C}^n$

\begin{definition}
A \bam{lattice configuration} is a map $\gamma : E(G) \to \pbrace{1, \dots, N} \equiv [N]$ \\
Denote $\Gamma$ the set of all lattice configurations for a given $R$-matrix $R(\bm{\lambda},\bm{\mu}) : \mbb{C}^N \otimes C^N \to \mbb{C}^N \otimes \mbb{C}^N$
\end{definition}

\begin{remark}
The purpose of this definition of a lattice configuration is such that we imagine the following picture 
\begin{center}
    \begin{tikzpicture}
    \draw[->,text=red] (-1,0) node[anchor = south] {$\lambda_1$} -- (4,0);
    \draw[->,text=red] (-1,1) node[anchor = south] {$\lambda_2$} -- (4,1);
    \draw[text=red] (-1.15,2) node [anchor = south] {$\vdots$};
    \draw[->,text = red] (-1,3) node[anchor = south] {$\lambda_m$} -- (4,3);
    
    \draw[->,text=red] (0,4) -- (0,-1) node[anchor = west] {$\mu_1$};
    \draw[->,text=red] (1,4) -- (1,-1) node[anchor = west] {$\mu_2$};
    \draw[text=red] (2,-1) node [anchor = west] {$\cdots$};
    \draw[->,text=red] (3,4) -- (3,-1) node[anchor = west] {$\mu_n$};
    \end{tikzpicture}
\end{center}
wherein each vertex corresponds to a scattering via an $R$ matrix as 
\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1,0) node[anchor = east] {i}  -- (1,0) node[anchor = west] {k} ;
    \draw[->] (0,1) node[anchor = south] {j} -- (0,-1) node[anchor = north] {l};
    \draw[text=red]  (-1,0) node[anchor=south] {$\lambda_a$};
    \draw[text=red] (0,-1) node[anchor=east] {$\mu_b$};
    
    \draw (2,0) node {$\sim$} ;
    
    \draw (2.5,0) node[anchor = west] {$R^{ij}_{kl}(\lambda_a - \mu_b)$};
    
    \end{tikzpicture}
\end{center}
I like to think of this as two states, $i,j$ going in, and coming out at $k,l$ with a new factor in front of them (very akin to how we visualise scattering, hence the use of the language). The red text identifies the rep space the vertex lives in, while the indices in black at the end of the lines tell us the state coming in or out. 
\end{remark}

\begin{definition}
The \bam{weight} of a lattice configuration is
\eq{
\wt_{\bm{\lambda},\bm{\mu}} : \Gamma &\to \mbb{C} \\
\gamma &\mapsto \prod_{\pangle{i,j}\in V(G)} R^{\gamma(W) \gamma(N)}_{\gamma(E) \gamma(S)}(\lambda_i,\mu_j)
}
\hl{1: this doesn't agree with the diagram above, so solve this issue}
\end{definition}

\begin{definition}
The \bam{partition function} is the weighted sum over all the lattice configurations 
\eq{
Z(\bm{\lambda},\bm{\mu}) = \sum_{\gamma \in \Gamma} \wt(\gamma)
}
\end{definition}

\begin{remark}
Note that we have freedom to rescale our $R$ matrix by a constant. Hence if we can scale s.t, for fixed $\bm{\lambda},\bm{\mu}$, $Z=1$, then we interpret $\wt(\gamma)$ as the probability of a given lattice configuration $\gamma$. In statistical mechanics the partition function is the main ingredient in working out what happens, and so if we can work it out, we are sorted (\hl{ maybe include an example of this? I've never done statistical mechanics so should include at least one})
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation modules}

Recall the two algebra homomorphisms 
\eq{
Y(\mf{gl}_N) &\to \mc{U}(\mf{gl}_N) \\
t_{ij}(u) &\mapsto \delta_{ij}I + u^{-1}E_{ij} \\
&\phantom{\to} \\
Y(\mf{gl}_N) &\to Y(\mf{gl}_N) \\
t_{ij}(u) &\mapsto t_{ij}(u - \mu)
}
where $\mu \in \mbb{C}$.

\begin{remark}
Note here I am being lazy and these are actually maps from $Y(\mf{gl}_N)[[u^{-1}]]$. 
\end{remark}

Hence, given a $\mf{gl}_N$-module $V$,  we may define the $Y(\mf{gl}_N)$-module $V(\mu)$ with the action  
\eq{
t_{ij}(u) \cdot v \equiv \psquare{\delta_{ij}I + (u - \mu)^{-1}E_{ij}} \cdot v
}
Let us write this as $t_{ij}(u) \mapsto \delta_{ij} + (u - \mu)^{-1}E_{ij}$.
Note that if $V = \mbb{C}^N$ with the fundamental rep, then this is the rep that acts as 
\eq{
T(u) \mapsto T_{V(\mu)}(u) = R(u-\mu) \in \End(V^{\otimes 2}) \cong (\End \mbb{C}^N)^{\otimes 2}
}

\begin{remark}
\hl{2: I have written this down, but now I do not believe it.} We consider $T_{V(\mu)}$ as a matrix with entries $\delta_{ij} + (u-\mu)^{-1}E_{ij}$, but $R(u-\mu)$ is a matrix with entries $\delta_{ij} - (u-\mu)^{-1}E_{ji}$!
\end{remark}

\begin{remark}
As $R$ solves the QYBE, $T$ as defined above solves the RTT relation. 
\end{remark}

We can then 'upgrade' to a lattice model by considering the module $V(\mu_1) \otimes \dots \otimes V(\mu_n)$. 

\begin{definition}\label{def:CQIS:monodromymatrix}
The \bam{row-to-row monodromy matrix} is
\eq{
T_{\bigotimes_{i=1}^n V(\mu_i)} (\lambda_a) = T(\lambda_a,\bm{\mu}) = R_{an}(\lambda_a - \mu_n) \dots R_{a1}(\lambda_a - \mu_1) \in \End \psquare{V(\lambda_a) \otimes \bigotimes_{i=1}^n V(\mu_i)}
}
for the $a^{th}$ row. It can be represented as an $N \times N$ matrix with entries $t_{ij}(\lambda_a,\bm{\mu})$ called the \bam{row-to-row transfer matrices}. Graphically we view the row-to-row transfer matrices as  
\begin{center}
    \begin{tikzpicture}
    \draw (-2.3,0) node[anchor = east] {$t_{ij}(\lambda) \mapsto$};
    \draw[->] (-2,0) node[anchor = east] {$j$} -- (3,0) node[anchor = west] {$i$};
    \draw[text=red] (-2,0) node[anchor = south] {$\lambda$};
    \draw[->,text=red] (-1,0.5) -- (-1,-0.5) node[anchor = west] {$\mu_1$} ;
    \draw[->,text=red] (0,0.5) -- (0,-0.5) node[anchor = west] {$\mu_2$} ;
    \draw[text=red] (1,-0.5) node {$\cdots$};
    \draw[->,text=red] (2,0.5) -- (2,-0.5) node[anchor = west] {$\mu_n$} ;
    \end{tikzpicture}
\end{center}
With this we can define the \bam{monodromy matrix}
\eq{
T(\bm{\lambda},\bm{\mu}) = R_{1n}(\lambda_1,\mu_n) \dots R_{11}(\lambda_1, \mu_1) \dots R_{mn}(\lambda_m,\mu_n) \dots R_{m1}(\lambda_m,\mu_1) \in \End \psquare{\bigotimes_{a=1}^m V(\lambda_m) \otimes \bigotimes_{i=1}^n V(\mu_i)}
}
which corresponds to the picture
\begin{center}
    \begin{tikzpicture}
    \draw[->,text=red] (-1,0) node[anchor = south] {$\lambda_1$} -- (4,0);
    \draw[->,text=red] (-1,1) node[anchor = south] {$\lambda_2$} -- (4,1);
    \draw[text=red] (-1.15,2) node [anchor = south] {$\vdots$};
    \draw[->,text = red] (-1,3) node[anchor = south] {$\lambda_m$} -- (4,3);
    
    \draw[->,text=red] (0,4) -- (0,-1) node[anchor = west] {$\mu_1$};
    \draw[->,text=red] (1,4) -- (1,-1) node[anchor = west] {$\mu_2$};
    \draw[text=red] (2,-1) node [anchor = west] {$\cdots$};
    \draw[->,text=red] (3,4) -- (3,-1) node[anchor = west] {$\mu_n$};
    \end{tikzpicture}
\end{center}

\end{definition}

\begin{lemma}\label{lemma:CQIS:PartitionFnFromMonodromy}
Let $\bm{a} \in [N]^{\times m}, \, \bm{b} \in [N]^{\times n }$, then 
\eq{
T(\bm{\lambda},\bm{\mu}) \pround{ \bigotimes_{i=1}^m v^{a_i}} \otimes \pround{ \bigotimes_{j=1}^n v^{b_j}} = \sum_{\bm{c},\bm{d}} Z^{\bm{a}\bm{b}}_{\bm{c}\bm{d}}(\bm{\lambda},\bm{\mu})\pround{ \bigotimes_{i=1}^m v^{c_i}} \otimes \pround{ \bigotimes_{j=1}^n v^{d_j}}
}
where 
\eq{
Z^{\bm{a}\bm{b}}_{\bm{c}\bm{d}}(\bm{\lambda},\bm{\mu}) = \sum_{\gamma \in \Gamma^{\bm{a}\bm{b}}_{\bm{c}\bm{d}}} \wt(\gamma)
}
for 
\eq{
\Gamma^{\bm{a}\bm{b}}_{\bm{c}\bm{d}} = \pbrace{\gamma : E(G) \to [N] \, | \, \gamma(i,0) = a_i, \, \gamma(i,n) = c_i, \, \gamma(1,j) = d-j, \, \gamma(m,j) = b_j}
}
\end{lemma}

\begin{remark}
We can see the lattice configurations $\Gamma^{\bm{a}\bm{b}}_{\bm{c}\bm{d}}$ pictorially as 
\begin{center}
    \begin{tikzpicture}
        \draw (-1,0) node[anchor = east] {$a_1$} -- (4,0) node[anchor = west] {$c_1$};
    \draw (-1,1) node[anchor = east] {$a_2$} -- (4,1) node[anchor = west] {$c_2$};
    \draw (-1.15,2) node [anchor = east] {$\vdots$};
    \draw (4.15,2) node [anchor = west] {$\vdots$};
    \draw (-1,3) node[anchor = east] {$a_m$} -- (4,3) node[anchor = west] {$c_m$};
    
    \draw (0,4) node[anchor=south] {$b_1$}-- (0,-1) node[anchor = north] {$d_1$};
    \draw (1,4) node[anchor=south] {$b_2$} -- (1,-1) node[anchor = north] {$d_2$};
    \draw (2,-1) node [anchor = north] {$\cdots$};
    \draw (2,4) node [anchor = south] {$\cdots$};
    \draw (3,4) node[anchor=south] {$b_n$} -- (3,-1) node[anchor = north] {$d_n$};
    \end{tikzpicture}
\end{center}
\end{remark}

\begin{lemma} We have 
\eq{
\sum_{k,l} R^{kl}_{c_i c_{i+1}}(\lambda_i, \lambda_{i+1}) Z^{\bm{a}\bm{b}}_{(c_1, \dots, k,l, \dots, c_m)\bm{d}}(\bm{\lambda},\bm{\mu}) = \sum_{k,l} Z^{(a_1, \dots, k,l, \dots, a_m)\bm{b}}_{\bm{c}\bm{d}}(\dots, \lambda_{i+1}, \lambda_i, \dots, \bm{\mu}) R_{kl}^{a_i a_{i+1}}(\lambda_i, \lambda_{i+1})
}
\end{lemma}
\begin{proof}
Apply the RTT relation \hl{and finish this / exercise}
\end{proof}

\begin{remark}
Note that when $m=1$, this reduces to the partition function for a single row, and $T$ reduces to the solution of the standard RTT relation we know. 
\end{remark}

\begin{lemma}
$T(\bm{\lambda},\bm{\mu}) = T_1(\lambda_1,\bm{\mu}) \cdots T_m(\lambda_m,\bm{\mu})$
\end{lemma}

\begin{lemma}
We can restate lemma \ref{lemma:CQIS:PartitionFnFromMonodromy} in terms of the transfer matrices as 
\eq{
t_{a_1 c_1}(\lambda_1, \bm{\mu})t_{a_2 c_2}(\lambda_2, \bm{\mu}) \cdots t_{a_m c_m}(\lambda_m, \bm{\mu}) \bigotimes_{j=1}^n e^{b_j} = \sum_{\bm{d}} Z^{\bm{a}\bm{b}}_{\bm{c}\bm{d}}(\bm{\lambda},\bm{\mu}) \bigotimes_{j=1}^n e^{d_j}
}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Special Cases: Periodic Boundary Conditions}
Let $\Gamma_{ij}$ be the set of lattice configurations where the right/left outer horizontal edges have values $i/j$. We assume periodic boundary conditions in the vertical directions. Graphically that is 
\begin{center}
    \begin{tikzpicture}
    \def\t{70}
    \def\a{45}
    \def\m{-2}
    \def\e{4}
    \foreach \n in {0,1,2,\e}
    {
    \draw ({\m+\n*sin(\t)},0) ellipse ({cos(\t)} and 1);
    }
    \foreach \k in {0,1,2,4,5} 
    {
    \draw[->][line width = {0.7*sin(\k*\a)} ] ({\m+sin(\k*\a)*cot(\t)-sin(\t)},{cos(\k*\a)}) node[anchor = east] {$i$} -- ({\m+(\e+1)*sin(\t)+sin(\k*\a)*cot(\t)},{cos(\k*\a)}) node[anchor = west] {$j$} ;
    }
    \end{tikzpicture}
\end{center}

\begin{lemma}
$\tr t_{ij}^m(\lambda,\bm{\mu}) = \sum_{\gamma \in \Gamma_{ij}} \wt_{\lambda,\bm{\mu}}(\gamma)$
\end{lemma}

\begin{lemma}
If $R(\lambda,\lambda^\prime)$ is invertible the the partition function for the cylinder 
\eq{
Z_{cyl}(\bm{\lambda},\bm{\mu}) = \sum_{\bm{a}} Z_{\bm{a}\bm{d}}^{\bm{a}\bm{c}}(\bm{\lambda},\bm{\mu})
}
is symmetric in the $\lambda_i$. In particular, row-to-row transfer matrices 
\eq{
\tau(\lambda_i,\bm{\mu}) = \tr_{\mbb{C}^N} T(\lambda_i,\bm{\mu}) = \sum_{a=1}^N t_{aa}(\lambda_i,\bm{\mu})
}
pairwise commute. 
\end{lemma}

Note that if instead of $\lambda\in \mbb{C}$ we had the formal parameter $u \in \mbb{C}(u)$, the same result may be derived. Hence expanding $\tau(u,\bm{\mu}) = \sum_{r \geq 0} u^{-r} \tau^{(r)}$, the $\tau^{(r)} \in \End \pround{\otimes V(\mu_j)}$ generate a commuting subalgebra

\hl{3: This definition is not exactly what is written in the notes where there is $u^r$ instead for the definition of the $\tau^{(r)}$. I am not sure that is correct.}

\begin{definition}
The algebra generated by the $\tau^{(r)}$ is called the \bam{Bethe algebra}.
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algebraic Bethe ansatz for \secmath{Y(\mf{gl}_2)} and the Heisenberg model}

A question we could now ask is whether the $\tau^{(r)}$ are simultaneously diagonalisable \hl{why would we ask this?}. We will see that in the case $N=2$ this is true, and the way to construct the solution is via a certain ansatz for the solution. This will allow us to solve a certain lattice model.  \\
Let us decompose 
\eq{
\mf{gl}_2 = \mf{n}^+ \oplus \mf{h} \oplus \mf{n}^-
}
where 
\eq{
\mf{n}^+ &= \mbb{C} E_{12} = \mbb{C}\begin{psmallmatrix} 0 & 1\\ 0 & 0 \end{psmallmatrix} \\
\mf{n}^- &= \mbb{C} E_{21} = \mbb{C}\begin{psmallmatrix} 0 & 0\\ 1 & 0 \end{psmallmatrix} \\
\mf{h} &= \mbb{C} E_{11} \oplus \mbb{C} E_{22} = \mbb{C}\begin{psmallmatrix} 1 & 0\\ 0 & 0 \end{psmallmatrix} \oplus \mbb{C}\begin{psmallmatrix} 0 & 0\\ 0 & 1 \end{psmallmatrix}
}

\begin{definition}
The \bam{quantum determinant} of $T(u) \in Y$ is 
\eq{
q\det T(u) = t_{11}(u) t_{22}(u) - t_{12}(u) t_{21}(u)
}
\end{definition}

\begin{fact}
The quantum determinant is a central element of $Y$
\end{fact}
\hl{Would like to have a simple-ish proof of this fact}

\begin{definition}
The \bam{Bethe subalgebra} is $B \subset Y$ generated by the coefficients in the Laurent series of $q\det T(u) $ and $\tr T(u) = t_{11}(u) + t_{22}(u)$.
\end{definition}

\begin{remark}
\hl{How does this connect with our previous definition in terms of the $\tau^{(r)}$?} 
\end{remark}

\begin{prop}
The Bethe subalgebra commutes with the image of the UEA under the evaluation homomorphism. 
\end{prop}
\begin{proof}
Note from our previously worked out commutation relations we have 
\eq{
\comm[t_{11}^{(r)}+t_{22}^{(r)}]{t_{ij}^{(1)}} &= \sum_{m=1}^{\min(1,r)}\psquare{t_{i1}^{(m-1)}t_{1j}^{(r+1-m)} - t_{i1}^{(r+1-m)}t_{1j}^{(m-1)} +t_{i2}^{(m-1)}t_{2j}^{(r+1-m)} - t_{i2}^{(r+1-m)}t_{2j}^{(m-1)}} \\
&=t_{i1}^{(0)}t_{1j}^{(r)} - t_{i1}^{(r)}t_{1j}^{(0)} +t_{i2}^{(0)}t_{2j}^{(r)} - t_{i2}^{(r)}t_{2j}^{(0)}
}
We can then simply check cases
\eq{
\comm[t_{11}^{(r)}+t_{22}^{(r)}]{t_{11}^{(1)}} &= t_{11}^{(r)} - t_{11}^{(r)} = 0 \\
\comm[t_{11}^{(r)}+t_{22}^{(r)}]{t_{12}^{(1)}} &= t_{12}^{(r)} - t_{12}^{(r)} =0\\
\comm[t_{11}^{(r)}+t_{22}^{(r)}]{t_{21}^{(1)}} &= - t_{21}^{(r)} +t_{21}^{(r)} =0\\
\comm[t_{11}^{(r)}+t_{22}^{(r)}]{t_{22}^{(1)}} &=  t_{22}^{(r)} +t_{22}^{(r)}=0 
}
We are given that the quantum determinant lies in the centre of $Y$, and so necessarily commutes with the Bethe subalgebra
\end{proof}

We will now restrict to the evaluation module $\bigotimes_{j=1}^n V(\mu_j)$ with $V = \mbb{C}^2$, and let $\pbrace{e_0,e_1}$ be the standard basis of $\mbb{C}^2$ (where we are now relabelling $1,2 \mapsto 0,1$). Following the convention in physics literature, re-write 
\eq{
 \begin{pmatrix} t_{00}(u) & t_{01}(u) \\ t_{10}(u) & t_{11}(u) \end{pmatrix} \mapsto T(u,\bm{\mu}) = \begin{pmatrix} A(u,\bm{\mu}) & B(u,\bm{\mu}) \\ C(u,\bm{\mu}) & D(u,\bm{\mu}) \end{pmatrix} \in \End\pround{\bigotimes_{j=1}^n V(\mu_j)}
}

\begin{lemma}
The evaluation module $\bigotimes_{j=1}^n V(\mu_j)$ is the highest weight representation of $Y(\mf{gl}_2)$ with highest weight vector $\Omega = e_0 \otimes \dots \otimes e_0$ and relations 
\eq{
A(\lambda,\bm{\mu}) \Omega &= \prod_{j=1}^n \psquare{1-(\lambda - \mu_j)^{-1}}\Omega \\
D(\lambda, \bm{\mu}) \Omega &= \Omega \\
C(\lambda,\bm{\mu}) \Omega &= 0
}
\end{lemma}
\begin{proof}
Recall graphically we wrote 
\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1,0) node[anchor = east] {i}  -- (1,0) node[anchor = west] {k} ;
    \draw[->] (0,1) node[anchor = south] {j} -- (0,-1) node[anchor = north] {l};
    \draw[text=red]  (-1,0) node[anchor=south] {$\lambda_a$};
    \draw[text=red] (0,-1) node[anchor=east] {$\mu_b$};
    
    \draw (2,0) node {$\sim$} ;
    
    \draw (2.5,0) node[anchor = west] {$R^{ij}_{kl}(\lambda_a - \mu_b)$};
    
    \end{tikzpicture}
\end{center}

Hence we can explicitly list the different vertex options. Recalling that 
\eq{
R(\lambda)(e_i \otimes e_j) = e_i \otimes e_j - \lambda^{-1} e_j \otimes e_i
}
we can explicitly write the non-zero vertices as 
\begin{center}
    \begin{tikzpicture}
    \def\O{2.5}
    \def\D{-2}
    \foreach \x/\y/\i/\j/\k/\l\v in {-\O/0/0/0/0/0/$1-\lambda^{-1}$,\O/0/1/1/1/1/$1-\lambda^{-1}$,%
    -\O/\D/0/1/0/1/1/1,\O/\D/1/0/1/0/1/1,%
    -\O/{2*\D}/1/0/0/1/$\lambda^{-1}$,\O/{2*\D}/0/1/1/0/$\lambda^{-1}$}
    {
    \draw[->] ({-0.5+\x},\y) node[anchor = east] {\i}  -- ({0.5+\x},\y) node[anchor = west] {\k} ;
    \draw[->] (\x,{0.5+\y}) node[anchor = south] {\j} -- (\x,{-0.5+\y}) node[anchor = north] {\l};
    \draw ({\x+1.35},\y) node {$\sim$} ;
    \draw ({\x+1.7},\y) node[anchor = west] {\v};
    }
    \end{tikzpicture}
\end{center}
Observe these are split into 3 pair, those with identity and swap contribution, those with identity contribution, and those with swap contribution. \\
Now using $T(\lambda,\bm{\mu}) = R_{0n}(\lambda,\mu_n) \dots R_{01}(\lambda,\mu_1)$ (\hl{which I currently doubt. I think we want a transpose for the $T$ at least, and this explains the problem with C, see below.}) we associate with the the calculation of $A(\lambda,\bm{\mu})\Omega$ (remembering definition \ref{def:CQIS:monodromymatrix}) the graphic 
\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1.5,0) node[anchor = east] {$0$} -- (2.5,0) node[anchor = west] {$0$};
    \draw[->] (-1,0.5) node[anchor = south] {$0$} -- (-1,-0.5)  ;
    \draw[->] (1,0.5) node[anchor = south] {$0$} -- (1,-0.5)  ;
    \draw[->] (-0.5,0.5) node[anchor = south] {$0$} -- (-0.5,-0.5)  ;
    \draw[->] (0.5,0.5) node[anchor = south] {$0$} -- (0.5,-0.5)  ;
    \draw[->] (0,0.5) node[anchor = south] {$0$}-- (0,-0.5)  ;
    \draw (1.5,0.5) node {$\cdots$};
    \draw (1.5,-0.5) node {$\cdots$};
    \draw[->] (2,0.5) node[anchor = south] {$0$} -- (2,-0.5) ;
    \draw[decorate, decoration={brace,amplitude=10pt},yshift=0.5cm,xshift=0pt] (-1,0.5) -- (2,0.5)  node [black,midway,yshift=0.6cm]
{$n$};
    \end{tikzpicture}
\end{center}
Using our vertex rules above we see the only non-zero output is 
\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1.5,0) node[anchor = east] {$0$} -- (2.5,0) node[anchor = west] {$0$};
    \draw[->] (-1,0.5) node[anchor = south] {$0$} -- (-1,-0.5) node[anchor = north] {$0$}  ;
    \draw[->] (1,0.5) node[anchor = south] {$0$} -- (1,-0.5) node[anchor = north] {$0$}  ;
    \draw[->] (-0.5,0.5) node[anchor = south] {$0$} -- (-0.5,-0.5) node[anchor = north] {$0$} ;
    \draw[->] (0.5,0.5) node[anchor = south] {$0$} -- (0.5,-0.5) node[anchor = north] {$0$} ;
    \draw[->] (0,0.5) node[anchor = south] {$0$}-- (0,-0.5) node[anchor = north] {$0$} ;
    \draw (1.5,0.5) node {$\cdots$};
    \draw (1.5,-0.5) node {$\cdots$};
    \draw[->] (2,0.5) node[anchor = south] {$0$} -- (2,-0.5) node[anchor = north] {$0$};
    \draw (3,0) node[anchor = west] {=} ;
    \draw (3.5,0) node[anchor = west] {$\prod_{j=1}^n\psquare{1- (\lambda-\mu_j)^{-1}}$};
    \end{tikzpicture}
\end{center}
Similarly, we know that for $D(\lambda,\bm{\mu})\Omega$ we will get the non-zero term 

\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1.5,0) node[anchor = east] {$1$} -- (2.5,0) node[anchor = west] {$1$};
    \draw[->] (-1,0.5) node[anchor = south] {$0$} -- (-1,-0.5) node[anchor = north] {$0$}  ;
    \draw[->] (1,0.5) node[anchor = south] {$0$} -- (1,-0.5) node[anchor = north] {$0$}  ;
    \draw[->] (-0.5,0.5) node[anchor = south] {$0$} -- (-0.5,-0.5) node[anchor = north] {$0$} ;
    \draw[->] (0.5,0.5) node[anchor = south] {$0$} -- (0.5,-0.5) node[anchor = north] {$0$} ;
    \draw[->] (0,0.5) node[anchor = south] {$0$}-- (0,-0.5) node[anchor = north] {$0$} ;
    \draw (1.5,0.5) node {$\cdots$};
    \draw (1.5,-0.5) node {$\cdots$};
    \draw[->] (2,0.5) node[anchor = south] {$0$} -- (2,-0.5) node[anchor = north] {$0$};
    \draw (3,0) node[anchor = west] {=} ;
    \draw (3.5,0) node[anchor = west] {$1$};
    \end{tikzpicture}
\end{center}

and for $C(\lambda,\bm{\mu})\Omega$ we get the graphic 

\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1.5,0) node[anchor = east] {$0$} -- (2.5,0) node[anchor = west] {$1$};
    \draw[->] (-1,0.5) node[anchor = south] {$0$} -- (-1,-0.5)  ;
    \draw[->] (1,0.5) node[anchor = south] {$0$} -- (1,-0.5)  ;
    \draw[->] (-0.5,0.5) node[anchor = south] {$0$} -- (-0.5,-0.5)  ;
    \draw[->] (0.5,0.5) node[anchor = south] {$0$} -- (0.5,-0.5)  ;
    \draw[->] (0,0.5) node[anchor = south] {$0$}-- (0,-0.5)  ;
    \draw (1.5,0.5) node {$\cdots$};
    \draw (1.5,-0.5) node {$\cdots$};
    \draw[->] (2,0.5) node[anchor = south] {$0$} -- (2,-0.5) ;
    \end{tikzpicture}
\end{center}

which we can see from our graphical rules will give $0$ contribution. 
\end{proof}

\begin{remark}
\hl{4: In order to make the previous proof work} I have treated $t_{10}\mapsto B, \, t_{01} \mapsto C$ as the correct labelling. I will continue with this labelling for the rest of the section.  
\end{remark}

\begin{remark}
If you are like me and have only seen the concept of a highest weight representation in the context of Kac-Moody Lie algebras where there are obvious raising/lowering operators, you may be wondering what they are here. \hl{I would like to find out}.  
\end{remark}

With this previous result we can now state the following claim:

\begin{prop}
Let $v(\bm{\xi}) = B(\xi_1,\bm{\mu}) \cdots B(\xi_k,\bm{\mu})\Omega$ where the parameters $\bm{\xi}$ satisfy 
\eq{
\prod_{j=1}^n \frac{\xi_i -\mu_j - 1}{\xi_i - \mu_j} = \prod_{l \neq i}^k \frac{\xi_i - \xi_l + 1}{\xi_i - \xi_l - 1}
}
Then 
\eq{
\psquare{A(\lambda,\bm{\mu}) + D(\lambda,\bm{\mu})}v(\bm{\xi}) = \pround{\prod_{j=1}^n \frac{\lambda -\mu_j - 1}{\lambda - \mu_j} \prod_{l =1}^k \frac{\lambda - \xi_l + 1}{\lambda - \xi_l }+ \prod_{l=1 }^k \frac{\lambda - \xi_l - 1}{\lambda - \xi_l }} v(\bm{\xi})
}
\end{prop}
\begin{proof}
Let us supress the $\bm{\mu}$ in notation as we know which rep space we are in. We see $B(\xi)\Omega$ is graphically 
\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1.5,0) node[anchor = east] {$1$} -- (2.5,0) node[anchor = west] {$0$};
    \draw[->] (-1,0.5) node[anchor = south] {$0$} -- (-1,-0.5)  ;
    \draw[->] (1,0.5) node[anchor = south] {$0$} -- (1,-0.5)  ;
    \draw[->] (-0.5,0.5) node[anchor = south] {$0$} -- (-0.5,-0.5)  ;
    \draw[->] (0.5,0.5) node[anchor = south] {$0$} -- (0.5,-0.5)  ;
    \draw[->] (0,0.5) node[anchor = south] {$0$}-- (0,-0.5)  ;
    \draw (1.5,0.5) node {$\cdots$};
    \draw (1.5,-0.5) node {$\cdots$};
    \draw[->] (2,0.5) node[anchor = south] {$0$} -- (2,-0.5) ;
    \end{tikzpicture}
\end{center}
A little bit of thinking gives that the non-zero sums come from terms like 
\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    \def\k{1}
    \def\l{5}
    \def\ans{$(\xi-\mu_1)^{-1}\prod_{a > 1}\psquare{1-(\xi-\mu_a)^{-1}}$}
    \foreach \le/\ri/\y in {1/0/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    \foreach \e/\x in {1/1,0/2,0/3,0/4,0/5,0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {$0$} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\e}  ;
    }
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}
\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    \def\k{1}
    \def\l{5}
    \def\ans{$(\xi-\mu_2)^{-1}\prod_{a > 2}\psquare{1-(\xi-\mu_a)^{-1}}$}
    \foreach \le/\ri/\y in {1/0/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    \foreach \e/\x in {0/1,1/2,0/3,0/4,0/5,0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {$0$} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\e}  ;
    }
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}
\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    \def\k{1}
    \def\l{5}
    \def\ans{$(\xi-\mu_3)^{-1}\prod_{a > 3}\psquare{1-(\xi-\mu_a)^{-1}}$}
    \foreach \le/\ri/\y in {1/0/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    \foreach \e/\x in {0/1,0/2,1/3,0/4,0/5,0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {$0$} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\e}  ;
    }
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}

Let us denote $\ket{a_1 a_2 \dots a_k}$ to be the state with $k$ 1s in the $a_1,a_2, \dots ,a_k$ positions. We can then rewrite 
\eq{
B(\xi)\Omega = \sum_{b\geq 1} (\xi - \mu_b)^{-1}\prod_{a > b}\psquare{1-(\xi-\mu_b)^{-1}} \ket{b}
}
We can then calculate the constituent terms in $A(\lambda)\ket{b}$ as 

\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    
    \def\k{1}
    \def\l{5}

    \def\ans{$\prod_{a \neq 1}\psquare{1-(\lambda-\mu_a)^{-1}}$}
    
    \foreach \le/\ri/\y in {0/0/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    
    \foreach \u/\d/\x in {1/1/1,0/0/2,0/0/3,0/0/4,0/0/5,0/0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {\u} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\d}  ;
    }
    
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}

\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    
    \def\k{1}
    \def\l{5}

    \def\ans{$(\lambda-\mu_1)^{-1}(\lambda-\mu_2)^{-1}\prod_{a>2}\psquare{1-(\lambda-\mu_a)^{-1}}$}
    
    \foreach \le/\ri/\y in {0/0/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    
    \foreach \u/\d/\x in {1/0/1,0/1/2,0/0/3,0/0/4,0/0/5,0/0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {\u} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\d}  ;
    }
    
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}

\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    
    \def\k{1}
    \def\l{5}

    \def\ans{$\prod_{a\neq 2}\psquare{1-(\lambda-\mu_a)^{-1}}$}
    
    \foreach \le/\ri/\y in {0/0/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    
    \foreach \u/\d/\x in {0/0/1,1/1/2,0/0/3,0/0/4,0/0/5,0/0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {\u} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\d}  ;
    }
    
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}

\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    
    \def\k{1}
    \def\l{5}

    \def\ans{$(\lambda-\mu_1)^{-1}(\lambda-\mu_3)^{-1}\prod_{a>3}\psquare{1-(\lambda-\mu_a)^{-1}}$}
    
    \foreach \le/\ri/\y in {0/0/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    
    \foreach \u/\d/\x in {1/0/1,0/0/2,0/1/3,0/0/4,0/0/5,0/0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {\u} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\d}  ;
    }
    
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}
In general, we see
\eq{
A(\lambda)\ket{b} &= 
\prod_{a \neq b} \psquare{1-(\lambda- \mu_a)^{-1}}
 \ket{b} + \sum_{c > b }(\lambda - \mu_b)^{-1} (\lambda - \mu_c)^{-1} \prod_{\substack{{a < b,} \\ {a > c}}} \psquare{1-(\lambda- \mu_a)^{-1}} \ket{c} \\
 %&= \prod_{a < b} \psquare{1 - (\lambda - \mu_a)^{-1}} \pbrace{\ket{b} + (\lambda-\mu_b)^{-1}\sum_{c > b}(\lambda - \mu_c)^{-1}\prod_{d > c} \psquare{1 - (\lambda - \mu_d)^{-1}}\ket{c}}
}
Note we always have that the $1$ is moving to the right. 
To continue to proceed let us now consider how $D$ acts on $B\Omega$: We see that terms in $D(\lambda)\ket{k}$ look like 

\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    
    \def\k{1}
    \def\l{5}

    \def\ans{$\psquare{1-(\lambda-\mu_1)^{-1}}$}
    
    \foreach \le/\ri/\y in {1/1/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    
    \foreach \u/\d/\x in {1/1/1,0/0/2,0/0/3,0/0/4,0/0/5,0/0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {\u} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\d}  ;
    }
    
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}

\begin{center}
    \begin{tikzpicture}
    \def\t{0.5}
    \def\s{-1.5}
    
    \def\k{1}
    \def\l{5}

    \def\ans{$\psquare{1-(\lambda-\mu_2)^{-1}}$}
    
    \foreach \le/\ri/\y in {1/1/1} {
    \draw[->] (\s,\t-0.5*\y) node[anchor = east] {\le} -- ({\s+0.5*(\l+3)},\t-0.5*\y) node[anchor = west] {\ri};
    }
    
    \foreach \u/\d/\x in {0/0/1,1/1/2,0/0/3,0/0/4,0/0/5,0/0/7} {
    \draw[->] ({\s+0.5*\x},\t) node[anchor = south] {\u} -- ({\s+0.5*\x},{\t-0.5*(\k+1)}) node[anchor = north] {\d}  ;
    }
    
    \draw ({\s+0.5*(\l+1)},\t) node {$\cdots$};
    \draw ({\s+0.5*(\l+1)},{\t-0.5*(\k+1)}) node {$\cdots$};
    
    \draw ({\s+0.5*(\l+4)},{\t - 0.25*(\k+1)}) node[anchor = west] {=} ;
    \draw ({\s+0.5*(\l+5)},{\t - 0.25*(\k+1)}) node[anchor = west] {\ans};
    \end{tikzpicture}
\end{center}

In general, we see 
\eq{
D(\lambda)\ket{b} = \psquare{1-(\lambda - \mu_b)^{-1}} \ket{b}
}
As such we get 
\eq{
\psquare{A(\lambda) + D(\lambda)}B(\xi)\Omega &= \sum_{b \geq 1} \psquare{A(\lambda) + D(\lambda)}  (\xi - \mu_b)^{-1}\prod_{a >b}\psquare{1-(\xi-\mu_a)^{-1}} \ket{b} \\
&= \sum_{b \geq 1}\left \lbrace (\xi - \mu_b)^{-1}\pround{\prod_{a > b}\psquare{1-(\xi-\mu_a)^{-1}}}\left[ \pround{\psquare{1-(\lambda - \mu_b)^{-1}} +  \prod_{a \neq b} \psquare{1 - (\lambda - \mu_a)^{-1}}}\ket{b} \right. \right. \\
&\phantom{=} \left. \left. + \sum_{c > b }(\lambda - \mu_b)^{-1} (\lambda - \mu_c)^{-1} \prod_{\substack{{a < b,} \\ {a > c}}} \psquare{1-(\lambda- \mu_a)^{-1}} \ket{c} \right] \right\rbrace \\
&= \sum_{b \geq 1}\left \lbrace (\xi - \mu_b)^{-1}\pround{\prod_{a > b}\psquare{1-(\xi-\mu_a)^{-1}}}\left( \psquare{1-(\lambda - \mu_b)^{-1}} +  \prod_{a \neq b} \psquare{1 - (\lambda - \mu_a)^{-1}} \right. \right. \\
&\phantom{=} \left. \left. + \sum_{c < b }(\lambda - \mu_b)^{-1} (\lambda - \mu_c)^{-1} \prod_{\substack{{a < c,} \\ {a > b}}} \psquare{1-(\lambda- \mu_a)^{-1}}  \right)\ket{b} \right\rbrace
}
Let us consider the multiplying term of $\ket{b}$
\eq{
\psquare{1-(\lambda - \mu_b)^{-1}} +  \prod_{a \neq b} \psquare{1 - (\lambda - \mu_a)^{-1}} + \sum_{c < b }(\lambda - \mu_b)^{-1} (\lambda - \mu_c)^{-1} \prod_{\substack{{a < c,} \\ {a > b}}} \psquare{1-(\lambda- \mu_a)^{-1}}
}

To complete the proof in the case $k=1$ we need that 
\eq{
\psquare{1-(\lambda - \mu_1)^{-1}} +  \prod_{a \neq 1} \psquare{1 - (\lambda - \mu_a)^{-1}} = \psquare{1-(\lambda - \xi)^{-1}} + \psquare{1+(\lambda - \xi)^{-1}}\prod_{a = 1}^n \psquare{1 - (\lambda - \mu_a)^{-1}} 
}
The condition we impose says 
\eq{
\prod_{a=1}^n \psquare{1 - (\xi - \mu_a)^{-1}} = 0 \Rightarrow \xi-\mu_b= 1 \quad \text{for some $b$} 
}
\hl{5: This doesn't look like it solves our problem}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Heisenberg spin chain}
We may now convert our algebraic picture into physics: Let $\mc{R}(u) = uR(iu) = u + i P$. If we then let the transfer matrix be 
\eq{
\tau(u) = \tr_{\mbb{C}^2} \mc{R}_{0n}(u- \frac{i}{2}) \dots \mc{R}_{01}(u- \frac{i}{2}) 
}
We then define the Hamiltonian of the XXX-model to be 
\eq{
H_{XXX} = \ev{i \frac{d}{du}\log \tau(u)}{u=\frac{i}{2}}
}
and the shift operator to be 
\eq{
e^{iP} = i^{-n} \tau(\frac{i}{2})
}
Taking each $\mu_j = \frac{1}{2}$ we interpret the rep space of the evaluation module to be a quantum mechanic state space with each vector being a spin configuration of a chain of $n$ spin-$\frac{1}{2}$ states. \\
Now $H_{XXX}$ governs dynamics via Schrodinger's equation. As is typical in QM we want to look for stationary states, which are eigenstates of the Hamiltonian. We have already found these to be $v(\bm{\xi})$ using the Bethe ansatz. 

\begin{lemma}
The Hamiltonian $H_{XXX}$ can be explicitly computed as 
\eq{
H_{XXX} = \sum_{j=1^n} (\sigma_j^x\sigma_{j+1}^{x}+\sigma_j^y\sigma_{j+1}^{y}+\sigma_j^z\sigma_{j+1}^{z})
}
where $\sigma_j^x, \dots$ are the Pauli matrices. 
\end{lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Representation theory of quantum affine algebras}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}
Let us work over $\mbb{C}$. Recall we define 
\eq{
\mf{sl}_2 = \pbrace{\begin{pmatrix}a & b \\ c & -a \end{pmatrix} \, | \, a,b,c \in \mbb{C}}
}
This algebra has a standard basis given by 
\eq{
e &= \begin{pmatrix}0 & 1 \\ 0 & 0 \end{pmatrix} \\
f &= \begin{pmatrix}0 & 0 \\ 1 & 0 \end{pmatrix} \\
h &= \begin{pmatrix}1 & 0 \\ 0 & -1 \end{pmatrix}
}
and these have commutation relations 
\eq{
\comm[e]{f} &= h \\
\comm[h]{e} &= 2e \\
\comm[h]{f} &= -2f
}
We then make a new definition of $\mf{sl}_2$ simply as the Lie algebra generated by $e,f,h$ with the above commutation relations. We can then study $\mf{sl}_2$ representations, i.e. LA homs $\mf{sl}_2 \to \End(V)$ for some vector space $V$ with the bracket on the RHS being commutation. We have some natural reps;
\begin{itemize}
    \item Fundamental rep with $V = \mbb{C}^2$, and sending $e \mapsto \begin{psmallmatrix} 0 & 1 \\ 0 & 0 \end{psmallmatrix} $, etc. 
    \item Adjoint rep with $V = \mbb{C}^3$, sending $e \mapsto \begin{psmallmatrix} 0 & 2 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{psmallmatrix}$, etc. 
\end{itemize}

For any LA $\mf{g}$ we have the Universal enveloping algebra $U(\mf{g}) = \faktor{T(\mf{g})}{I}$ where $I = \pangle{x \otimes y - y \otimes x - \comm[x]{y}}$ is an ideal. We get a natural embedding $i:\mf{g} \hookrightarrow U(\mf{g})$\footnote{\hl{Supposedly we ned the Poicare Birkhoff Witt theorem to show this is indeed an embedding - look into this}}. 
\begin{prop}
The pair $(U(\mf{g}),i)$ has the universal property that given $A$ an associative algebra, and a LA hom $\phi:\mf{g} \to A^{Lie}$, $\exists ! \bar{\phi} : U(\mf{g}) \to A$ an associative algebra hom, s.t. the diagram 
\begin{tkz}
U(\mf{g}) \arrow[dr,dashed, "\exists ! \bar{\phi}"] & \\
\mf{g} \arrow[u,"i",hook] \arrow[r,"\phi"'] & A 
\end{tkz}
commutes. 
\end{prop}

\begin{idea}
$U(\mf{g})$ contains all the information of reps of $\mf{g}$, taking $A = \End(V)$ for $V$ a rep space. It then turns out the concept of a Casimir is well defined in the UEA (\hl{include a discussion on this later}), and so classifying reps of the UEA by acting the casimirs on $V$ is possible, and this lets us learn about reps of $\mf{g}$. 
\end{idea}

\begin{example}
$U(\mf{sl}_2)$ is the unital associative algebra  defined by $e,f,h$ with the commutation relations. 
\end{example}

\begin{prop}
$U(\mf{g})$ is a co-commutative Hopf algebra with co-product 
\eq{
\Delta : U(\mf{g}) &\to U(\mf{g}) \otimes U(\mf{g}) \\
\mf{g} \ni x &\mapsto 1 \otimes x + x \otimes 1  
}
and counit $\eps(x) = 0$, antipode $S(x) = -x$.
\end{prop}

Now we want to deform the Hopf algebra structure to get a non-commutative Hopf algebra that is now quasi-triangular (braided) 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Hopf algebra \secmath{U_q(\mf{sl}_2)}}

Let $q \in \mbb{C}\setminus \pbrace{-1,0,1}$. 

\begin{definition}
$U_q(\mf{sl}_2)$ is the unital associative $\mbb{C}$-algebra
on the generators $e,f, t^{\pm 1}$ with relations 
\eq{
t e t^{-1} &= q^2 e \\
t f t^{-1} &= q^{-2} f \\
tt^{-1} &= 1 = t^{-1}t \\
\comm[e]{f} &= \frac{t - t^{-1}}{q - q^{-1}}
}
If we consider the limit $q \to 1$ we should recover $U(\mf{sl}_2)$.
\end{definition}

\begin{ex}
Justify the relations of $U_q(\mf{g})$ by 
\begin{enumerate}
    \item Formally setting $t = q^h$
    \item Let $q \to 1$
    \item Recover $U(\mf{sl}_2)$
\end{enumerate}
For example, consider the relation $t e t^{-1} = q^2 e$. In $U(\mf{sl}_2)$ the relation is $\comm[h]{e} = 2e \Rightarrow he =e(h+2)$. Now 
\begin{enumerate}
    \item Let $t = q^h = \sum_{m \geq 0} \frac{1}{m!} (\log q)^m h^m$. Then 
    \eq{
    te &=  \sum_{m \geq 0} \frac{1}{m!} (\log q)^m h^m e\\
    &=  \sum_{m \geq 0} \frac{1}{m!} (\log q)^m e(h+2)^m \\
    &= \dots \\
    &= q^2 et
    }
\end{enumerate}
\end{ex}

\begin{lemma}
The map 
\eq{
U_q(\mf{sl}_2) &\to U_q(\mf{sl}_2) \\
e &\mapsto f \\
f &\mapsto e \\
t &\mapsto t^{-1}
}
is an algebra automorphism. 
\end{lemma}

\begin{lemma}
$\forall r \in \mbb{Z}_{\geq 1}$
\eq{
\comm[e]{f^r} &= q^{1-r}[r] \frac{f^{r-1}t - t^{-1}f^{r-1}}{q-q^{-1}}, & \comm[e^r]{f} &= q^{1-r}[r] \frac{te^{r-1} - e^{r-1}t^{-1}}{q-q^{-1}}
}
\end{lemma}
\begin{proof}
Prove by induction given the base case $r=1$, which is by definition. Note once one of the relations has been proved, the second following by applying the previous automorphism. 
\end{proof}

We can give $U_q(\mf{sl}_2)$ a Hopf algebra structure by having 
\eq{
\Delta(e) &= e \otimes 1 +t \otimes e \\
\Delta(f) &= f \otimes t^{-1} + 1 \otimes f \\
\Delta(t^{\pm 1}) &= t^{\pm 1} \otimes t^{\pm 1} \\
\eps(e) &= 0 = \eps(f) \\
\eps(t^{\pm 1}) &= 1 \\
S(e) &= -t^{-1} e \\
S(f) &= - ft \\
S(t^{\pm 1}) &= t^{\mp}
}

\begin{ex}
Show that $\Delta, \eps, S$ preserve algebra relations. For example we have 
\eq{
S(ef - fe - \frac{t - t^{-1}}{q - q^{-1}}) &= S(f) S(e) - S(e) S(f) - \frac{S(t) - S(t^{-1})}{q -q^{-1}} \\
&= (-ft)(-t^{-1}e) - (-t^{-1}e)(-ft) - \frac{t - t^{-1}}{q - q^{-1}} \\
&= ef - fe - \frac{t - t^{-1}}{q - q^{-1}} = 0
}
\end{ex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\secmath{\text{Irreps of $U_q(\mf{sl}_2)$}}}

\begin{definition}
Choose a basis $v_0,v_1$ of $\mbb{C}^2$ and wrt this basis define 
\eq{
\pi(e) &= \begin{pmatrix}0 & 1 \\ 0 & 0 \end{pmatrix} \\
\pi(f) &= \begin{pmatrix}0 & 0 \\ 1 & 0 \end{pmatrix} \\
\pi(t) &= \begin{pmatrix}q & 0 \\ 0 & q^{-1} \end{pmatrix} = q^{\begin{psmallmatrix} 1 & 0 \\ 0 & -1\end{psmallmatrix} }
}
i.e. $\pi(e)(v_1) = v_0, \, \pi(f)(v_0) = v_1, \, \pi(t)(v_0) = qv_0, \, \pi(t)(v_1) = q^{-1}v_1$. The linear map $\pi:U_q(\mf{sl}_2) \to \End(\mbb{C}^2)$ is called the \bam{vector rep} of $U_q(\mf{sl}_2)$
\end{definition}

\begin{ex}
Check that $\pi$ is a LA hom. E.g. 
\eq{
\pi\pround{\frac{t-t^{-1}}{q-q^{-1}}} &= \frac{\pi(t) - \pi(t)^{-1}}{q-q^{-1}} = \frac{1}{q-q^{-1}}\psquare{\begin{pmatrix}q & 0 \\ 0 & q^{-1} \end{pmatrix} - \begin{pmatrix}q^{-1} & 0 \\ 0 & q \end{pmatrix}} = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} \\
\pi(\comm[e]{f}) &= \pi(e) \pi(f) - \pi(f) \pi(e) =  \begin{pmatrix}0 & 1 \\ 0 & 0 \end{pmatrix}\begin{pmatrix}0 & 0 \\ 1 & 0 \end{pmatrix} -  \begin{pmatrix}0 & 0 \\ 1 & 0 \end{pmatrix}\begin{pmatrix}0 & 1 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}
}
\end{ex}

\begin{definition}
We let 
\eq{
[n] = \frac{q^n - q^{-n}}{q - q^{-1}} = q^{n-1} + q^{n-3} + \dots + q^{3-n} + q^{1-n}
}
and call it a \bam{q-integer}
\end{definition}

\begin{lemma}
The q-integers obey 
\begin{itemize}
    \item $[1] = 1$
    \item $\lim_{q \to 1}[n] = n$
\end{itemize}
\end{lemma}

\begin{definition}
Let $V^{(n)} = \mbb{C} v_0^{(n)} \oplus \dots \oplus \mbb{C}v_n^{(n)}$ for $n \geq 0$. Note $\dim V^{(n)} = n+1$. Set 
\eq{
\pi^{(n)}(e)(v_i^{(n)}) &= [n-i+1] v_{i-1}^{(n)} \\
\pi^{(n)}(f )(v_i^{(n)}) &= [i+1] v_{i+1}^{(n)} \\
\pi^{(n)}(t)(v_i^{(n)}) &= q^{n-2i} v_{i}^{(n)}
}
where $v_{-1}^{(n)} = 0 = v_{n+1}^{(n)}$. This is called the \bam{Spin}-$\mathbf{\frac{n}{2}}$ rep of $U_q(\mf{sl}_2)$. 
\end{definition}

\begin{remark}
Considering the $n=0$ example, we get $V^{(0)} \cong \mbb{C}$, and 
\eq{
\pi^{(0)}(e) v_0^{(0)} &= 0 = \pi^{(0)}(f) v_0^{(0)} \\
\pi^{(0)}(t) v_0^{(0)} &= v_0^{(0)}
}
i.e. we have the trivial rep. We also can see $\pi^{(1)} = \pi$. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\secmath{\text{Tensor products of $U_q(\mf{sl}_2)$ reps}}}

\begin{idea}
It turns out we want a counit to have trivial reps, antipodes to have dual reps, and coproducts in order to have tensor products of reps. 
\end{idea}

Use the coproduct as follows: 
\eq{
((\pi_1,V_1),(\pi_2,V_2)) &\mapsto (\pi_{12},V_1 \otimes V_2)
}
where we take 
\eq{
\pi_{12}(x) = (\pi_1 \otimes \pi_2)(\Delta(x)) \in \End(V_1) \otimes \End(V_2) \cong \End(V_1 \otimes V_2)
}

\begin{theorem}
$\forall n,m \in \mbb{Z}_{\geq 0}$ we have 
\eq{
V^{(n)} \otimes V^{(m)} \cong V^{(n+m)} \oplus V^{(n+m-2)} \oplus \dots \oplus V^{(\abs{n-m})}
}
as $U_q(\mf{sl}_2)$-modules
\end{theorem}

\begin{example}
Take $n=m=1$ so $V^{(1)} \otimes V^{(1)} \cong V^{(2)} \oplus V^{(0)}$. Letting $u_0 = v_0^{(1)} \otimes v_0^{(1)}$ we have   
\eq{
e \cdot u_0 &=  \Delta(e)(v_0^{(1)} \otimes v_0^{(1)}) = (e \otimes 1 + t \otimes e)(v_0^{(1)} \otimes v_0^{(1)})  = 0 \\
[1]u_1 = f \cdot u_0 &=(f \otimes t^{-1} + 1 \otimes f)(v_0^{(1)} \otimes v_0^{(1)}) = q^{-1}v_1^{(1)} \otimes v_0^{(1)} + v_0^{(1)} \otimes v_1^{(1)} \\
[2]u_2 = f \cdot u_1 &=(f \otimes t^{-1} + 1 \otimes f)(q^{-1}v_1^{(1)} \otimes v_0^{(1)} + v_0^{(1)} \otimes v_1^{(1)})= (q+q^{-1})v_1^{(1)} \otimes v_1^{(1)} \\
f \cdot u_2 &= 0
}
Hence $W^{(2)} = \mbb{C} u_0 \oplus \mbb{C} u_1 \oplus \mbb{C} u_2$ is a submodule. We have 
\eq{
W^{(2)} &\overset{\cong}{\to} V^{(2)} \\
u_i &\mapsto v_i^{(2)}
}
If we let $\tilde{u} = q v_1^{(1)} \otimes v_0^{(1)} - v_0^{(1)} \otimes v_1^{(1)}$ we find $e \cdot \tilde{u} = 0 = f \cdot \tilde{u}, \, t \cdot \tilde{u} = \tilde{u}$, and so we have the submodule $W^{(0)} = \mbb{C} \tilde{u} \cong V^{(0)}$. Hence we have found 
\eq{
V^{(1)} \otimes V^{(1)} = W^{(2)} \oplus W^{(0)} \cong V^{(2)} \oplus V^{(0)}
}
as the theorem states. As $q \to 1$, $W^{(2)}$ are symmetric tensors in $V^{(1)} \oplus V^{(1)}$ and $W^{(0)}$ are antisymmetric. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\secmath{U_q(\hat{\mf{sl}_2})}}

We now need to introduce another quantum parameter, so we will take $z$, the \bam{loop parameter}, which will be related to the spectral parameter $u$ by $z = e^{2u}$. \\
We will follow work by Drinfeld and Jimbo: deform the UEA of a Kac-Moody Lie algebra. Kac-Moody algebras include all finite-dimensional semi-simple Lie algebras through the Chevally-Serre presentation, but that also include loop algebras of the semi-simple LAs. This information can be represented in terms of Dynkin diagrams, or equivalently Cartan matrices. \\

\begin{definition}
$U_q(\hat{\mf{sl}_2})$ is the unital associative algebra on the generators $e_i,f_i, t_i^{\pm 1}$, $i =0,1$ with 
\eq{
t_i e_j t_i^{-1} &= q^{a_{ij}} e_j \\
t_i f_j t_i^{-1} &= q^{-a_{ij}} f_j \\
\comm[e_i]{f_j} &= \delta_{ij}\frac{t_i - t_i^{-1}}{q-q^{-1}} \\
t_i t_i^{-1} &= 1 =t_i^{-1} t_i \\
t_i t_j &= t_j t_i \\
e_i e_j^3 - [3] e_j e_i e_j^2 + [3] e_j^2 e_i e_j - e_j^3 e_i &= 0 \quad \text{(the $\mathbf{q}$\textbf{-Serre relations}, also exists for f)} 
}
Here $a = \begin{psmallmatrix} 2 & -2 \\ -2 & 2 \end{psmallmatrix}$ is the Cartan matrix. The is called the \bam{Chevalley generator realisation}.
\end{definition}

\begin{remark}
The definition for the q-deformed UEA for a different Kac-Moody algebra behaves the same using its corresponding Cartan matrix. We get one example of this in $U_q(\mf{sl}_2)$, where the Cartan matrix is just $a=2$. 
\end{remark}

\begin{prop}
We can make $U_q(\hat{\mf{sl}_2})$ into a Hopf algebra with 
\eq{
\Delta(e_i) &= e_i \otimes 1 + t_i \otimes e_i \\
\Delta(f_i) &= f_i \otimes t_i^{-1} + 1 \otimes f_i \\
\Delta(t_i) &= t_i \otimes t_i \\
\eps(e_i) &= 0 \\
\eps(f_i) &= 0 \\
\eps(t_i) &= 1 \\
S(e_i) &= - t_i^{-1} e_i \\
S(f_i) &= -f_i t_i \\
S(t_i) &= t_i^{-1}
}
\end{prop}

\begin{definition}
Given a Lie algebra $\mf{g}$ we can define 
\begin{itemize}
    \item The loops algebra $L\mf{g} = \mf{g} \otimes C^\infty(S^1) = \mf{g} \otimes \mbb{C}[z,z^{-1}] $
    \item The affine Lie algebra $\hat{\mf{g}} = L\mf{g} \oplus \mbb{C}c$
    \item The extended affine Lie algebra $\tilde{\mf{g}} = \hat{\mf{g}} + \mbb{C}d$. 
\end{itemize}
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation reps of \secmath{U_q(\hat{\mf{sl}_2})}}

\begin{lemma}
Let $z \in \mbb{C}^\times$ be spectral parameter. Then $\exists ev_z: U_q(\hat{\mf{sl}_2}) \to U_q(\mf{sl}_2)$ a LA hom s.t. 
\eq{
ev_z(e_0) &= zf \\
ev_z(f_0) &= z^{-1}e \\
ev_z(t_0) &= t^{-1} \\ \\
ev_z(e_1) &= e \\
ev_z(f_1) &= f \\
ev_z(t_1) &= t
}
\end{lemma}
\begin{ex}
Check that the relations of $U_q(\hat{\mf{sl}_2})$ are preserved. E.g. 
\eq{
ev_z\pround{\comm[e_0]{f_0} - \frac{t_0 - t_0^{-1}}{q - q^{-1}}} &= \comm[zf]{z^{-1}e} - \frac{t^{-1} - t}{q - q^{-1}} \\
&= \comm[f]{e} + \frac{t - t^{-1}}{q - q^{-1}} = 0 
}
\end{ex}


This means we can now get the diagram 
\begin{tkz}
U_q(\hat{\mf{sl}_2}) \arrow[r,"ev_z"] \arrow[dr,"\pi_z^{(n)}"']& U_q(\mf{sl}_2)  \arrow[d,"\pi^{(n)}"] \\
& \End(V^{(n)})
\end{tkz}
In this representation we call the rep space $V_z^{(n)}$.

\begin{example}
When $n=1$ the rep space is $V^{(1)}_z\cong \mbb{C}^2$, and the rep $\pi_z^{(1)}$ is 
\eq{
\pi_z^{(1)}(e_0) &= \begin{pmatrix} 0 & 0 \\ z & 0\end{pmatrix} = z \pi_z^{(1)}(f_1)\\ 
\pi_z^{(1)}(f_0) &= \begin{pmatrix} 0 & z^{-1} \\ 0 & 0\end{pmatrix} = z^{-1} \pi_z^{(1)}(e_1) \\ 
\pi_z^{(1)}(t_0) &= \begin{pmatrix} q & 0 \\ 0 & q^{-1}\end{pmatrix} = \pround{\pi_z^{(1)}(t_1)}^{-1} 
}
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Tensor products}

\begin{theorem}
$V_{z_1}^{(1)} \otimes V_{z_2}^{(1)}$ is irreducible as a $U_q(\hat{\mf{sl}_2})$-module iff $z_1 \neq z_2 q^{\pm 2}$. Otherwise we get two SESs
\begin{tkz}
(i) & 0 \arrow[r] & V_z^{(2)} \arrow[r,"i"] & V_{zq^{-1}}^{(1)} \otimes V_{zq}^{(1)} \arrow[r,"\tau"] & V_z^{(0)} \arrow[r] & 0 \\
(ii) & 0 \arrow[r] & V_z^{(0)} \arrow[r,"i^\prime"] & V_{zq}^{(1)} \otimes V_{zq^{-1}}^{(1)} \arrow[r,"\tau^\prime"] & V_z^{(2)} \arrow[r] & 0 
\end{tkz}
\end{theorem}
\begin{proof}
\hl{$(i)$ is an exercise:} \\
For the second sequence, consider $w = v_0 \otimes v_1 - q v_1 \otimes v_0 \in V_{z_1}^{(1)} \otimes V_{z_2}^{(1)}$. We may calculate
\eq{
(\pi_{z_1}^{(1)} \otimes \pi_{z_2}^{(1)}) (\Delta(e_1))(w) &= (\pi_{z_1}^{(1)} \otimes \pi_{z_2}^{(1)})(e_1 \otimes 1 + t_1 \otimes e_1)(v_0 \otimes v_1 - q v_1 \otimes v_0) \\
&= (\pi_{z_1}^{(1)}(e_1) \otimes 1 + \pi_{z_1}^{(1)}(t_1) \otimes \pi_{z_2}^{(1)}(e_1))(v_0 \otimes v_1 - q v_1 \otimes v_0) \\
&= - v_0 \otimes v_0 + q^{-1}q v_0 \otimes v_0 = 0 
}
It can also be shown 
\eq{
(\pi_{z_1} \otimes \pi_{z_2}) (\Delta(f_1))(w) &= 0
}
Hence $\mbb{C}w$ is a 1-dimensional submodule of $U_q(\mf{sl}_2)$. However 
\eq{
(\pi_{z_1} \otimes \pi_{z_2}) (\Delta(e_0))(w) &= (z_1 - q^2 z_2) v_1 \otimes v_1
}
and so this is 0 iff $z_1 = z_2 q^2$, meaning we get a $U_q(\hat{\mf{sl}}_2)$ iff $z_1 = z_2 q^2$ . \\
\hl{To finish this proof, use example on tensor product of reps as direct sums}. 
\end{proof}

\begin{remark}
The SES $(i)$ tells us that $V_z^{(2)} \hookrightarrow V_{zq^{-1}}^{(1)} \otimes V_{zq}^{(1)}$ is an embedding, and that 
\eq{
\faktor{V_{zq^{-1}}^{(1)} \otimes V_{zq}^{(1)}}{\image(\tau)} \cong V_z^{(0)}
}
\end{remark}

\hl{Now we do know that $V_{zq^{-1}}^{(1)} \otimes V_{zq}^{(1)} \cong V_z^{(2)} \oplus V_z^{(0)}$ as vector spaces from the above.(Had this copied down from lectures, but think this is incorrect now - think about it)}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{R-matrices}

We have seen that if $z_i$ are generic then we have $V_{z_1}^{(1)} \otimes V_{z_2}^{(2)}$, $V_{z_2}^{(1)} \otimes V_{z_1}^{(2)}$ are irreducible as $U_q(\hat{\mf{sl}_2})$-modules. \\
We will see they are isomorphic as the modules, as it turns out $\exists \check{R}(z_1, z_2) : V_{z_1}^{(1)} \otimes V_{z_2}^{(2)} \to V_{z_2}^{(1)} \otimes V_{z_1}^{(2)}$ invertible s.t. 
\eq{
\check{R}(z_1, z_2)(\pi_{z_1} \otimes \pi_{z_2})(\Delta(x)) = (\pi_{z_2} \otimes \pi_{z_1})(\Delta(x)) \check{R}(z_1, z_2)
}

\begin{lemma}[Schur's lemma]
If $A$ is a $\mbb{C}$-algebra and $V, W$ are irreps of $A$, then if $\exists \phi_1, \phi_2 : V \to W$ non-zero homomorphisms of $A$-modules then 
\begin{enumerate}
    \item these are isomorphisms
    \item If $V=W$, then $\exists \lambda_i \in \mbb{C}$ s.t. $\phi_i = \lambda_i \id$
    \item $\exists \lambda \in \mbb{C}$ s.t. $\phi_1 = \lambda \phi_2$
\end{enumerate}
\end{lemma}
\begin{proof}
$1)$ First note that $\ker \phi_i, \image \phi_i $ are submodules of $V,W$ respectively as 
\eq{
\forall v \in \ker\phi_i, \, z \in \mbb{C}, \, \phi_i(z \cdot v) = z \cdot \phi_i(v) = 0 &\Rightarrow z \cdot v \in \ker\phi_i \\
\forall w=\phi_i(u) \in \image\phi_i, \, z \in \mbb{C}, \, z\cdot w = \phi_i(z \cdot u) &\Rightarrow z\cdot w \in \image \phi_i 
}
They are also clearly closed under addition. Then as neither $V,W$ have non-trivial proper submodules, and $\phi_i \neq 0$, we must have $\ker\phi_i = 0, \, \image \phi_i = W \Rightarrow \phi_i$ is an iso. \\
$2)$ We know $\phi_i$ must have an eigenvalue \hl{(I know this to be true for f.d. vector spaces by fundamental theorem of algebra, why in general?)} $\lambda_i$. Then the map 
\eq{
\phi_i -\lambda_i \id : W \to W
}
must have a non-zero kernel, which by irreducibility we know must be all of $V$, hence $\phi_i = \lambda_i \id$. \\
$3)$ Consider $\phi_1 \circ \phi_2^{-1} : W \to W$. by $2)$ there must be $\lambda \in \mbb{C}$ s.t. $\phi_1 \circ \phi_2^{-1} = \lambda \id \Rightarrow \phi_1 = \lambda \phi_2$. 
\end{proof}

\begin{prop}
$\exists ! \, \check{R}$ satisfying the required condition normalised s.t. 
\eq{
\check{R}(z_1, z_2) (v_0^{(1)} \otimes v_0^{(1)}) = v_0^{(1)} \otimes v_0^{(1)}
}
\end{prop}
\begin{proof}
Uniqueness comes from Schur's lemma, and the answer is explicitly 
\eq{
\check{R}(z_1, z_2) = \begin{pmatrix} 1 & 0 & 0 & 0 \\
0 & \frac{z_1(q-q^{-1})}{z_1 q - z_2 q^{-1}} & \frac{z_1 - z_2}{z_1 q - z_2 q^{-1}} & 0 \\
0 & \frac{z_1 - z_2}{z_1 q - z_2 q^{-1}} & \frac{z_2(q-q^{-1})}{z_1 q - z_2 q^{-1}} & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
}
\end{proof}

We can then define an $R$-matrix by $R = P\circ \check{R} : V_{z_1}^{(1)} \otimes V_{z_2}^{(1)} \to V_{z_1}^{(1)} \otimes V_{z_2}^{(1)}$, where recall $P(v \otimes w) = w \otimes v$.  

\begin{prop}\label{prop:CQIS:introducinguniversalR}
$R(z_1, z_2)(\pi_{z_1} \otimes \pi_{z_2})(\Delta(x)) = (\pi_{z_2}\otimes \pi_{z_1})(\Delta^\prime) R(z_1, z_2)$, where $\Delta^\prime = P \Delta P$ is the opposite ordered coproduct.
\end{prop}
\begin{proof}
We can see 
\eq{
R(z_1, z_2)(\pi_{z_1} \otimes \pi_{z_2})(\Delta(x)) &= P \check{R}(z_1, z_2)(\pi_{z_1} \otimes \pi_{z_2})(\Delta(x)) \\
&= P(\pi_{z_2} \otimes \pi_{z_1})(\Delta(x)) \check{R}(z_1, z_2) \\
&= P(\pi_{z_2} \otimes \pi_{z_1})(\Delta(x)) P R(z_1, z_2)
}
\hl{It would be good to 100\% understand why I can then pull the $P$ inside the representation, which I'm not sure now}. Note that where I have written $\Delta^\prime = P\Delta P$ I mean 
\eq{
(P \Delta P)(x) &= P(\Delta(P(x))) = P(\Delta(x))
}
\end{proof}

\begin{remark}
Note $\check{R}(z_1,z_2)$, and hence $R(z_1,z_2)$ only depend on $\frac{z_1}{z_2}$.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Yang-Baxter equation}
Noting that $V_{z_1}^{(1)} \otimes V_{z_2}^{(1)} \otimes V_{z_3}^{(1)}$ is generically irreducible, we can construct an isomorphism to $V_{z_3}^{(1)} \otimes V_{z_2}^{(1)} \otimes V_{z_1}^{(1)}$ in two ways: 

\begin{center}
    \begin{tikzpicture}[commutative diagrams/every diagram]
\node (P0) at (90:2.6cm) {$V_{z_1}^{(1)} \otimes V_{z_2}^{(1)} \otimes V_{z_3}^{(1)}$};
\node (P1) at (90+60:2.3cm) {$V_{z_2}^{(1)} \otimes V_{z_1}^{(1)} \otimes V_{z_3}^{(1)}$} ;
\node (P2) at (90+2*60:2.3cm) {$V_{z_2}^{(1)} \otimes V_{z_3}^{(1)} \otimes V_{z_1}^{(1)}$};
\node (P3) at (90+3*60:2.6cm) {$V_{z_3}^{(1)} \otimes V_{z_2}^{(1)} \otimes V_{z_1}^{(1)}$};
\node (P4) at (90+4*60:2.3cm) {$V_{z_1}^{(1)} \otimes V_{z_3}^{(1)} \otimes V_{z_2}^{(1)}$};
\node (P5) at (90+5*60:2.3cm) {$V_{z_3}^{(1)} \otimes V_{z_1}^{(1)} \otimes V_{z_2}^{(1)}$};

\path[commutative diagrams/.cd, every arrow, every label]
(P0) edge node[swap] {$\check{R}(z_1,z_2) \otimes 1$} (P1)
(P1) edge node[swap] {$1 \otimes \check{R}(z_1,z_3)$} (P2)
(P2) edge node[swap] {$\check{R}(z_2,z_3) \otimes 1$} (P3)
(P0) edge node {$1 \otimes \check{R}(z_2,z_3)$} (P5)
(P5) edge node {$\check{R}(z_1,z_3) \otimes 1$} (P4)
(P4) edge node {$1 \otimes \check{R}(z_1,z_2)$} (P3) ;
\end{tikzpicture}
\end{center}
By Schur's lemma, they are proportional, and computing the action on $v_0 \otimes v_0 \otimes v_0$ we find they are equal, so
\eq{
\psquare{\check{R}(z_2,z_3) \otimes 1} \psquare{1 \otimes \check{R}(z_1,z_3)} \psquare{\check{R}(z_1,z_2) \otimes 1} = \psquare{1 \otimes \check{R}(z_1,z_2)}\psquare{\check{R}(z_1,z_3) \otimes 1}\psquare{1 \otimes \check{R}(z_2,z_3)}
}
this is the \bam{Yang-Baxter equation (YBE)} or the \bam{braid relation}. 
\begin{prop}
In terms of $R$ this becomes the standard CYBE. 
\end{prop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Universal R-matrix}
So far we have discussed Hopf algebras $U$ with coproduct $\Delta$, counit $\eps$, and antipode $S$. Now consider a Hopf algebra equipped with some extra structure 

\begin{definition}
A \bam{universal R-matrix} is $\mc{R} \in U \otimes U$ invertible that satisfies $\forall x \in U$,
\begin{enumerate}
    \item $\forall x \in U, \, \mc{R}\Delta(x) = \Delta^\prime(x) \mc{R}$
    \item $(\Delta \otimes 1)\mc{R} = \mc{R}_{13} \mc{R}_{23}$
    \item $(1 \otimes \Delta)\mc{R} = \mc{R}_{13} \mc{R}_{12}$
\end{enumerate}
A Hopf algebra is called \bam{quasitriangular} (or \bam{braided}) if it has a universal R-matrix. If $\mc{R}^{-1} = P\mc{R}$, then the Hopf algebra is called \bam{triangular} 
\end{definition}

\begin{remark}
\hl{I do not know right now what is the deeper meaning of a triangular Hopf algebra, but supposedly it means the difference between the category of $H$-modules being a braided monoidal category and a symmetric monoidal category. }
\end{remark}


\begin{prop}
$\mc{R}$ satisfies the algebraic YBE in $U \otimes U \otimes U$
\eq{
\mc{R}_{23} \mc{R}_{13} \mc{R}_{12} = \mc{R}_{12} \mc{R}_{13} \mc{R}_{23}
}
\end{prop}
\begin{proof}
Use the notation $\mc{R} = \sum_i A_i \otimes B_i$. Then the LHS is 
\eq{
\mc{R}_{23} (1 \otimes \Delta)\mc{R} &= \mc{R}_{23} \psquare{\sum_i A_i \otimes \Delta(B_i)} \quad \text{(expanding 3)} \\
&= \psquare{\sum_i A_i \otimes \Delta^\prime(B_i)} \mc{R}_{23} \quad \text{(swapping $R$ and $\Delta$)} \\
&= P_{23} \psquare{\sum_i A_i \otimes \Delta(B_i)} P_{23} \mc{R}_{23} \quad \text{(using def of $\Delta^\prime)$} \\
&= P_{23} \mc{R}_{13} \mc{R}_{12} P_{23} \mc{R}_{23} = \mc{R}_{12} \mc{R}_{13} \mc{R}_{23} \quad \text{(using 3 in reverse and then applying $P$) } 
}
\end{proof}

\begin{theorem}
It is possible to construct such an $\mc{R}$ for $U_q(\mf{g})$, and $\mc{R}$ lies in an extension of $U_q(\mf{g}) \otimes U_q(\mf{g})$. 
\end{theorem}

Given reps $(\pi_V,V), (\pi_W,W)$ of some Hopf algebra $U$, we can construct $\mc{R}_{VW}  : V \otimes W \to V \otimes W $ by 
\eq{
\mc{R}_{VW} = (\pi_V \otimes \pi_W) \mc{R}
}
\begin{prop}
$\mc{R}_{VW}$ satisfies 
\begin{enumerate}
    \item $\forall x \in U, \mc{R}_{VW}(\pi_V \otimes \pi_W) \Delta(x) = (\pi_V \otimes \pi_W) (\Delta^\prime(x)) \mc{R}_{VW}$.
    \item $\mc{R}_{V_1 \otimes V_2,W} = \mc{R}_{V_1 ,W}\mc{R}_{V_2,W}$
    \item $\mc{R}_{V,W_1 \otimes W_2} = \mc{R}_{V,W_1} \mc{R}_{V,W_2}$
    \item $\mc{R}_{V_1,V_2}\mc{R}_{V_1,V_3} \mc{R}_{V_2,V_3} = \mc{R}_{V_2,V_3} \mc{R}_{V_1,V_3} \mc{R}_{V_1,V_2}$
\end{enumerate}
\end{prop}

\begin{example}
$R(z_1,z_2) = \mc{R}_{V_{z_1}^{(1)},V_{z_2}^{(1)}} = (\pi_{V_{z_1}^{(1)}} \otimes \pi_{V_{z_2}^{(1)}})\mc{R}$, where here $U= U_q(\hat{\mf{sl}}_2)$ is our Hopf algebra. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The RTT relation}

\begin{prop}
Letting $z = q^{-2u}, q = e^{\eta}$, we get 
\eq{
R(z) = \begin{pmatrix} \sinh(\eta(1-u)) & 0 & 0 & 0 \\
0 & \sinh(-\eta u) & e^{\eta u}\sinh(\eta) & 0 \\
0 & e^{-\eta u}\sinh(\eta) & \sinh(-\eta u) & 0 \\
0 & 0 & 0 & \sinh(\eta(1-u)) 
\end{pmatrix} = -u\eta (1-u^{-1}P) + \mc{O}(\eta^2)
}
\end{prop}
\begin{prop}
Letting $z = \frac{z_1}{z_2}$, recall that 
\eq{
\check{R}\pround{z} = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 
0 & \frac{z(q-q^{-1})}{zq-q^{-1}} & \frac{z-1}{zq-q^{-1}} & 0 \\
0 & \frac{z-1}{zq-q^{-1}} & \frac{(q-q^{-1})}{zq-q^{-1}} & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
}
using $R = P \check{R}$ we get (up to a rescaling)
\eq{
R(z) = \frac{1}{zq-q^{-1}}\begin{pmatrix} zq-q^{-1} & 0 & 0 & 0 \\ 
0 & z-1 & q-q^{-1} & 0 \\
0 & z(q-q^{-1}) & z-1 & 0 \\
0 & 0 & 0 & zq-q^{-1}
\end{pmatrix}
}
Making the substitution $z = q^{-2u}, \, q = e^{\eta}$, gives 
\eq{
z &= e^{-2\eta u}\\ 
\Rightarrow z-1 &= 2e^{-\eta u}\sinh(-\eta u) \\
zq-q^{-1} &= z^{\frac{1}{2}}\psquare{(z^\frac{1}{2}q) - (z^\frac{1}{2}q)^{-1}} \\
&= 2e^{-\eta u}\sinh(\eta(1-u)) \\
q-q^{-1} &= 2\sinh(\eta)
}
Thus 
\eq{
R(z) = \frac{1}{\sinh(\eta(1-u))}\begin{pmatrix} \sinh(\eta(1-u)) & 0 & 0 & 0 \\ 
0 & \sinh(-\eta u) & e^{\eta u}\sinh(\eta) & 0 \\
0 & e^{-\eta u}\sinh(\eta) & \sinh(-\eta u) & 0 \\
0 & 0 & 0 & \sinh(\eta(1-u))
\end{pmatrix}
}
\end{prop}

\begin{remark}
We can view our $R(z)$ as a deformation of Yang's R-matrix. As such we can repeat the approach of defining Hopf algebra structure from an R-matrix, with out new matrix. \\
The two approaches can be seen symbolically as the following: 
\eq{
R(u) &\to \text{Hopf algebra} \\
\text{Hopf algebra} + \mc{R} &\to R(z)
}
It is a natural question to ask whether these Hopf algebras are the same, and it is a theorem of Reshetikhin and Semenov-Tian-Shansky that they are. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The 6-vertex model / XXZ quantum spin chain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The 6-vertex model}

\begin{remark}
We will change slightly the evaluation representation: introduce $\zeta \in \mbb{C}^\times$, and then define 
\eq{
ev_\zeta : U_q(\hat{\mf{sl}_2}) & \to U_q(\mf{sl}_2) \\
e_0 &\mapsto \zeta f \\
e_1 &\mapsto \zeta e \\
f_0 &\mapsto \zeta^{-1} e \\
f_1 &\mapsto \zeta^{-1} f \\
t_0 &\mapsto t^{-1} \\
t_1 &\mapsto t
}
This is called the \bam{principal evaluation rep} as opposed to the \bam{homogeneous evaluation rep}
We now get the $R$-matrix to be $R\pround{\frac{\zeta_1}{\zeta_2}} : V_{\zeta_1}^{(1)} \otimes V_{\zeta_2}^{(1)} \to V_{\zeta_1}^{(1)} \otimes V_{\zeta_2}^{(1)}$ given by 
\eq{
R(\zeta) = \begin{pmatrix} 1 & 0 & 0 & 0 \\
0 & \frac{(1-\zeta^2)q}{(1-q^2 \zeta^2)} & \frac{(1-q^2)\zeta}{(1-q^2 \zeta^2)} & 0 \\ 0 & \frac{(1-q^2)\zeta}{(1-q^2 \zeta^2)} & \frac{(1-\zeta^2)q}{(1-q^2 \zeta^2)} & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}
}
Note $R(\zeta)$ is symmetric, and satisfies $R(1) = P$. Graphically $P$ is represented as 
\begin{center}
    \begin{tikzpicture}
    \draw[->, rounded corners] (-0.5,0) -- (0,0) -- (0,-0.5);
    \draw[->,rounded corners] (0,0.5) -- (0,0) -- (0.5,0);
    \end{tikzpicture}
\end{center}
\end{remark}



\begin{ex}
Construct this $R$ for this principal evaluation rep. 
\end{ex}

We now want to define (local Boltzmann) weights of a 2d statistical mechanics lattice model. On the edges of a lattice we have variables $\in \pbrace{0,1}$. $R^{ij}_{kl}(\zeta)$ is the weight associated with the vertex 
\begin{center}
    \begin{tikzpicture}
    \draw[->] (-1,0) node[anchor = east] {i}  -- (1,0) node[anchor = west] {k} ;
    \draw[->] (0,1) node[anchor = south] {j} -- (0,-1) node[anchor = north] {l};
    \end{tikzpicture}
\end{center}

Now 

\begin{definition}
Let the \bam{monodromy matrix} be 
\eq{
T(\lambda,\bm{\zeta}) = R_{0n}\pround{\frac{\lambda}{\zeta_n}} \dots R_{01}\pround{\frac{\lambda}{\zeta_1}} : (\mbb{C}^2)^{\otimes (n+1)} \to (\mbb{C}^2)^{\otimes (n+1)} 
}
corresponding to the configuration 
\begin{center}
    \begin{tikzpicture}
    \draw (-2,0)  -- (3,0);
    \draw[text = red] (-2,0) node[anchor=south] {$\lambda$};
    \draw[text=red] (-1,1) node[anchor = west] {$\zeta_1$} -- (-1,-1);
    \draw[text=red] (0,1) node[anchor = west] {$\zeta_2$} -- (0,-1) ;
    \draw (1,1) node[anchor = west] {$\dots$} -- (1,-1) ;
    \draw[text=red] (2,1) node[anchor = west] {$\zeta_n$} -- (2,-1) ;
    \end{tikzpicture}
\end{center}
\end{definition}

\begin{definition}
Let the \bam{transfer matrix} corresponding to monodromy be 
\eq{
t(\lambda,\bm{\zeta}) = \tr_0(T(\lambda,\bm{\zeta})) : (\mbb{C}^2)^{\otimes n} \to (\mbb{C}^2)^{\otimes n}
}
That is 
\eq{
t_{b_1 b_2  \dots b_n}^{a_1 a_2 \dots a_n} = T_{0b_1\dots b_2}^{0a_1 \dots a_n} + T^{1a_1 \dots a_n}_{1b_1 \dots b_n} = T^{ca_1 \dots a_n}_{cb_1 \dots b_n}
}
\end{definition}

\begin{definition}
Define the \bam{partition function} to be 
\eq{
Z(\lambda,\bm{\zeta}) = \tr_{(\mbb{C}^2)^{\otimes n}}(t(\lambda_1,\bm{\zeta}) \dots t(\lambda_n,\bm{\zeta})) \in \mbb{C}
}
That is 
\eq{
Z = t^{c_1 \dots c_n}_{c_1 \dots c_n}
}
\end{definition}

The defines the \bam{6-vertex model} as there are 6 non-zero weighted vertices. 

\begin{prop}
The transfer matrix satisfy $\comm[t(\lambda,\bm{\zeta})]{t(\lambda^\prime,\bm{\zeta})}=0$, and so there is a simultaneous eigenbasis of $(\mbb{C}^2)^{\otimes n}$, $\pbrace{v}_{\bm{\zeta}}$. 
\end{prop}

Let 
\eq{
t(\lambda,\bm{\zeta}) v = f_v(\lambda,\bm{\zeta})v 
}
giving the eigenvalue, and then we know 
\eq{
Z(\lambda,\bm{\zeta}) = \sum_v f_v(\lambda_1,\bm{\zeta}) \cdots f_v(\lambda_n,\bm{\zeta})
}
Recall we can find $v,f_v$ from the Bethe ansatz. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The XXZ model}
Consider the special case of the transfer matrix $t(\lambda) = t(\lambda,\bm{1})$. Then 
\eq{
t(1) = \tr_0(P_{01} \dots P_{0n}) = \text{cyclic right shift}
}
Graphically we see $t(1)$ as 
\begin{center}
    \begin{tikzpicture}
    \draw[->,rounded corners] (0,0) node[anchor=east]{0}  -- (0.5,0) -- (0.5,-0.5);
    \draw[->,rounded corners] (0.5,0.5) -- (0.5,0) -- (1,0) -- (1,-0.5) ;
    \draw[->,rounded corners] (1,0.5) -- (1,0) -- (1.5,0) -- (1.5,-0.5) ;
    \draw[->,rounded corners] (1.5,0.5) -- (1.5,0) -- (2,0) -- (2,-0.5) ;
    \draw[->,rounded corners] (2,0.5) -- (2,0) -- (2.5,0) node[anchor=west]{0};
    \draw (3,0) node {$+$};
    \draw[->,rounded corners] (3.5,0) node[anchor=east]{1}  -- (4,0) -- (4,-0.5);
    \draw[->,rounded corners] (4,0.5) -- (4,0) -- (4.5,0) -- (4.5,-0.5) ;
    \draw[->,rounded corners] (4.5,0.5) -- (4.5,0) -- (5,0) -- (5,-0.5) ;
    \draw[->,rounded corners] (5,0.5) -- (5,0) -- (5.5,0) -- (5.5,-0.5) ;
    \draw[->,rounded corners] (5.5,0.5) -- (5.5,0) -- (6,0) node[anchor=west]{1};
    \end{tikzpicture}
\end{center}

By analogy $t(1)^{-1} = \text{cyclic left shift}$. We can then see that \hl{I don't understand why $t^{-1}t$ shouldn't just be 1.}
\begin{center}
    \begin{tikzpicture}
    
    \end{tikzpicture}
\end{center}


\eq{
t^{-1}(1) t^\prime(1) &= \sum_i \begin{tikzpicture}[baseline] \draw[->, rounded corners]; (-0.25,0.25) -- (0,0.25) -- (0,-0.25) \end{tikzpicture} \\
&= \sum_{i=1}^n \check{R}^\prime(1)_{i,i+1} \\
&= AH_{XXZ} + BI
}
where 
\eq{
H_{XXZ} = - \frac{1}{2} \sum_{i=1}^n \pround{\sigma_i^x\sigma_{i+1}^x  + \sigma_i^y \sigma_{i+1}^y  + \frac{1}{2}(q+ q^{-1})\sigma_i^z\sigma_{i+1}^z}
}

\begin{ex}
Show this, finding A,B
\end{ex}
We here are now considering $H_{XXZ}$ as a quantum Hamiltonian for some spin system. \\
Moreover, take $t(\lambda) = t^{(0)} + (\lambda-1)t^{(1)} + \dots = \sum_{k=0}^\infty (\lambda -1)^k t^{(k)}$. Then 
\eq{
\comm[t(\lambda)]{t(\lambda^\prime)} = 0 \Rightarrow \comm[t(\lambda)]{t^{(i)}} = 0 \Rightarrow \comm[t^{(i)}]{t^{(j)}}=0
}
and so 
\eq{
\comm[t(\lambda)]{H_{XXZ}} = 0 \Rightarrow \comm[t^{(i)}]{H_{XXZ}} = 0
}
The $t^{(i)}$ are then multiple conserved charges of the theory, giving a quantum analogue of the condition of Liouville integrability. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Comments}
$H_{XXZ}$ is interesting itself, as it is a model of real material which are effectively 1 dimensional, e.g. copper sulphate. We can then use this theory to make real predictions for a laboratory setting. \\
Further, we recall that all physically interesting quantities are written as correlation functions $\pangle{v^\prime \, | \, \mc{O} \, | \, v}$ in quantum field theory. Computing these via quantum integrable structure is possible. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\secmath{\mc{R}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quasi-triangular Hopf algebras}
We want to try understand what is the point of having this universal R-matrix. This is made by the following proposition:

\begin{prop}
Let $\pi_V:H \to \End(V), \, \pi_W:H\to \End(W)$, be two representations of $H$ (equivalently left $H$-modules) where $H$ is a quasitriangular Hopf algebra. Then $V \otimes W \cong W \otimes V$ as $H$-modules. 
\end{prop}
\begin{proof}
Let $\check{R}_{VW} = P(\pi_V \otimes \pi_W)(\mc{R})$. Then emulating the proof of prop \ref{prop:CQIS:introducinguniversalR} in reverse we get 
\eq{
\forall h \in H, \, \check{R}_{VW}(\pi_V \otimes \pi_W)(\Delta(h)) &= (\pi_W \otimes \pi_V)(\Delta(h)) \check{R}_{VW} \\
\Rightarrow \check{R}_{VW}(\pi_V \otimes \pi_W)(\Delta(h))\check{R}_{VW}^{-1} &=(\pi_W \otimes \pi_V)(\Delta(h))
}
This constructs an explicit isomorphism. 
\end{proof}

\begin{example}
A simple example of a quasitriangular Hopf algebra is a cocommutative Hopf algebra $(\Delta = \Delta^\prime)$ with $\mc{R} = 1$. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sweedler's Hopf algebra}
This subsection will be devoted to getting some practice with one particular Hopf algebra, and we will work through each stage. \\

\begin{definition}
Let $H=H_4$ be the algebra generated by the symbols $f,g$ with the relations
\eq{
f^2 &= 0 \\
g^2 &= 1 \\
fg &= -gf 
}
It is spanned by $\pbrace{1,f,g,fg}$, and hence is 4-dimensional. It is called \bam{Sweedler's Hopf algebra}
\end{definition}

\begin{prop}
$H_4$ is a Hopf algebra with the operations 
\eq{
\Delta(f) &= f \otimes g + 1 \otimes f \\
\Delta(g) &= g \otimes g \\
&\phantom{=} \\
\eps(f) &= 0 \\
\eps(g) &= 1 \\
&\phantom{=} \\
S(f) &= -fg^{-1} \\
S(g) &= g^{-1}
}
\end{prop}
\begin{proof}
Recall we need the following commuting diagrams:

\begin{tkz}
H \otimes H \arrow[r,"\Delta \otimes \id"] & H \otimes H \otimes H \\ 
H \arrow[u,"\Delta"] \arrow[r,"\Delta"] & H \otimes H \arrow[u,"\id \otimes \Delta"']
\end{tkz}
\begin{tkz}
H \otimes H \arrow[r,"\id \otimes \eps"] & H \otimes \mbb{C} \\
H \arrow[u,"\Delta"] \arrow[ur,"\sim"']
\end{tkz}
\begin{tkz}
H \otimes H \arrow[r,"\eps \otimes \id"] & \mbb{C} \otimes H  \\
H \arrow[u,"\Delta"] \arrow[ur,"\sim"']
\end{tkz}

\begin{tkz}
H \otimes H \arrow[r,"S \otimes \id"] & H \otimes H \arrow[d,"m"] \\
H \arrow[u,"\Delta"] \arrow[r,"1 \circ \eps"'] & H
\end{tkz}

\begin{tkz}
H \otimes H \arrow[r,"\id \otimes S"] & H \otimes H \arrow[d,"m"] \\
H \arrow[u,"\Delta"] \arrow[r,"1 \circ \eps"'] & H
\end{tkz}

As the maps are defined on generators, it suffices to check them on these. For example
\eq{
(\id \otimes \Delta) \circ \Delta(f) &= (\id \otimes \Delta)(f \otimes g + 1 \otimes f) \\
&= f \otimes g \otimes g + 1 \otimes (f \otimes g + 1 \otimes f) \\
&\phantom{=} \\
(\Delta \otimes \id) \circ \Delta(f) &= (\Delta \otimes \id) (f \otimes g + 1 \otimes f) \\
&= (f \otimes g + 1 \otimes f) \otimes g + 1 \otimes 1 \otimes f
}
\end{proof}

\begin{fact}
$H_4$ is the smallest non-commutative, non-cocommutative Hopf algebra
\end{fact}

Now as $P\circ \Delta \neq \Delta$, we do not have an immediate isomorphism between $H$-modules $V \otimes W$ and $W \otimes V$. Hence if we want to rectify this we are going to want that $H$ is quasitriangular. 

\begin{prop}
For $\beta \in \mbb{C}$, let
\eq{
\mc{R}_\beta = \frac{1}{2}\pround{1\otimes 1 + 1 \otimes g + g \otimes 1 - g\otimes g}\pround{1 \otimes 1 + \beta f \otimes fg}
}
Then $\mc{R}_\beta$ is a universal $R$ matrix for $H$
\end{prop}
\begin{proof}
As 
\eq{
\mc{R}_0 = \frac{1}{2}(1 \otimes 1 + 1 \otimes g  + g \otimes 1 - g \otimes g)
}
we can write it as a $16\times 16$ matrix as 
\begin{center}
\begin{blockarray}{*{17}{c}}
& \rotatebox{90}{(1,1)} & \rotatebox{90}{(1,f)} & \rotatebox{90}{(1,g)} & \rotatebox{90}{(1,fg)} 
& \rotatebox{90}{(f,1)} & \rotatebox{90}{(f,f)} & \rotatebox{90}{(f,g)} & \rotatebox{90}{(f,fg)}
& \rotatebox{90}{(g,1)} & \rotatebox{90}{(g,f)} & \rotatebox{90}{(g,g)} & \rotatebox{90}{(g,fg)} 
& \rotatebox{90}{(fg,1)} & \rotatebox{90}{(fg,f)} & \rotatebox{90}{(fg,g)} & \rotatebox{90}{(fg,fg)} \\
\begin{block}{c(*{16}{c})}
   (1,1)& 1 &   & 1 &   &   &   &   &   & 1 &   & 1 &   &   &   &   &   \\
   (1,f)&   & 1 &   &-1 &   &   &   &   &   & 1 &   &-1 &   &   &   &   \\
   (1,g)& 1 &   & 1 &   &   &   &   &   & 1 &   & 1 &   &   &   &   &   \\
  (1,fg)&   &-1 &   & 1 &   &   &   &   &   &-1 &   & 1 &   &   &   &   \\
   (f,1)&   &   &   &   & 1 &   & 1 &   &   &   &   &   &-1 &   &-1 &   \\
   (f,f)&   &   &   &   &   & 1 &   &-1 &   &   &   &   &   &-1 &   & 1 \\
   (f,g)&   &   &   &   & 1 &   & 1 &   &   &   &   &   &-1 &   &-1 &   \\
  (f,fg)&   &   &   &   &   &-1 &   & 1 &   &   &   &   &   & 1 &   &-1 \\
   (g,1)& 1 &   & 1 &   &   &   &   &   & 1 &   & 1 &   &   &   &   &   \\
   (g,f)&   & 1 &   &-1 &   &   &   &   &   & 1 &   &-1 &   &   &   &   \\
   (g,g)& 1 &   & 1 &   &   &   &   &   & 1 &   & 1 &   &   &   &   &   \\
  (g,fg)&   &-1 &   & 1 &   &   &   &   &   &-1 &   & 1 &   &   &   &   \\
  (fg,1)&   &   &   &   &-1 &   &-1 &   &   &   &   &   & 1 &   & 1 &   \\
  (fg,f)&   &   &   &   &   &-1 &   & 1 &   &   &   &   &   & 1 &   &-1 \\
  (fg,g)&   &   &   &   &-1 &   &-1 &   &   &   &   &   & 1 &   & 1 &   \\
 (fg,fg)&   &   &   &   &   & 1 &   &-1 &   &   &   &   &   &-1 &   & 1 \\                         
\end{block}
\end{blockarray}
\end{center}
Using your favourite technique, we can see that this matrix has non-zero determinant, and so $\mc{R}_0$ is invertible. Moreover we can find
\eq{
\mc{R}_0^2 &= \frac{1}{4}(1 \otimes 1 + 1 \otimes g  + g \otimes 1 - g \otimes g)^2 \\
&=  \frac{1}{4}(1 \otimes 1 + 1 \otimes g  + g \otimes 1 - g \otimes g \\
& \phantom{=} + 1 \otimes g + 1 \otimes 1  + g \otimes g - g \otimes 1 \\
& \phantom{=} +g \otimes 1 + g \otimes g  + 1 \otimes 1 - 1 \otimes g \\
& \phantom{=} -g \otimes g - g \otimes 1  - 1 \otimes g + 1 \otimes 1) = 1 \otimes 1
}
so $\mc{R}_0^{-1} = \mc{R}_0$. Defining $\tilde{\mc{R}}_\beta = \mc{R}_0^{-1} \mc{R}_\beta$ gives 
\eq{
\tilde{\mc{R}}_\beta = 1 \otimes 1 + \beta f \otimes fg
}
It is simple to check that $\tilde{\mc{R}}_\beta^{-1} = 1 \otimes 1 - \beta f \otimes fg = \tilde{\mc{R}}_{-\beta}$, so $\mc{R}_\beta$ is invertible, with 
\eq{
\mc{R}_\beta^{-1} &= \tilde{\mc{R}}_\beta^{-1} \mc{R}_0^{-1} \\
&= \tilde{\mc{R}}_{-\beta}\mc{R}_0 \\
&= \frac{1}{2}(1 \otimes 1 - \beta f \otimes fg)(1 \otimes 1 + 1 \otimes g  + g \otimes 1 - g \otimes g) \\
&= \frac{1}{2}(1 \otimes 1 + 1 \otimes g + g \otimes 1 + g \otimes g)(1 \otimes 1 + \beta fg \otimes f) \\
&= P\mc{R}_\beta
}
This means that, if $H$ is quasi-triangular, it is actually triangular. 
Now observe
\eq{
\tilde{\mc{R}}_\beta \Delta(f) &= (1 \otimes 1 + \beta f \otimes fg)(f \otimes g + 1 \otimes f) \\
&= f \otimes g + 1 \otimes f + \beta(f^2 \otimes fg^2 + f \otimes fgf) \\
&= f \otimes g + 1 \otimes f -\beta f^2g \\ 
&= f \otimes g + 1 \otimes f\\
\Delta(f) \tilde{\mc{R}}_\beta &= (f \otimes g + 1 \otimes f)(1 \otimes 1 + \beta f \otimes fg) \\
&= f \otimes g + 1 \otimes f + \beta(f^2 \otimes f + f\otimes f^2g) \\
&= f \otimes g + 1 \otimes f 
}
showing that $\Delta(g)$ commutes with $\tilde{\mc{R}}_\beta$ is a similar calculation. As a result we deduce that $\forall h \in H, \, \tilde{\mc{R}}_\beta \Delta(h) = \Delta(h) \tilde{\mc{R}}_\beta$\\
Further, we have 
\eq{
\Delta^\prime(f) &= g \otimes f + f \otimes 1 \\
\Delta^\prime(g) &= g \otimes g 
}
As $\mc{R}_0$ contains only $g$ terms we clearly have $\mc{R}_0\Delta(g) = \Delta(g)\mc{R}_0 = \Delta^\prime(g) \mc{R}_0$. Also
\eq{
\mc{R}_0 \Delta(f) &= \frac{1}{2}(1 \otimes 1 + 1 \otimes g  + g \otimes 1 - g \otimes g)(f \otimes g + 1 \otimes f) \\
&= \frac{1}{2}(f \otimes g + 1 \otimes f + f \otimes g^2 + 1 \otimes gf + gf \otimes g + g\otimes f - gf \otimes g^2 -g \otimes gf) \\
&= \frac{1}{2}(f \otimes g + 1 \otimes f + f \otimes 1 - 1 \otimes fg - fg \otimes g + g\otimes f + fg \otimes 1 + g \otimes fg) \\
&= \frac{1}{2}(g \otimes f + f \otimes 1)(1 \otimes 1 + 1 \otimes g  + g \otimes 1 - g \otimes g) \\
&= \Delta^\prime(f)\mc{R}_0
}
Hence we have $\forall h \in H, \, \mc{R}_0\Delta(h) = \Delta^\prime(h) \mc{R}_0$. Hence $\forall h \in H$,
\eq{
\mc{R}_\beta \Delta(h) &= \mc{R}_0 \tilde{\mc{R}}_\beta \Delta(h) =  \Delta^\prime(h)\mc{R}_0 \tilde{\mc{R}}_\beta = \Delta^\prime(h) \mc{R}_\beta
}
Finally we need that 
\eq{
(\Delta \otimes 1) \mc{R}_\beta &= (\mc{R}_\beta)_{13}(\mc{R}_\beta)_{23} \\
(1 \otimes \Delta)\mc{R}_\beta &= (\mc{R}_\beta)_{13}(\mc{R}_\beta)_{12}
}
Out of laziness, we will only show the first equation for $\mc{R}_0$. Then 
\eq{
(\Delta \otimes 1) \mc{R}_0 &= \frac{1}{2}\psquare{(1 \otimes 1) \otimes 1 + (1 \otimes 1) \otimes g + (g\otimes g) \otimes 1 - (g\otimes g)\otimes g} \\
&\phantom{=} \\
(\mc{R}_0)_{13} (\mc{R}_0)_{23} &= \frac{1}{4} \pround{1 \otimes 1 \otimes 1 + 1 \otimes 1 \otimes g + g \otimes 1 \otimes 1 - g \otimes 1 \otimes g}\pround{1 \otimes 1 \otimes 1 + 1 \otimes 1 \otimes g + 1 \otimes g \otimes 1 - 1 \otimes g \otimes g} \\
&= \frac{1}{4}(1 \otimes 1 \otimes 1 + 1 \otimes 1 \otimes g + 1 \otimes g \otimes 1 - 1 \otimes g \otimes g \\
&\phantom{=} + 1 \otimes 1 \otimes g + 1 \otimes 1 \otimes 1 + 1 \otimes g \otimes g - 1 \otimes g \otimes 1 \\
&\phantom{=} + g \otimes 1 \otimes 1 + g \otimes 1 \otimes g + g \otimes g \otimes 1 - g \otimes g \otimes g \\
&\phantom{=} + g \otimes 1 \otimes g + g \otimes 1 \otimes 1 + g \otimes g \otimes g - g \otimes g \otimes 1 ) = (\Delta \otimes 1) \mc{R}_0 \quad \text{(pairing up terms)}
}
\end{proof}

\begin{idea}
We are still trying to get our head around the question "why do we care about triangular Hopf algebras?". Gelaki \cite{Gelaki200} says 
\begin{displayquote}
Triangular Hopf algebras are the quasitriangular Hopf algebras whose representations
form a symmetric tensor category. In that sense they are the closest to
group algebras. The structure of triangular Hopf algebras is far from trivial, and yet is more tractable than that of general Hopf algebras, due to
their proximity to groups. This makes triangular Hopf algebras an excellent testing ground for general Hopf algebraic ideas, methods and conjectures.
\end{displayquote}
This is perhaps a good enough reason. 
\end{idea}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\secmath{U_q(\mf{sl}_2)}: the bar involution}

Recall that we thought of $U_q(\mf{sl}_2)$ as a deformation of $U(\mf{sl}_2)$, which we could recover in the limit $q \to 1$. We may similarly though consider $U_{q^{-1}}(\mf{sl}_2)$ in the same limit, and so we want to have some sense of the two giving a double cover. This is made precise through the following concept:

\begin{definition}
The \bam{bar involution} is the unique algebra homomorphism given by
\eq{
\bar{\phantom{e}}: U_q(\mf{sl}_2) &\to U_{q^{-1}}(\mf{sl}_2) \\
e &\mapsto \bar{e} = e \\
f &\mapsto \bar{f} = f \\
t^{\pm 1} &\mapsto \overline{t^{\pm1}} = t^{\mp 1}
}
\end{definition}

\begin{prop}\label{prop:CQIS:barinvolutionhomomorphism}
The bar involution exists.
\end{prop}
\begin{proof}
We need that the relations 
\eq{
tet^{-1} &= q^2 e \\
tft^{-1} &= q^{-2} f \\
tt^{-1} &= 1 = t^{-1} t \\
\comm[e]{f} &= \frac{t-t^{-1}}{q-q^{-1}}
}
are preserved (changing $q \mapsto q^{-1}$) under the map. Note the 3rd relation is immediate. We want
\eq{
 \bar{t} \bar{e} \bar{t^{-1}} &= q^2 \bar{e} \\
 \Leftrightarrow t^{-1} e t &= q^2 e \\
 \Leftrightarrow q^{-2} e &= tet^{-1}
}
which holds in $U_{q^{-1}}(\mf{sl}_2)$, and likewise for the second relation. Now we want 
\eq{
\comm[\bar{e}]{\bar{f}} &= \frac{\bar{t}- \bar{t^{-1}}}{q-q^{-1}} \\
\Leftrightarrow \comm[e]{f} &=\frac{t^{-1} - t}{q-q^{-1}} = \frac{t-t^{-1}}{q^{-1}-q} 
}
which again holds in $U_{q^{-1}}(\mf{sl}_2)$
\end{proof}

\begin{remark}
Note that because the bar involution $U_q \to U_{q^{-1}}$ exists, the involution $U_{q^{-1}} \to U_q$ also exists, and we can see they are mutual inverses. Hence the bar involution is actually an isomorphism. This motivates calling it an involution.  
\end{remark}

Now note we can define the map $U_q \otimes U_q \to U_{q^{-1}} \otimes U_{q^{-1}}$ by 
\eq{
\bar{u \otimes v} = \bar{u} \otimes \bar{v}
}

\begin{prop}
The bar involution is not an coalgebra homomorphism. 
\end{prop}
\begin{proof}
Note that the coproduct and co-unit on $U_q(\mf{sl}_2)$ do not depend on $q$, so any relation including a $t$ shows the problem, e.g. 
\eq{
\Delta(\bar{e}) &= \Delta(e) = e \otimes 1 + t \otimes e \\
\bar{\Delta(e)} &= \bar{e \otimes 1 + t \otimes e} = e \otimes 1 + t^{-1} \otimes e
}
\end{proof}

Observe that the problem simply arises from the $t$ terms, we are then inclined to make the definition of the algebra homomorphism
\eq{
\bar{\bar{\Delta}} : U_q(\mf{sl}_2) &\to U_q(\mf{sl}_2) \otimes U_q(\mf{sl}_2) \\
h &\mapsto \bar{\Delta(\bar{h})} 
}

\begin{prop}
$\bar{\bar{\Delta}}$ is a coproduct on $U_q(\mf{sl}_2)$. 
\end{prop}
\begin{proof}
As in prop \ref{prop:CQIS:barinvolutionhomomorphism}, we only need to check the relations. For example, we want 
\eq{
\bar{\bar{\Delta}}(t) \bar{\bar{\Delta}}(e) \bar{\bar{\Delta}}(t^{-1}) &= q^2 \bar{\bar{\Delta}}(e) \\
\Leftrightarrow (t \otimes t)(e \otimes 1 + t^{-1} \otimes e)(t^{-1} \otimes t^{-1}) &= q^2 (e \otimes 1 + t^{-1} \otimes e) \\
\Leftrightarrow tet^{-1} \otimes 1 + t^{-1} \otimes tet^{-1} &= (q^2 e) \otimes 1 + t^{-1} \otimes (q^2 e)
}
which is given by the relations in $ U_q(\mf{sl}_2) \otimes U_q(\mf{sl}_2)$. 
\end{proof}

We have now constructed a second coproduct on $U_q(\mf{sl}_2)$. It turns out that this will be helpful in an eventual goal of showing that $U_q(\mf{sl}_2)$ is quasitriangular. More specifically, if we can find $\tilde{\mc{R}}, \mc{R}_0$ s.t. $\forall u \in U_q(\mf{sl}_2)$
\eq{
\tilde{\mc{R}}\Delta(u) &= \bar{\bar{\Delta}}(u) \tilde{\mc{R}} \\
\mc{R}_0 \bar{\bar{\Delta}}(u) &= \Delta^\prime(u) \mc{R}_0
}
Then $\mc{R} = \mc{R}_0 \tilde{\mc{R}}$ satisfies
\eq{
\mc{R} \Delta(u) = \Delta^\prime(u) \mc{R}
}
Note how this is similar to the proof that Sweedler's Hopf algebra is quasitriangular, where we had instead of $\bar{\bar{\Delta}}$ just $\Delta$ again. \\
It turns out that such an $\mc{R}_0, \tilde{\mc{R}}$ do not exist in $U_q(\mf{sl}_2) \otimes U_q(\mf{sl}_2)$. It is thus necessary to take an extension called $U_q(\mf{sl}_2)^{\hat{\otimes}2}$, which will be define later. This encounters some problems, such as given representations of $U_q(\mf{sl}_2)$ $(V,\pi_V), \, (W,\pi_W)$, and $\mc{R} \in U_q(\mf{sl}_2)^{\hat{\otimes}2}$, it is not guaranteed that $R_{VW} \equiv (\pi_V \otimes \pi_W)(\mc{R})$ is a well defined element of $\End(V \otimes W)$\footnote{\hl{That seems important, can we get a proof?}}. We have a solution to this issue

\begin{definition}
A representation $(\pi,V)$ of $U_q(\mf{sl}_2)$ is of \bam{type-1} if 
\eq{
V = \bigoplus_{\lambda \in \mbb{Z}} V_\lambda
}
where 
\eq{
V_\lambda = \pbrace{v \in V \, | \, \pi(t)(v) = q^\lambda v}
}
is the $\lambda$-weight space.
\end{definition}

\begin{lemma}
Every highest weight representation if of type-1. 
\end{lemma}

\begin{prop}
If $V,W$ are type-1 representations, then $R_{VW}\in \End(V \otimes W)$ is well defined.  
\end{prop}

\begin{remark}
More generally, $R_{VW}$ is defined if $V,W$ are representation in \bam{category $\mc{O}$}, a semisimple category: i.e. a category whose objects are all direct sums of simple objects, which are the highest weight reps. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\secmath{U_q(\mf{sl}_2)}: the universal R-matrix}
Recalling the basis of $U_q(\mf{sl}_2) \otimes U_q(\mf{sl}_2)$ is $\pbrace{e \otimes 1, f \otimes1, t^{\pm1}\otimes 1, 1\otimes e, 1\otimes f, 1 \otimes t^{\pm 1}}$, now consider the extension of the basis by the formal element $\mc{R}_0$ with relations 
\eq{
\mc{R}_0(e \otimes 1) &= (e \otimes t) \mc{R}_0 & \mc{R}_0(1 \otimes e) &= (t \otimes e) \mc{R}_0  \\
\mc{R}_0 (f \otimes 1) &= (f \otimes t^{-1}) \mc{R}_0 & \mc{R}_0(1 \otimes f) &= (t^{-1} \otimes f) \mc{R}_0 \\
\mc{R}_0(t \otimes 1) &= (t \otimes 1) \mc{R}_0 & \mc{R}_0(1 \otimes t) &= (1 \otimes t) \mc{R}_0 
}
As we thought of $t$ as $q^h$, we think of $\mc{R}_0$ as $q^{\frac{h \otimes h}{2}}$. This motivates our relations for $\mc{R}_0$, e.g. 
\eq{
\mc{R}_0 (e \otimes 1) &= q^{\frac{h \otimes h}{2}}(e \otimes 1) \\
&= (e \otimes 1) q^{\frac{(h+2) \otimes h}{2}} \\
&= (e \otimes 1) q^{1 \otimes h + \frac{h \otimes h}{2}} \\
&= (e \otimes 1)(1 \otimes t) q^{\frac{h \otimes h}{2}} \\
&= (e \otimes t) \mc{R}_0
}

\begin{lemma}
$\forall u \in U_q(\mf{sl}_2), \, \mc{R}_0 \bar{\bar{\Delta}}(u) = \Delta^\prime(u) \mc{R}_0$
\end{lemma}
\begin{proof}
Simply check generators:
\eq{
\mc{R}_0 \bar{\bar{\Delta}}(e) &= \mc{R}_0(e \otimes 1 + t^{-1} \otimes e) \\ 
&= \mc{R}_0(e \otimes 1) + \mc{R}_0(t^{-1} \otimes 1)(1 \otimes e)\\
&= (e \otimes t) \mc{R}_0 + ( t^{-1} \otimes 1)(t \otimes e)\mc{R}_0 \\
&= (e \otimes t + 1 \otimes e) \mc{R}_0 \\
&= \Delta^\prime(e) \mc{R}_0
}
The other cases are the same. 
\end{proof}

\begin{definition}
The algebra $U_q(\mf{sl}_2)^{\hat{\otimes}2}$ is the extension to  $U_q(\mf{sl}_2) \otimes U_q(\mf{sl}_2)$ formed by joining the element $\mc{R}_0$, defined as above, along with elements of the form $\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s$ for sequences of complex numbers $(c_{rs})$ subject to 
\eq{
c_{rs} = \delta_{r1}\delta_{s0} &\Rightarrow \sum_{r,s \geq 0} c_{rs} f^r \otimes e^s = f \otimes 1 \\
c_{rs} = \delta_{r0}\delta_{s1} &\Rightarrow \sum_{r,s \geq 0} c_{rs} f^r \otimes e^s = 1 \otimes e 
}
\eq{
(t \otimes 1)\pround{\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s} (t^{-1} \otimes 1) &= \sum_{r,s \geq 0} q^{-2r}c_{rs} f^r \otimes e^s \\
(1 \otimes t) \pround{\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s}(1 \otimes t^{-1}) &= \sum_{r,s \geq 0} q^{2s}c_{rs} f^r \otimes e^s \\
\pround{\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s} \pround{\sum_{r^\prime,s^\prime \geq 0} c^\prime_{r^\prime s^\prime} f^{r^\prime} \otimes e^{s^\prime}} &= \sum_{R,S \geq 0} \pround{\sum_{\substack{{r,r^\prime \geq 0} \\ {r+r^\prime=R}}}\sum_{\substack{{s,s^\prime \geq 0} \\ {s+s^\prime=S}}}c_{rs}c^\prime_{r^\prime s^\prime}} f^R \otimes e^S 
}
\eq{
\comm[\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s]{1 \otimes f} &= (1 \otimes t) \pround{\sum_{r,s \geq 0} \frac{q^{-s}[s+1]c_{r,s+1}}{q-q^{-1}} f^r \otimes e^s} - \pround{\sum_{r,s \geq 0} \frac{q^{-s}[s+1]c_{r,s+1}}{q-q^{-1}} f^r \otimes e^s}(1 \otimes t^{-1}) \\
\comm[\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s]{e \otimes 1} &= (t^{-1} \otimes 1) \pround{\sum_{r,s \geq 0} \frac{q^{-r}[r+1]c_{r+1,s}}{q-q^{-1}} f^r \otimes e^s} - \pround{\sum_{r,s \geq 0} \frac{q^{-r}[r+1]c_{r+1,s}}{q-q^{-1}} f^r \otimes e^s}(t \otimes 1)
}
We need to also specify how $\mc{R}_0$ commutes with power series to fully define $U_q(\mf{sl}_2)^{\hat{\otimes}2}$, but as it will not be neccesary for us we will omit it \hl{(Can I introduce it back in to makes these notes a little bit more complete)}
\end{definition}

Now we need to find a $\tilde{\mc{R}}$. 

\begin{prop}
Let 
\eq{
d_r = \frac{(q-q^{-1})^r q^{\frac{r(r-1)}{2}}}{[r]!}d_0
}
for $d_0 \in \mbb{C}$, where $[r]! = \prod_{s=1}^r [s]$. Then $\mc{R} = \mc{R}_0 \sum_{r \geq 0} d_r f^r \otimes e^r$ satisfies 
\eq{
\mc{R} \Delta(u)  = \Delta^\prime(u) \mc{R}
}
\end{prop}
\begin{proof}
We provide an instructive proof as to how $\mc{R}$ is found, rather than just checking. Take an ansatz of 
\eq{
\tilde{\mc{R}} = \sum_{r,s \geq 0} c_{rs} f^r \otimes e^s
}
Recall we need $\forall u \in U_q(\mf{sl}_2), \, \tilde{\mc{R}}\Delta(u) = \bar{\bar{\Delta}}(u) \tilde{\mc{R}}$. Then as $\Delta(t^{\pm1}) = t^{\pm1} \otimes t^{\pm1} = \bar{\bar{\Delta}}(t^{\pm 1})$ we see that 
\eq{
\bar{\bar{\Delta}}(t^{\pm}) \tilde{\mc{R}} &= (t^{\pm1} \otimes 1)(1 \otimes t^{\pm 1} \pround{\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s} \\
&= (t^{\pm1} \otimes 1)\pround{\sum_{r,s \geq 0} q^{\pm2s} c_{rs} f^r \otimes e^s}(1 \otimes t^{\pm1}) \\
&= \pround{\sum_{r,s \geq 0} q^{\pm2(s-r)} c_{rs}c_{rs} f^r \otimes e^s} (t^{\pm1} \otimes 1)(1 \otimes t^{\pm1})
}
so it is imposed upon us that $c_{rs} = \delta_{rs}d_r$ for some sequence $(d_r)$. Now we calculate 
\eq{
\tilde{\mc{R}}\Delta(e) - \bar{\bar{\Delta}}(e) \tilde{\mc{R}} &= \comm[\tilde{\mc{R}}]{e \otimes 1} + \tilde{\mc{R}}(t \otimes e) - (t^{-1} \otimes e) \tilde{\mc{R}} \\
&= (t^{-1} \otimes 1) \pround{\sum_{r \geq 0} \frac{q^{-r}[r+1]d_{r+1}}{q-q^{-1}} f^r \otimes e^{r+1}} - \pround{\sum_{r \geq 0} \frac{q^{-r}[r+1]d_{r+1}}{q-q^{-1}} f^r \otimes e^{r+1}}(t \otimes 1) \\
&\phantom{=} + \pround{\sum_{r \geq 0}d_r f^r \otimes e^{r+1}}(t \otimes 1) - (t^{-1} \otimes 1)\pround{\sum_{r \geq 0}d_r f^r \otimes e^{r+1}} \\
&=  \psquare{\sum_{r \geq 0}\pround{d_r -\frac{q^{-r}[r+1]d_{r+1}}{q-q^{-1}}} f^r \otimes e^{r+1}}(t \otimes 1) \\
&\phantom{=} \quad - (t^{-1} \otimes 1) \psquare{\sum_{r \geq 0}\pround{d_r -\frac{q^{-r}[r+1]d_{r+1}}{q-q^{-1}}} f^r \otimes e^{r+1}}
}
Hence we are forced to have the recursion relation for $d_r$,
\eq{
d_{r+1} = \frac{(q-q^{-1})q^r}{[r+1]} d_r \Rightarrow d_r = \frac{(q-q^{-1})^r q^{\frac{r(r-1)}{2}}}{[r]!}d_0
}
The calculation for $f$ gives the same result. 
\end{proof}

We can lift the coproduct $\Delta$ to one on $U_q(\mf{sl}_2)^{\hat{\otimes}2}$ in a natural way in the sense that 
\eq{
(\Delta \otimes \id) (\mc{R}_0) &= (\mc{R}_0)_{13} (\mc{R}_0)_{23} & (\Delta \otimes 1) \pround{\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s} &= \sum_{r,s \geq 0} c_{rs} \Delta(f)^r \otimes e^s\\
(\id \otimes \Delta)(\mc{R}_0) &= (\mc{R}_0)_{13} (\mc{R}_0)_{12} &  (\id \otimes \Delta)\pround{\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s} &= \sum_{r,s \geq 0} c_{rs} f^r \otimes \Delta(e)^s
}

\begin{prop}
If $d_0=1$, $\mc{R}$ defined as above satisfies 
\eq{
(\Delta \otimes \id)(\mc{R}) &= \mc{R}_{13}\mc{R}_{23} \\
(\id \otimes \Delta)(\mc{R}) &= \mc{R}_{13}\mc{R}_{12}
}
\end{prop}
\begin{proof}
We see 
\eq{
(\Delta \otimes \id)(\mc{R}) = \mc{R}_{13}\mc{R}_{23} \qquad &\Leftrightarrow& (\Delta \otimes \id)(\mc{R}_0 \tilde{\mc{R}}) &= (\mc{R}_0\tilde{\mc{R}})_{13} (\mc{R}_0\tilde{\mc{R}})_{23} \\
&\Leftrightarrow& (\mc{R}_0)_{13} (\mc{R}_0)_{23} (\Delta \otimes \id)(\tilde{\mc{R}}) &= (\mc{R}_0)_{13} \tilde{\mc{R}}_{13} (\mc{R}_0)_{23}\tilde{\mc{R}}_{23} \\
&\Leftrightarrow& (\Delta \otimes \id)(\tilde{\mc{R}}) &= (\mc{R}_0)^{-1}_{23} \tilde{\mc{R}}_{13} (\mc{R}_0)_{23} \tilde{\mc{R}}_{23}
}
We have that 
\eq{
(\mc{R}_0)^{-1}_{23} \tilde{\mc{R}}_{13} (\mc{R}_0)_{23} = \sum_{s \geq 0} d_s f^s \otimes t^{-s} \otimes e^s
}
\hl{(why?)}
and so we want $(\Delta \otimes \id)(\tilde{\mc{R}}) - \pround{\sum_{s \geq 0}d_s f^s \otimes t^{-s} \otimes e^s}\tilde{\mc{R}}_{23} = 0$. We can calculate 
\eq{
(\Delta \otimes \id)(\tilde{\mc{R}}) - \pround{\sum_{s \geq 0}d_s f^s \otimes t^{-s} \otimes e^s}\tilde{\mc{R}}_{23} &= \sum_{r \geq 0} d_r(f\otimes t^{-1} + 1 \otimes f)^r \otimes e^r \\
&\phantom{=} \quad - \sum_{s,s^\prime \geq 0} d_s d_{s^\prime} (f^s \otimes t^{-s} \otimes e^s)(1 \otimes f^{s^\prime} \otimes e^{s^\prime}) \\
&= \sum_{r \geq 0} \psquare{d_r(f\otimes t^{-1} + 1 \otimes f)^r - \sum_{\substack{{s,s^\prime \geq 0} \\ {s+s^\prime = r}}}d_s d_{s^\prime} f^s \otimes t^{-s} f^{s^\prime}} \otimes e^r
}
An induction argument shows this vanished if $d_0 = 1$. 
\end{proof}

\begin{theorem}
$\mc{R}$ satisfies all the axioms of a universal $R$-matrix. 
\end{theorem}
\begin{proof}
It only remains to show that $\mc{R}$ is invertible in $U_q(\mf{sl}_2)^{\hat{\otimes}2}$, but this follows as $\mc{R}_0$ is invertible by definition, and $\tilde{\mc{R}}$ is invertible because it is a formal power series with invertible constant term. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}

Let us start with a simple question: why did we do all that work above? Well first we solve the following exercise

\begin{ex}
Argue how $\pi^{(m)} \otimes \pi^{(n)}$ acts on $\sum_{r,s \geq 0} c_{rs} f^r \otimes e^s$ and $\mc{R}_0 = q^{\frac{h \otimes h}{2}}$. 
\end{ex}

With this we can obtain a solution to the Yang-Baxter equation given by $(\pi^{(m)} \otimes \pi^{(n)})(\mc{R})$

\begin{example}
Let $\pi = \pi^{(1)}$ be the 2d rep of $U_q(\mf{sl}_2)$, where 
\eq{
\pi(e) &= \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \\
\pi(f) &= \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} \\
\pi(t) &= \begin{pmatrix} q & 0 \\ 0 & q^{-1} \end{pmatrix}
}
ordering the basis as $\pbrace{v_0,v_1}$. Formally identifying $t = q^h$, we see $\pi(h) = \begin{psmallmatrix} 1 & 0 \\ 0 & -1 \end{psmallmatrix}$. Then we have 
\eq{
(\pi \otimes \pi)(\mc{R}_0) &= q^{\frac{1}{2} \pi(h) \otimes \pi(h)} \\
&=  q^{\frac{1}{2} \begin{psmallmatrix} 1 & 0 \\ 0 & -1 \end{psmallmatrix} \otimes \begin{psmallmatrix} 1 & 0 \\ 0 & -1 \end{psmallmatrix}} \\
&= q^{\frac{1}{2}\begin{psmallmatrix} 1 & 0 & 0 & 0 \\ 0 & -1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & 1\end{psmallmatrix}} \\
&= q^{-\frac{1}{2}}\begin{pmatrix} q & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & q\end{pmatrix}
}
Further note $\pi(e)^2 = 0 = \pi(f)^2$, so 
\eq{
(\pi \otimes \pi)(\tilde{\mc{R}}) &= \sum_{r \geq 0} d_r \pi(f)^r \otimes \pi(e)^r \\
&= d_0 1 \otimes 1 + d_1 \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} \otimes \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \\
&= \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{pmatrix} + (q-q^{-1})\begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix}
}
Hence 
\eq{
(\pi \otimes \pi)(\mc{R}) &= (\pi \otimes \pi)(\mc{R}_0)(\pi \otimes \pi)(\tilde{\mc{R}}) \\
&= q^{-\frac{1}{2}}\begin{pmatrix} q & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & q\end{pmatrix} + q^{-\frac{1}{2}}\begin{pmatrix} q & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & q\end{pmatrix}\begin{pmatrix} 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & q-q^{-1} & 0 & 0 \\ 0 & 0 & 0 & 0\end{pmatrix} \\
&= q^{-\frac{1}{2}}\begin{pmatrix} q & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & q-q^{-1} & 1 & 0 \\ 0 & 0 & 0 & q\end{pmatrix}
}
By our theory we now know this matrix will satisfy the Yang-Baxter equation. 
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{../../bib/custom-bib-style}
\bibliography{../../bib/library}
%\printbibliography

\end{document}
